{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "x_fPhAC9u2FE",
    "outputId": "ca04dfe0-2bb8-48c0-fe91-2be1469a0660"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "#use Google Coolab with free GPU to train model\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kWQfJ0MPvGXJ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "\n",
    "from typing import Iterable\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7MIBzDDPvI80"
   },
   "outputs": [],
   "source": [
    "#LeNet with Batch Normalization\n",
    "class LeNetBN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # convolution layers\n",
    "        self._body = nn.Sequential(\n",
    "            # First convolution Layer\n",
    "            # input size = (32, 32), output size = (28, 28)\n",
    "            nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5),\n",
    "            nn.BatchNorm2d(6),\n",
    "            # ReLU activation\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Max pool 2-d\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            \n",
    "            # Second convolution layer\n",
    "            # input size = (14, 14), output size = (10, 10)\n",
    "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            # output size = (5, 5)\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self._head = nn.Sequential(\n",
    "            # First fully connected layer\n",
    "            # in_features = total number of weight in last conv layer = 16 * 5 * 5\n",
    "            nn.Linear(in_features=16 * 5 * 5, out_features=120), \n",
    "            \n",
    "            # ReLU activation\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # second fully connected layer\n",
    "            # in_features = output of last linear layer = 120 \n",
    "            nn.Linear(in_features=120, out_features=84), \n",
    "            \n",
    "            # ReLU activation\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Third fully connected layer. It is also output layer\n",
    "            # in_features = output of last linear layer = 84\n",
    "            # and out_features = number of classes = 10 (0-9)\n",
    "            nn.Linear(in_features=84, out_features=10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # apply feature extractor\n",
    "        x = self._body(x)\n",
    "        # flatten the output of conv layers\n",
    "        # dimension should be batch_size * number_of weight_in_last conv_layer\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        # apply classification head\n",
    "        x = self._head(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "colab_type": "code",
    "id": "0skH0fkYQSsj",
    "outputId": "9bcc311a-42cb-4a02-98d1-51c968f90810"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNetBN(\n",
      "  (_body): Sequential(\n",
      "    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (_head): Sequential(\n",
      "    (0): Linear(in_features=400, out_features=120, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=120, out_features=84, bias=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Linear(in_features=84, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = LeNetBN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aRAT2WWsvWQz"
   },
   "outputs": [],
   "source": [
    "train_test_transforms_SVHM = transforms.Compose([\n",
    "    #convert to grayscale                                             \n",
    "    transforms.Grayscale(1),\n",
    "    #re-scale image tensor values between 0-1. image_tensor /= 255\n",
    "    transforms.ToTensor(),\n",
    "    # subtract mean (0.2860) and divide by variance (0.2897).\n",
    "    # This mean and variance is calculated on training data \n",
    "    transforms.Normalize((0.2860, ), (0.2897, ))\n",
    "])\n",
    "\n",
    "train_test_transforms_MNIST = transforms.Compose([\n",
    "    #resize to (32,32)                                              \n",
    "    transforms.Resize((32, 32)),\n",
    "    # re-scale image tensor values between 0-1. image_tensor /= 255\n",
    "    transforms.ToTensor(),\n",
    "    # subtract mean (0.2860) and divide by variance (0.2897).\n",
    "    # This mean and variance is calculated on training data\n",
    "    transforms.Normalize((0.2860, ), (0.2897, ))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cIVZRdzk3vZq"
   },
   "outputs": [],
   "source": [
    "#download data from Google Drive\n",
    "trainSVHM = datasets.SVHN('../content/gdrive/My Drive/SVHN', split='train', download=False, transform=train_test_transforms_SVHM)\n",
    "testSVHM = datasets.SVHN('../content/gdrive/My Drive/SVHN', split='test', download=False, transform=train_test_transforms_SVHM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Jzggwr8TOwo"
   },
   "outputs": [],
   "source": [
    "#download data from Google Drive\n",
    "trainMNIST = datasets.MNIST('../content/gdrive/My Drive', train=True, download=False, transform=train_test_transforms_MNIST)\n",
    "testMNIST = datasets.MNIST('../content/gdrive/My Drive', train=False, download=False, transform=train_test_transforms_MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "abyGcW0d_AOS"
   },
   "outputs": [],
   "source": [
    "#generate diferent image of digits by OpenCV\n",
    "train_aug = []\n",
    "for digit in range(10):\n",
    "    for font in [0,2,3,4,6,7]:\n",
    "        for scale in [0.8, 1.0]:\n",
    "            for color in [70, 100, 120, 160, 200, 220, 255]:\n",
    "                for thik in [0,1,2]:\n",
    "                    for line in [cv2.FILLED, cv2.LINE_4, cv2.LINE_8, cv2.LINE_AA]:\n",
    "                        img = np.zeros((32,32), dtype=np.uint8)\n",
    "                        img = cv2.putText(img, str(digit), (6,25), font, scale, color, thik, line)\n",
    "                        train_aug.append((transforms.Normalize((0.2860, ), (0.2897, ))(transforms.ToTensor()(img)), digit)) #transform to tensor and normalize. put in list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yq59BeHv_86d"
   },
   "outputs": [],
   "source": [
    "test_aug = []\n",
    "for digit in range(10):\n",
    "    for scale in [0.8, 1.0]:\n",
    "        for color in [70, 100, 120, 160, 200, 220, 255]:\n",
    "            for thik in [0,1,2]:\n",
    "                for line in [cv2.FILLED, cv2.LINE_4, cv2.LINE_8, cv2.LINE_AA]:\n",
    "                    img = np.zeros((32,32), dtype=np.uint8)\n",
    "                    img = cv2.putText(img, str(digit), (6,25), 16, scale, color, thik, line)\n",
    "                    #plt.imshow(img,'gray');plt.show()\n",
    "                    test_aug.append((transforms.Normalize((0.2860, ), (0.2897, ))(transforms.ToTensor()(img)), digit))#transform to tensor and normalize. put in list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kk2gufzwQfgs"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "#if torch.cuda.is_available():\n",
    "#    torch.backend.cudnn_benchmark_enabled = True\n",
    "#    torch.backend.cudnn.deterministic = True\n",
    "\n",
    "#training parameters\n",
    "batch_size = 32\n",
    "epochs_count = 20\n",
    "learning_rate = 0.01\n",
    "log_interval = 100\n",
    "test_interval = 1\n",
    "num_workers  = 10\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "KhG-KqsEviSQ",
    "outputId": "b05eb6e0-efea-4574-f749-6e01579d841b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    print('GPU')\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    batch_size = 16\n",
    "    num_workers = 0\n",
    "    epochs_count = 10\n",
    "    print('CPU')\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "#SGDoptimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#array for plotting\n",
    "best_loss = torch.tensor(np.inf)\n",
    "\n",
    "# epoch train/test loss\n",
    "epoch_train_loss = np.array([])\n",
    "epoch_test_loss = np.array([])\n",
    "\n",
    "# epch train/test accuracy\n",
    "epoch_train_acc = np.array([])\n",
    "epoch_test_acc = np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZOdWeCXxvkaC"
   },
   "outputs": [],
   "source": [
    "# train dataloader\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "     trainMNIST + trainSVHM + train_aug,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "# test dataloader\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    testMNIST + testSVHM + test_aug,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VE6H7m-svksn"
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader, epoch_idx):\n",
    "    # change model in training mood\n",
    "    model.train()\n",
    "\n",
    "    # to get batch loss\n",
    "    batch_loss = np.array([])\n",
    "\n",
    "    # to get batch accuracy\n",
    "    batch_acc = np.array([])\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "        # clone target\n",
    "        indx_target = target.clone()\n",
    "        # send data to device (its is medatory if GPU has to be used)\n",
    "        data = data.to(device)\n",
    "        # send target to device\n",
    "        target = target.to(device)\n",
    "\n",
    "        # reset parameters gradient to zero\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass to the model\n",
    "        output = model(data)\n",
    "\n",
    "        # cross entropy loss\n",
    "        loss = F.cross_entropy(output, target)\n",
    "\n",
    "        # find gradients w.r.t training parameters\n",
    "        loss.backward()\n",
    "        # Update parameters using gardients\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_loss = np.append(batch_loss, [loss.item()])\n",
    "\n",
    "        # Score to probability using softmax\n",
    "        prob = F.softmax(output, dim=1)\n",
    "\n",
    "        # get the index of the max probability\n",
    "        pred = prob.data.max(dim=1)[1]\n",
    "\n",
    "        # correct prediction\n",
    "        correct = pred.cpu().eq(indx_target).sum()\n",
    "\n",
    "        # accuracy\n",
    "        acc = float(correct) / float(len(data))\n",
    "\n",
    "        batch_acc = np.append(batch_acc, [acc])\n",
    "\n",
    "        if batch_idx % log_interval == 0 and batch_idx > 0:\n",
    "            print(\n",
    "                'Train Epoch: {} [{}/{}] Loss: {:.6f} Acc: {:.4f}'.format(\n",
    "                    epoch_idx, batch_idx * len(data), len(train_loader.dataset), loss.item(), acc\n",
    "                )\n",
    "            )\n",
    "\n",
    "    epoch_loss = batch_loss.mean()\n",
    "    epoch_acc = batch_acc.mean()\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def validate(model, test_loader):\n",
    "    #\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    count_corect_predictions = 0\n",
    "    for data, target in test_loader:\n",
    "        indx_target = target.clone()\n",
    "        data = data.to(device)\n",
    "\n",
    "        target = target.to(device)\n",
    "\n",
    "        output = model(data)\n",
    "        # add loss for each mini batch\n",
    "        test_loss += F.cross_entropy(output, target).item()\n",
    "\n",
    "        # Score to probability using softmax\n",
    "        prob = F.softmax(output, dim=1)\n",
    "\n",
    "        # get the index of the max probability\n",
    "        pred = prob.data.max(dim=1)[1]\n",
    "\n",
    "        # add correct prediction count\n",
    "        count_corect_predictions += pred.cpu().eq(indx_target).sum()\n",
    "\n",
    "    # average over number of mini-batches\n",
    "    test_loss = test_loss / len(test_loader)\n",
    "\n",
    "    # average over number of dataset\n",
    "    accuracy = 100. * count_corect_predictions / len(test_loader.dataset)\n",
    "\n",
    "    print(\n",
    "        '\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "            test_loss, count_corect_predictions, len(test_loader.dataset), accuracy\n",
    "        )\n",
    "    )\n",
    "    return test_loss, accuracy / 100.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "vjythJS-vkv_",
    "outputId": "272b902f-a353-4caa-9c71-c1e7db03fcfd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [3200/143337] Loss: 2.279205 Acc: 0.0625\n",
      "Train Epoch: 0 [6400/143337] Loss: 2.098725 Acc: 0.3125\n",
      "Train Epoch: 0 [9600/143337] Loss: 1.985968 Acc: 0.5000\n",
      "Train Epoch: 0 [12800/143337] Loss: 1.543944 Acc: 0.4688\n",
      "Train Epoch: 0 [16000/143337] Loss: 1.612607 Acc: 0.3750\n",
      "Train Epoch: 0 [19200/143337] Loss: 1.399809 Acc: 0.5000\n",
      "Train Epoch: 0 [22400/143337] Loss: 1.022071 Acc: 0.6875\n",
      "Train Epoch: 0 [25600/143337] Loss: 0.875807 Acc: 0.7188\n",
      "Train Epoch: 0 [28800/143337] Loss: 1.055039 Acc: 0.6875\n",
      "Train Epoch: 0 [32000/143337] Loss: 0.811369 Acc: 0.7188\n",
      "Train Epoch: 0 [35200/143337] Loss: 0.650539 Acc: 0.9062\n",
      "Train Epoch: 0 [38400/143337] Loss: 0.589121 Acc: 0.7812\n",
      "Train Epoch: 0 [41600/143337] Loss: 0.591326 Acc: 0.7812\n",
      "Train Epoch: 0 [44800/143337] Loss: 0.862348 Acc: 0.7188\n",
      "Train Epoch: 0 [48000/143337] Loss: 0.740395 Acc: 0.8125\n",
      "Train Epoch: 0 [51200/143337] Loss: 0.386394 Acc: 0.9375\n",
      "Train Epoch: 0 [54400/143337] Loss: 0.672879 Acc: 0.8125\n",
      "Train Epoch: 0 [57600/143337] Loss: 0.586803 Acc: 0.8438\n",
      "Train Epoch: 0 [60800/143337] Loss: 0.587510 Acc: 0.8125\n",
      "Train Epoch: 0 [64000/143337] Loss: 0.518559 Acc: 0.8438\n",
      "Train Epoch: 0 [67200/143337] Loss: 0.463161 Acc: 0.8750\n",
      "Train Epoch: 0 [70400/143337] Loss: 0.572711 Acc: 0.8125\n",
      "Train Epoch: 0 [73600/143337] Loss: 0.655989 Acc: 0.7812\n",
      "Train Epoch: 0 [76800/143337] Loss: 0.392802 Acc: 0.8750\n",
      "Train Epoch: 0 [80000/143337] Loss: 0.846889 Acc: 0.8125\n",
      "Train Epoch: 0 [83200/143337] Loss: 0.459506 Acc: 0.8750\n",
      "Train Epoch: 0 [86400/143337] Loss: 0.349137 Acc: 0.8750\n",
      "Train Epoch: 0 [89600/143337] Loss: 0.572171 Acc: 0.8750\n",
      "Train Epoch: 0 [92800/143337] Loss: 0.523071 Acc: 0.8750\n",
      "Train Epoch: 0 [96000/143337] Loss: 0.304530 Acc: 0.9375\n",
      "Train Epoch: 0 [99200/143337] Loss: 0.622077 Acc: 0.8438\n",
      "Train Epoch: 0 [102400/143337] Loss: 0.334369 Acc: 0.9375\n",
      "Train Epoch: 0 [105600/143337] Loss: 0.376092 Acc: 0.8438\n",
      "Train Epoch: 0 [108800/143337] Loss: 0.702533 Acc: 0.7188\n",
      "Train Epoch: 0 [112000/143337] Loss: 0.300412 Acc: 0.8750\n",
      "Train Epoch: 0 [115200/143337] Loss: 0.504473 Acc: 0.8125\n",
      "Train Epoch: 0 [118400/143337] Loss: 0.826744 Acc: 0.8125\n",
      "Train Epoch: 0 [121600/143337] Loss: 0.538309 Acc: 0.8438\n",
      "Train Epoch: 0 [124800/143337] Loss: 0.578352 Acc: 0.8438\n",
      "Train Epoch: 0 [128000/143337] Loss: 0.326793 Acc: 0.8750\n",
      "Train Epoch: 0 [131200/143337] Loss: 0.600305 Acc: 0.8750\n",
      "Train Epoch: 0 [134400/143337] Loss: 0.323039 Acc: 0.9062\n",
      "Train Epoch: 0 [137600/143337] Loss: 0.407319 Acc: 0.8750\n",
      "Train Epoch: 0 [140800/143337] Loss: 0.300959 Acc: 0.9375\n",
      "Elapsed 37.11s, 37.11 s/epoch, 0.01 s/batch, ets 705.07s\n",
      "\n",
      "Test set: Average loss: 0.5436, Accuracy: 31668/37712 (84%)\n",
      "\n",
      "Train Epoch: 1 [3200/143337] Loss: 0.148871 Acc: 0.9688\n",
      "Train Epoch: 1 [6400/143337] Loss: 0.629410 Acc: 0.8125\n",
      "Train Epoch: 1 [9600/143337] Loss: 0.714583 Acc: 0.7500\n",
      "Train Epoch: 1 [12800/143337] Loss: 0.386677 Acc: 0.9688\n",
      "Train Epoch: 1 [16000/143337] Loss: 0.311111 Acc: 0.9062\n",
      "Train Epoch: 1 [19200/143337] Loss: 0.452718 Acc: 0.8125\n",
      "Train Epoch: 1 [22400/143337] Loss: 0.472798 Acc: 0.8125\n",
      "Train Epoch: 1 [25600/143337] Loss: 0.259730 Acc: 0.9688\n",
      "Train Epoch: 1 [28800/143337] Loss: 0.311919 Acc: 0.8750\n",
      "Train Epoch: 1 [32000/143337] Loss: 0.365992 Acc: 0.8438\n",
      "Train Epoch: 1 [35200/143337] Loss: 0.290402 Acc: 0.9375\n",
      "Train Epoch: 1 [38400/143337] Loss: 0.399584 Acc: 0.8750\n",
      "Train Epoch: 1 [41600/143337] Loss: 0.366572 Acc: 0.8438\n",
      "Train Epoch: 1 [44800/143337] Loss: 0.371078 Acc: 0.8438\n",
      "Train Epoch: 1 [48000/143337] Loss: 0.179469 Acc: 0.9375\n",
      "Train Epoch: 1 [51200/143337] Loss: 0.479504 Acc: 0.8750\n",
      "Train Epoch: 1 [54400/143337] Loss: 0.161492 Acc: 1.0000\n",
      "Train Epoch: 1 [57600/143337] Loss: 0.561086 Acc: 0.8125\n",
      "Train Epoch: 1 [60800/143337] Loss: 0.359599 Acc: 0.8750\n",
      "Train Epoch: 1 [64000/143337] Loss: 0.162845 Acc: 0.9375\n",
      "Train Epoch: 1 [67200/143337] Loss: 0.489246 Acc: 0.8750\n",
      "Train Epoch: 1 [70400/143337] Loss: 0.356311 Acc: 0.9375\n",
      "Train Epoch: 1 [73600/143337] Loss: 0.489451 Acc: 0.9062\n",
      "Train Epoch: 1 [76800/143337] Loss: 0.239553 Acc: 0.9375\n",
      "Train Epoch: 1 [80000/143337] Loss: 0.697338 Acc: 0.8438\n",
      "Train Epoch: 1 [83200/143337] Loss: 0.163935 Acc: 1.0000\n",
      "Train Epoch: 1 [86400/143337] Loss: 0.695178 Acc: 0.8438\n",
      "Train Epoch: 1 [89600/143337] Loss: 0.455252 Acc: 0.7812\n",
      "Train Epoch: 1 [92800/143337] Loss: 0.464521 Acc: 0.8438\n",
      "Train Epoch: 1 [96000/143337] Loss: 0.694439 Acc: 0.7500\n",
      "Train Epoch: 1 [99200/143337] Loss: 0.400973 Acc: 0.9062\n",
      "Train Epoch: 1 [102400/143337] Loss: 0.387630 Acc: 0.8125\n",
      "Train Epoch: 1 [105600/143337] Loss: 0.387832 Acc: 0.9062\n",
      "Train Epoch: 1 [108800/143337] Loss: 0.388190 Acc: 0.8750\n",
      "Train Epoch: 1 [112000/143337] Loss: 0.648966 Acc: 0.7500\n",
      "Train Epoch: 1 [115200/143337] Loss: 0.415994 Acc: 0.8750\n",
      "Train Epoch: 1 [118400/143337] Loss: 0.132673 Acc: 0.9688\n",
      "Train Epoch: 1 [121600/143337] Loss: 0.530455 Acc: 0.8750\n",
      "Train Epoch: 1 [124800/143337] Loss: 0.219550 Acc: 0.9062\n",
      "Train Epoch: 1 [128000/143337] Loss: 0.506346 Acc: 0.8438\n",
      "Train Epoch: 1 [131200/143337] Loss: 0.284915 Acc: 0.8750\n",
      "Train Epoch: 1 [134400/143337] Loss: 0.260158 Acc: 0.9375\n",
      "Train Epoch: 1 [137600/143337] Loss: 0.210011 Acc: 0.9062\n",
      "Train Epoch: 1 [140800/143337] Loss: 0.266160 Acc: 0.9062\n",
      "Elapsed 82.98s, 41.49 s/epoch, 0.01 s/batch, ets 746.86s\n",
      "\n",
      "Test set: Average loss: 0.4365, Accuracy: 32777/37712 (87%)\n",
      "\n",
      "Train Epoch: 2 [3200/143337] Loss: 0.249055 Acc: 0.9375\n",
      "Train Epoch: 2 [6400/143337] Loss: 0.390143 Acc: 0.9062\n",
      "Train Epoch: 2 [9600/143337] Loss: 0.343121 Acc: 0.8750\n",
      "Train Epoch: 2 [12800/143337] Loss: 0.281189 Acc: 0.9375\n",
      "Train Epoch: 2 [16000/143337] Loss: 0.354268 Acc: 0.8750\n",
      "Train Epoch: 2 [19200/143337] Loss: 0.215236 Acc: 0.9375\n",
      "Train Epoch: 2 [22400/143337] Loss: 0.175330 Acc: 0.9375\n",
      "Train Epoch: 2 [25600/143337] Loss: 0.281191 Acc: 0.8750\n",
      "Train Epoch: 2 [28800/143337] Loss: 0.299556 Acc: 0.9062\n",
      "Train Epoch: 2 [32000/143337] Loss: 0.486423 Acc: 0.9062\n",
      "Train Epoch: 2 [35200/143337] Loss: 0.371697 Acc: 0.9375\n",
      "Train Epoch: 2 [38400/143337] Loss: 0.725418 Acc: 0.8750\n",
      "Train Epoch: 2 [41600/143337] Loss: 0.250097 Acc: 0.8750\n",
      "Train Epoch: 2 [44800/143337] Loss: 0.105555 Acc: 1.0000\n",
      "Train Epoch: 2 [48000/143337] Loss: 0.661520 Acc: 0.8125\n",
      "Train Epoch: 2 [51200/143337] Loss: 0.320321 Acc: 0.9062\n",
      "Train Epoch: 2 [54400/143337] Loss: 0.134604 Acc: 0.9688\n",
      "Train Epoch: 2 [57600/143337] Loss: 0.324751 Acc: 0.9375\n",
      "Train Epoch: 2 [60800/143337] Loss: 0.255559 Acc: 0.9062\n",
      "Train Epoch: 2 [64000/143337] Loss: 0.204568 Acc: 0.9375\n",
      "Train Epoch: 2 [67200/143337] Loss: 0.407723 Acc: 0.8438\n",
      "Train Epoch: 2 [70400/143337] Loss: 0.419170 Acc: 0.8438\n",
      "Train Epoch: 2 [73600/143337] Loss: 0.207413 Acc: 0.9375\n",
      "Train Epoch: 2 [76800/143337] Loss: 0.214558 Acc: 0.9375\n",
      "Train Epoch: 2 [80000/143337] Loss: 0.248061 Acc: 0.9062\n",
      "Train Epoch: 2 [83200/143337] Loss: 0.431193 Acc: 0.9062\n",
      "Train Epoch: 2 [86400/143337] Loss: 0.219035 Acc: 0.9375\n",
      "Train Epoch: 2 [89600/143337] Loss: 0.138420 Acc: 0.9375\n",
      "Train Epoch: 2 [92800/143337] Loss: 0.126320 Acc: 0.9688\n",
      "Train Epoch: 2 [96000/143337] Loss: 0.379122 Acc: 0.8750\n",
      "Train Epoch: 2 [99200/143337] Loss: 0.194822 Acc: 0.9688\n",
      "Train Epoch: 2 [102400/143337] Loss: 0.370768 Acc: 0.8750\n",
      "Train Epoch: 2 [105600/143337] Loss: 0.297036 Acc: 0.8750\n",
      "Train Epoch: 2 [108800/143337] Loss: 0.173442 Acc: 0.9688\n",
      "Train Epoch: 2 [112000/143337] Loss: 0.232268 Acc: 0.9375\n",
      "Train Epoch: 2 [115200/143337] Loss: 0.306688 Acc: 0.9062\n",
      "Train Epoch: 2 [118400/143337] Loss: 0.259581 Acc: 0.8750\n",
      "Train Epoch: 2 [121600/143337] Loss: 0.485695 Acc: 0.8125\n",
      "Train Epoch: 2 [124800/143337] Loss: 0.186461 Acc: 0.9688\n",
      "Train Epoch: 2 [128000/143337] Loss: 0.232193 Acc: 0.9688\n",
      "Train Epoch: 2 [131200/143337] Loss: 0.323051 Acc: 0.9062\n",
      "Train Epoch: 2 [134400/143337] Loss: 0.220528 Acc: 0.9062\n",
      "Train Epoch: 2 [137600/143337] Loss: 0.216917 Acc: 0.8750\n",
      "Train Epoch: 2 [140800/143337] Loss: 0.587068 Acc: 0.8438\n",
      "Elapsed 128.25s, 42.75 s/epoch, 0.01 s/batch, ets 726.74s\n",
      "\n",
      "Test set: Average loss: 0.3839, Accuracy: 33464/37712 (89%)\n",
      "\n",
      "Train Epoch: 3 [3200/143337] Loss: 0.278809 Acc: 0.9062\n",
      "Train Epoch: 3 [6400/143337] Loss: 0.281108 Acc: 0.8438\n",
      "Train Epoch: 3 [9600/143337] Loss: 0.216446 Acc: 0.9062\n",
      "Train Epoch: 3 [12800/143337] Loss: 0.182468 Acc: 0.9375\n",
      "Train Epoch: 3 [16000/143337] Loss: 0.139216 Acc: 0.9375\n",
      "Train Epoch: 3 [19200/143337] Loss: 0.124981 Acc: 0.9375\n",
      "Train Epoch: 3 [22400/143337] Loss: 0.358548 Acc: 0.8125\n",
      "Train Epoch: 3 [25600/143337] Loss: 0.325196 Acc: 0.9062\n",
      "Train Epoch: 3 [28800/143337] Loss: 0.205032 Acc: 0.9375\n",
      "Train Epoch: 3 [32000/143337] Loss: 0.235703 Acc: 0.9062\n",
      "Train Epoch: 3 [35200/143337] Loss: 0.428935 Acc: 0.7500\n",
      "Train Epoch: 3 [38400/143337] Loss: 0.201658 Acc: 0.9688\n",
      "Train Epoch: 3 [41600/143337] Loss: 0.247431 Acc: 0.8750\n",
      "Train Epoch: 3 [44800/143337] Loss: 0.195038 Acc: 0.9688\n",
      "Train Epoch: 3 [48000/143337] Loss: 0.164483 Acc: 0.9375\n",
      "Train Epoch: 3 [51200/143337] Loss: 0.730304 Acc: 0.8438\n",
      "Train Epoch: 3 [54400/143337] Loss: 0.273374 Acc: 0.9062\n",
      "Train Epoch: 3 [57600/143337] Loss: 0.342739 Acc: 0.8750\n",
      "Train Epoch: 3 [60800/143337] Loss: 0.276340 Acc: 0.9375\n",
      "Train Epoch: 3 [64000/143337] Loss: 0.186920 Acc: 0.9375\n",
      "Train Epoch: 3 [67200/143337] Loss: 0.427140 Acc: 0.8750\n",
      "Train Epoch: 3 [70400/143337] Loss: 0.247761 Acc: 0.9062\n",
      "Train Epoch: 3 [73600/143337] Loss: 0.202973 Acc: 0.9375\n",
      "Train Epoch: 3 [76800/143337] Loss: 0.236458 Acc: 0.9375\n",
      "Train Epoch: 3 [80000/143337] Loss: 0.033396 Acc: 1.0000\n",
      "Train Epoch: 3 [83200/143337] Loss: 0.207792 Acc: 0.9062\n",
      "Train Epoch: 3 [86400/143337] Loss: 0.118207 Acc: 0.9688\n",
      "Train Epoch: 3 [89600/143337] Loss: 0.343506 Acc: 0.9062\n",
      "Train Epoch: 3 [92800/143337] Loss: 0.121886 Acc: 1.0000\n",
      "Train Epoch: 3 [96000/143337] Loss: 0.346224 Acc: 0.9375\n",
      "Train Epoch: 3 [99200/143337] Loss: 0.212601 Acc: 0.9375\n",
      "Train Epoch: 3 [102400/143337] Loss: 0.196107 Acc: 0.9688\n",
      "Train Epoch: 3 [105600/143337] Loss: 0.213040 Acc: 0.9375\n",
      "Train Epoch: 3 [108800/143337] Loss: 0.332817 Acc: 0.9688\n",
      "Train Epoch: 3 [112000/143337] Loss: 0.404118 Acc: 0.9375\n",
      "Train Epoch: 3 [115200/143337] Loss: 0.130586 Acc: 0.9688\n",
      "Train Epoch: 3 [118400/143337] Loss: 0.086823 Acc: 0.9688\n",
      "Train Epoch: 3 [121600/143337] Loss: 0.313868 Acc: 0.9062\n",
      "Train Epoch: 3 [124800/143337] Loss: 0.095363 Acc: 0.9688\n",
      "Train Epoch: 3 [128000/143337] Loss: 0.230336 Acc: 0.9375\n",
      "Train Epoch: 3 [131200/143337] Loss: 0.160858 Acc: 0.9688\n",
      "Train Epoch: 3 [134400/143337] Loss: 0.084108 Acc: 0.9688\n",
      "Train Epoch: 3 [137600/143337] Loss: 0.250258 Acc: 0.9375\n",
      "Train Epoch: 3 [140800/143337] Loss: 0.252573 Acc: 0.9062\n",
      "Elapsed 172.98s, 43.25 s/epoch, 0.01 s/batch, ets 691.93s\n",
      "\n",
      "Test set: Average loss: 0.3709, Accuracy: 33631/37712 (89%)\n",
      "\n",
      "Train Epoch: 4 [3200/143337] Loss: 0.235688 Acc: 0.9375\n",
      "Train Epoch: 4 [6400/143337] Loss: 0.317835 Acc: 0.8750\n",
      "Train Epoch: 4 [9600/143337] Loss: 0.154119 Acc: 0.9375\n",
      "Train Epoch: 4 [12800/143337] Loss: 0.576041 Acc: 0.8438\n",
      "Train Epoch: 4 [16000/143337] Loss: 0.583126 Acc: 0.8438\n",
      "Train Epoch: 4 [19200/143337] Loss: 0.367406 Acc: 0.8750\n",
      "Train Epoch: 4 [22400/143337] Loss: 0.162326 Acc: 0.9688\n",
      "Train Epoch: 4 [25600/143337] Loss: 0.302913 Acc: 0.8750\n",
      "Train Epoch: 4 [28800/143337] Loss: 0.281850 Acc: 0.9062\n",
      "Train Epoch: 4 [32000/143337] Loss: 0.168450 Acc: 0.9062\n",
      "Train Epoch: 4 [35200/143337] Loss: 0.051524 Acc: 1.0000\n",
      "Train Epoch: 4 [38400/143337] Loss: 0.186078 Acc: 0.9688\n",
      "Train Epoch: 4 [41600/143337] Loss: 0.458041 Acc: 0.9375\n",
      "Train Epoch: 4 [44800/143337] Loss: 0.426915 Acc: 0.9062\n",
      "Train Epoch: 4 [48000/143337] Loss: 0.280697 Acc: 0.9062\n",
      "Train Epoch: 4 [51200/143337] Loss: 0.297430 Acc: 0.9375\n",
      "Train Epoch: 4 [54400/143337] Loss: 0.197083 Acc: 0.8750\n",
      "Train Epoch: 4 [57600/143337] Loss: 0.277525 Acc: 0.9062\n",
      "Train Epoch: 4 [60800/143337] Loss: 0.174915 Acc: 0.9375\n",
      "Train Epoch: 4 [64000/143337] Loss: 0.583938 Acc: 0.8438\n",
      "Train Epoch: 4 [67200/143337] Loss: 0.330762 Acc: 0.9062\n",
      "Train Epoch: 4 [70400/143337] Loss: 0.284617 Acc: 0.9062\n",
      "Train Epoch: 4 [73600/143337] Loss: 0.401666 Acc: 0.9062\n",
      "Train Epoch: 4 [76800/143337] Loss: 0.216140 Acc: 0.9375\n",
      "Train Epoch: 4 [80000/143337] Loss: 0.322068 Acc: 0.9062\n",
      "Train Epoch: 4 [83200/143337] Loss: 0.393921 Acc: 0.9062\n",
      "Train Epoch: 4 [86400/143337] Loss: 0.289714 Acc: 0.8750\n",
      "Train Epoch: 4 [89600/143337] Loss: 0.421192 Acc: 0.8750\n",
      "Train Epoch: 4 [92800/143337] Loss: 0.434177 Acc: 0.9062\n",
      "Train Epoch: 4 [96000/143337] Loss: 0.186383 Acc: 0.9688\n",
      "Train Epoch: 4 [99200/143337] Loss: 0.096682 Acc: 1.0000\n",
      "Train Epoch: 4 [102400/143337] Loss: 0.159722 Acc: 0.9375\n",
      "Train Epoch: 4 [105600/143337] Loss: 0.423184 Acc: 0.8750\n",
      "Train Epoch: 4 [108800/143337] Loss: 0.135436 Acc: 0.9688\n",
      "Train Epoch: 4 [112000/143337] Loss: 0.204369 Acc: 0.9375\n",
      "Train Epoch: 4 [115200/143337] Loss: 0.316482 Acc: 0.9062\n",
      "Train Epoch: 4 [118400/143337] Loss: 0.186528 Acc: 0.9062\n",
      "Train Epoch: 4 [121600/143337] Loss: 0.034981 Acc: 1.0000\n",
      "Train Epoch: 4 [124800/143337] Loss: 0.077277 Acc: 0.9688\n",
      "Train Epoch: 4 [128000/143337] Loss: 0.246492 Acc: 0.9375\n",
      "Train Epoch: 4 [131200/143337] Loss: 0.180476 Acc: 0.9375\n",
      "Train Epoch: 4 [134400/143337] Loss: 0.302912 Acc: 0.8750\n",
      "Train Epoch: 4 [137600/143337] Loss: 0.138404 Acc: 0.9688\n",
      "Train Epoch: 4 [140800/143337] Loss: 0.448766 Acc: 0.8750\n",
      "Elapsed 217.17s, 43.43 s/epoch, 0.01 s/batch, ets 651.52s\n",
      "\n",
      "Test set: Average loss: 0.3418, Accuracy: 33968/37712 (90%)\n",
      "\n",
      "Train Epoch: 5 [3200/143337] Loss: 0.272992 Acc: 0.9062\n",
      "Train Epoch: 5 [6400/143337] Loss: 0.151036 Acc: 0.9688\n",
      "Train Epoch: 5 [9600/143337] Loss: 0.194146 Acc: 0.9375\n",
      "Train Epoch: 5 [12800/143337] Loss: 0.043166 Acc: 1.0000\n",
      "Train Epoch: 5 [16000/143337] Loss: 0.121107 Acc: 0.9688\n",
      "Train Epoch: 5 [19200/143337] Loss: 0.447338 Acc: 0.8125\n",
      "Train Epoch: 5 [22400/143337] Loss: 0.225954 Acc: 0.9062\n",
      "Train Epoch: 5 [25600/143337] Loss: 0.252253 Acc: 0.9688\n",
      "Train Epoch: 5 [28800/143337] Loss: 0.140228 Acc: 0.9375\n",
      "Train Epoch: 5 [32000/143337] Loss: 0.341716 Acc: 0.9375\n",
      "Train Epoch: 5 [35200/143337] Loss: 0.493571 Acc: 0.8750\n",
      "Train Epoch: 5 [38400/143337] Loss: 0.165249 Acc: 0.9688\n",
      "Train Epoch: 5 [41600/143337] Loss: 0.022861 Acc: 1.0000\n",
      "Train Epoch: 5 [44800/143337] Loss: 0.180777 Acc: 0.9062\n",
      "Train Epoch: 5 [48000/143337] Loss: 0.182979 Acc: 0.9688\n",
      "Train Epoch: 5 [51200/143337] Loss: 0.387011 Acc: 0.9062\n",
      "Train Epoch: 5 [54400/143337] Loss: 0.074498 Acc: 0.9688\n",
      "Train Epoch: 5 [57600/143337] Loss: 0.261564 Acc: 0.9062\n",
      "Train Epoch: 5 [60800/143337] Loss: 0.239182 Acc: 0.9062\n",
      "Train Epoch: 5 [64000/143337] Loss: 0.139962 Acc: 0.9688\n",
      "Train Epoch: 5 [67200/143337] Loss: 0.063177 Acc: 0.9688\n",
      "Train Epoch: 5 [70400/143337] Loss: 0.376311 Acc: 0.8750\n",
      "Train Epoch: 5 [73600/143337] Loss: 0.112925 Acc: 0.9375\n",
      "Train Epoch: 5 [76800/143337] Loss: 0.123130 Acc: 0.9688\n",
      "Train Epoch: 5 [80000/143337] Loss: 0.290700 Acc: 0.9062\n",
      "Train Epoch: 5 [83200/143337] Loss: 0.107074 Acc: 0.9688\n",
      "Train Epoch: 5 [86400/143337] Loss: 0.214302 Acc: 0.9375\n",
      "Train Epoch: 5 [89600/143337] Loss: 0.123433 Acc: 0.9688\n",
      "Train Epoch: 5 [92800/143337] Loss: 0.122285 Acc: 0.9375\n",
      "Train Epoch: 5 [96000/143337] Loss: 0.174813 Acc: 0.9375\n",
      "Train Epoch: 5 [99200/143337] Loss: 0.112240 Acc: 0.9375\n",
      "Train Epoch: 5 [102400/143337] Loss: 0.305302 Acc: 0.9062\n",
      "Train Epoch: 5 [105600/143337] Loss: 0.415312 Acc: 0.8750\n",
      "Train Epoch: 5 [108800/143337] Loss: 0.395547 Acc: 0.9375\n",
      "Train Epoch: 5 [112000/143337] Loss: 0.067853 Acc: 0.9688\n",
      "Train Epoch: 5 [115200/143337] Loss: 0.078440 Acc: 0.9688\n",
      "Train Epoch: 5 [118400/143337] Loss: 0.167399 Acc: 0.9688\n",
      "Train Epoch: 5 [121600/143337] Loss: 0.306438 Acc: 0.8438\n",
      "Train Epoch: 5 [124800/143337] Loss: 0.257632 Acc: 0.9062\n",
      "Train Epoch: 5 [128000/143337] Loss: 0.187375 Acc: 0.9375\n",
      "Train Epoch: 5 [131200/143337] Loss: 0.368388 Acc: 0.9375\n",
      "Train Epoch: 5 [134400/143337] Loss: 0.256150 Acc: 0.8750\n",
      "Train Epoch: 5 [137600/143337] Loss: 0.167808 Acc: 0.9688\n",
      "Train Epoch: 5 [140800/143337] Loss: 0.123596 Acc: 0.9688\n",
      "Elapsed 261.26s, 43.54 s/epoch, 0.01 s/batch, ets 609.61s\n",
      "\n",
      "Test set: Average loss: 0.3345, Accuracy: 34025/37712 (90%)\n",
      "\n",
      "Train Epoch: 6 [3200/143337] Loss: 0.171353 Acc: 0.9375\n",
      "Train Epoch: 6 [6400/143337] Loss: 0.073742 Acc: 0.9688\n",
      "Train Epoch: 6 [9600/143337] Loss: 0.124047 Acc: 0.9688\n",
      "Train Epoch: 6 [12800/143337] Loss: 0.050118 Acc: 1.0000\n",
      "Train Epoch: 6 [16000/143337] Loss: 0.473811 Acc: 0.9062\n",
      "Train Epoch: 6 [19200/143337] Loss: 0.734520 Acc: 0.9062\n",
      "Train Epoch: 6 [22400/143337] Loss: 0.153642 Acc: 0.9375\n",
      "Train Epoch: 6 [25600/143337] Loss: 0.086565 Acc: 1.0000\n",
      "Train Epoch: 6 [28800/143337] Loss: 0.197846 Acc: 0.9688\n",
      "Train Epoch: 6 [32000/143337] Loss: 0.206533 Acc: 0.9375\n",
      "Train Epoch: 6 [35200/143337] Loss: 0.108319 Acc: 0.9688\n",
      "Train Epoch: 6 [38400/143337] Loss: 0.276867 Acc: 0.9375\n",
      "Train Epoch: 6 [41600/143337] Loss: 0.442010 Acc: 0.9062\n",
      "Train Epoch: 6 [44800/143337] Loss: 0.260517 Acc: 0.9688\n",
      "Train Epoch: 6 [48000/143337] Loss: 0.065594 Acc: 0.9688\n",
      "Train Epoch: 6 [51200/143337] Loss: 0.185375 Acc: 0.9688\n",
      "Train Epoch: 6 [54400/143337] Loss: 0.057445 Acc: 0.9688\n",
      "Train Epoch: 6 [57600/143337] Loss: 0.410627 Acc: 0.9062\n",
      "Train Epoch: 6 [60800/143337] Loss: 0.458426 Acc: 0.8438\n",
      "Train Epoch: 6 [64000/143337] Loss: 0.132801 Acc: 0.9375\n",
      "Train Epoch: 6 [67200/143337] Loss: 0.050274 Acc: 1.0000\n",
      "Train Epoch: 6 [70400/143337] Loss: 0.222360 Acc: 0.9062\n",
      "Train Epoch: 6 [73600/143337] Loss: 0.198465 Acc: 0.9062\n",
      "Train Epoch: 6 [76800/143337] Loss: 0.165953 Acc: 0.9375\n",
      "Train Epoch: 6 [80000/143337] Loss: 0.130291 Acc: 0.9688\n",
      "Train Epoch: 6 [83200/143337] Loss: 0.161479 Acc: 0.9688\n",
      "Train Epoch: 6 [86400/143337] Loss: 0.438456 Acc: 0.9375\n",
      "Train Epoch: 6 [89600/143337] Loss: 0.209813 Acc: 0.9062\n",
      "Train Epoch: 6 [92800/143337] Loss: 0.023763 Acc: 1.0000\n",
      "Train Epoch: 6 [96000/143337] Loss: 0.065830 Acc: 1.0000\n",
      "Train Epoch: 6 [99200/143337] Loss: 0.151347 Acc: 0.9688\n",
      "Train Epoch: 6 [102400/143337] Loss: 0.217297 Acc: 0.9688\n",
      "Train Epoch: 6 [105600/143337] Loss: 0.088349 Acc: 0.9688\n",
      "Train Epoch: 6 [108800/143337] Loss: 0.210677 Acc: 0.9062\n",
      "Train Epoch: 6 [112000/143337] Loss: 0.741883 Acc: 0.9062\n",
      "Train Epoch: 6 [115200/143337] Loss: 0.121048 Acc: 0.9688\n",
      "Train Epoch: 6 [118400/143337] Loss: 0.232510 Acc: 0.9375\n",
      "Train Epoch: 6 [121600/143337] Loss: 0.194930 Acc: 0.9375\n",
      "Train Epoch: 6 [124800/143337] Loss: 0.150007 Acc: 0.9375\n",
      "Train Epoch: 6 [128000/143337] Loss: 0.048757 Acc: 0.9688\n",
      "Train Epoch: 6 [131200/143337] Loss: 0.317550 Acc: 0.8750\n",
      "Train Epoch: 6 [134400/143337] Loss: 0.215814 Acc: 0.9688\n",
      "Train Epoch: 6 [137600/143337] Loss: 0.319642 Acc: 0.8438\n",
      "Train Epoch: 6 [140800/143337] Loss: 0.175623 Acc: 0.9375\n",
      "Elapsed 305.09s, 43.58 s/epoch, 0.01 s/batch, ets 566.59s\n",
      "\n",
      "Test set: Average loss: 0.3229, Accuracy: 34187/37712 (91%)\n",
      "\n",
      "Train Epoch: 7 [3200/143337] Loss: 0.080696 Acc: 0.9688\n",
      "Train Epoch: 7 [6400/143337] Loss: 0.233586 Acc: 0.8750\n",
      "Train Epoch: 7 [9600/143337] Loss: 0.174726 Acc: 0.9375\n",
      "Train Epoch: 7 [12800/143337] Loss: 0.132292 Acc: 0.9375\n",
      "Train Epoch: 7 [16000/143337] Loss: 0.124870 Acc: 0.9375\n",
      "Train Epoch: 7 [19200/143337] Loss: 0.156980 Acc: 0.9375\n",
      "Train Epoch: 7 [22400/143337] Loss: 0.222116 Acc: 0.9375\n",
      "Train Epoch: 7 [25600/143337] Loss: 0.307509 Acc: 0.9062\n",
      "Train Epoch: 7 [28800/143337] Loss: 0.302073 Acc: 0.9062\n",
      "Train Epoch: 7 [32000/143337] Loss: 0.092591 Acc: 1.0000\n",
      "Train Epoch: 7 [35200/143337] Loss: 0.344814 Acc: 0.8750\n",
      "Train Epoch: 7 [38400/143337] Loss: 0.094700 Acc: 0.9375\n",
      "Train Epoch: 7 [41600/143337] Loss: 0.688685 Acc: 0.8125\n",
      "Train Epoch: 7 [44800/143337] Loss: 0.414422 Acc: 0.8750\n",
      "Train Epoch: 7 [48000/143337] Loss: 0.210493 Acc: 0.9062\n",
      "Train Epoch: 7 [51200/143337] Loss: 0.247218 Acc: 0.9375\n",
      "Train Epoch: 7 [54400/143337] Loss: 0.215996 Acc: 0.9375\n",
      "Train Epoch: 7 [57600/143337] Loss: 0.085559 Acc: 1.0000\n",
      "Train Epoch: 7 [60800/143337] Loss: 0.074355 Acc: 1.0000\n",
      "Train Epoch: 7 [64000/143337] Loss: 0.195261 Acc: 0.9375\n",
      "Train Epoch: 7 [67200/143337] Loss: 0.198117 Acc: 0.9375\n",
      "Train Epoch: 7 [70400/143337] Loss: 0.063446 Acc: 1.0000\n",
      "Train Epoch: 7 [73600/143337] Loss: 0.208634 Acc: 0.9688\n",
      "Train Epoch: 7 [76800/143337] Loss: 0.185176 Acc: 1.0000\n",
      "Train Epoch: 7 [80000/143337] Loss: 0.186488 Acc: 0.9375\n",
      "Train Epoch: 7 [83200/143337] Loss: 0.159869 Acc: 0.9375\n",
      "Train Epoch: 7 [86400/143337] Loss: 0.267895 Acc: 0.9062\n",
      "Train Epoch: 7 [89600/143337] Loss: 0.249802 Acc: 0.9062\n",
      "Train Epoch: 7 [92800/143337] Loss: 0.170188 Acc: 0.9375\n",
      "Train Epoch: 7 [96000/143337] Loss: 0.246875 Acc: 0.9375\n",
      "Train Epoch: 7 [99200/143337] Loss: 0.234038 Acc: 0.9688\n",
      "Train Epoch: 7 [102400/143337] Loss: 0.073706 Acc: 0.9688\n",
      "Train Epoch: 7 [105600/143337] Loss: 0.126952 Acc: 0.9688\n",
      "Train Epoch: 7 [108800/143337] Loss: 0.380705 Acc: 0.8750\n",
      "Train Epoch: 7 [112000/143337] Loss: 0.128570 Acc: 0.9375\n",
      "Train Epoch: 7 [115200/143337] Loss: 0.207639 Acc: 0.9375\n",
      "Train Epoch: 7 [118400/143337] Loss: 0.171002 Acc: 0.9375\n",
      "Train Epoch: 7 [121600/143337] Loss: 0.139503 Acc: 0.9375\n",
      "Train Epoch: 7 [124800/143337] Loss: 0.064387 Acc: 1.0000\n",
      "Train Epoch: 7 [128000/143337] Loss: 0.210548 Acc: 0.9688\n",
      "Train Epoch: 7 [131200/143337] Loss: 0.170821 Acc: 0.9375\n",
      "Train Epoch: 7 [134400/143337] Loss: 0.174272 Acc: 0.9375\n",
      "Train Epoch: 7 [137600/143337] Loss: 0.212357 Acc: 0.9375\n",
      "Train Epoch: 7 [140800/143337] Loss: 0.190898 Acc: 0.9375\n",
      "Elapsed 348.73s, 43.59 s/epoch, 0.01 s/batch, ets 523.10s\n",
      "\n",
      "Test set: Average loss: 0.2972, Accuracy: 34480/37712 (91%)\n",
      "\n",
      "Train Epoch: 8 [3200/143337] Loss: 0.310146 Acc: 0.9062\n",
      "Train Epoch: 8 [6400/143337] Loss: 0.041035 Acc: 1.0000\n",
      "Train Epoch: 8 [9600/143337] Loss: 0.258758 Acc: 0.9062\n",
      "Train Epoch: 8 [12800/143337] Loss: 0.225088 Acc: 0.8750\n",
      "Train Epoch: 8 [16000/143337] Loss: 0.077609 Acc: 1.0000\n",
      "Train Epoch: 8 [19200/143337] Loss: 0.298093 Acc: 0.9375\n",
      "Train Epoch: 8 [22400/143337] Loss: 0.157101 Acc: 0.9688\n",
      "Train Epoch: 8 [25600/143337] Loss: 0.172687 Acc: 0.9688\n",
      "Train Epoch: 8 [28800/143337] Loss: 0.315969 Acc: 0.9062\n",
      "Train Epoch: 8 [32000/143337] Loss: 0.180175 Acc: 0.9375\n",
      "Train Epoch: 8 [35200/143337] Loss: 0.219982 Acc: 0.9062\n",
      "Train Epoch: 8 [38400/143337] Loss: 0.153134 Acc: 0.9375\n",
      "Train Epoch: 8 [41600/143337] Loss: 0.106106 Acc: 0.9688\n",
      "Train Epoch: 8 [44800/143337] Loss: 0.084077 Acc: 0.9688\n",
      "Train Epoch: 8 [48000/143337] Loss: 0.063165 Acc: 1.0000\n",
      "Train Epoch: 8 [51200/143337] Loss: 0.379199 Acc: 0.9062\n",
      "Train Epoch: 8 [54400/143337] Loss: 0.150027 Acc: 0.9375\n",
      "Train Epoch: 8 [57600/143337] Loss: 0.290584 Acc: 0.9375\n",
      "Train Epoch: 8 [60800/143337] Loss: 0.231127 Acc: 0.9062\n",
      "Train Epoch: 8 [64000/143337] Loss: 0.239858 Acc: 0.9062\n",
      "Train Epoch: 8 [67200/143337] Loss: 0.077471 Acc: 1.0000\n",
      "Train Epoch: 8 [70400/143337] Loss: 0.122168 Acc: 0.9375\n",
      "Train Epoch: 8 [73600/143337] Loss: 0.084393 Acc: 0.9688\n",
      "Train Epoch: 8 [76800/143337] Loss: 0.088444 Acc: 0.9688\n",
      "Train Epoch: 8 [80000/143337] Loss: 0.231258 Acc: 0.9062\n",
      "Train Epoch: 8 [83200/143337] Loss: 0.081510 Acc: 0.9688\n",
      "Train Epoch: 8 [86400/143337] Loss: 0.024526 Acc: 1.0000\n",
      "Train Epoch: 8 [89600/143337] Loss: 0.306041 Acc: 0.8750\n",
      "Train Epoch: 8 [92800/143337] Loss: 0.038690 Acc: 0.9688\n",
      "Train Epoch: 8 [96000/143337] Loss: 0.147375 Acc: 0.9375\n",
      "Train Epoch: 8 [99200/143337] Loss: 0.281506 Acc: 0.9375\n",
      "Train Epoch: 8 [102400/143337] Loss: 0.229685 Acc: 0.9062\n",
      "Train Epoch: 8 [105600/143337] Loss: 0.167053 Acc: 0.9375\n",
      "Train Epoch: 8 [108800/143337] Loss: 0.608005 Acc: 0.9375\n",
      "Train Epoch: 8 [112000/143337] Loss: 0.043518 Acc: 1.0000\n",
      "Train Epoch: 8 [115200/143337] Loss: 0.171566 Acc: 0.9375\n",
      "Train Epoch: 8 [118400/143337] Loss: 0.241442 Acc: 0.9062\n",
      "Train Epoch: 8 [121600/143337] Loss: 0.210114 Acc: 0.9062\n",
      "Train Epoch: 8 [124800/143337] Loss: 0.214036 Acc: 0.9062\n",
      "Train Epoch: 8 [128000/143337] Loss: 0.150719 Acc: 0.9375\n",
      "Train Epoch: 8 [131200/143337] Loss: 0.232817 Acc: 0.9375\n",
      "Train Epoch: 8 [134400/143337] Loss: 0.340468 Acc: 0.8438\n",
      "Train Epoch: 8 [137600/143337] Loss: 0.282166 Acc: 0.9375\n",
      "Train Epoch: 8 [140800/143337] Loss: 0.144899 Acc: 0.9688\n",
      "Elapsed 392.32s, 43.59 s/epoch, 0.01 s/batch, ets 479.50s\n",
      "\n",
      "Test set: Average loss: 0.3214, Accuracy: 34216/37712 (91%)\n",
      "\n",
      "Train Epoch: 9 [3200/143337] Loss: 0.165004 Acc: 0.9688\n",
      "Train Epoch: 9 [6400/143337] Loss: 0.066569 Acc: 0.9688\n",
      "Train Epoch: 9 [9600/143337] Loss: 0.160567 Acc: 0.9375\n",
      "Train Epoch: 9 [12800/143337] Loss: 0.037262 Acc: 1.0000\n",
      "Train Epoch: 9 [16000/143337] Loss: 0.045758 Acc: 1.0000\n",
      "Train Epoch: 9 [19200/143337] Loss: 0.136069 Acc: 0.9688\n",
      "Train Epoch: 9 [22400/143337] Loss: 0.365901 Acc: 0.8438\n",
      "Train Epoch: 9 [25600/143337] Loss: 0.060544 Acc: 1.0000\n",
      "Train Epoch: 9 [28800/143337] Loss: 0.367338 Acc: 0.9375\n",
      "Train Epoch: 9 [32000/143337] Loss: 0.301455 Acc: 0.9688\n",
      "Train Epoch: 9 [35200/143337] Loss: 0.297889 Acc: 0.8750\n",
      "Train Epoch: 9 [38400/143337] Loss: 0.099652 Acc: 0.9688\n",
      "Train Epoch: 9 [41600/143337] Loss: 0.066260 Acc: 0.9688\n",
      "Train Epoch: 9 [44800/143337] Loss: 0.207606 Acc: 0.8750\n",
      "Train Epoch: 9 [48000/143337] Loss: 0.074601 Acc: 0.9688\n",
      "Train Epoch: 9 [51200/143337] Loss: 0.098485 Acc: 0.9688\n",
      "Train Epoch: 9 [54400/143337] Loss: 0.229641 Acc: 0.9062\n",
      "Train Epoch: 9 [57600/143337] Loss: 0.041116 Acc: 1.0000\n",
      "Train Epoch: 9 [60800/143337] Loss: 0.155504 Acc: 0.9688\n",
      "Train Epoch: 9 [64000/143337] Loss: 0.114666 Acc: 0.9688\n",
      "Train Epoch: 9 [67200/143337] Loss: 0.711887 Acc: 0.9062\n",
      "Train Epoch: 9 [70400/143337] Loss: 0.217675 Acc: 0.9375\n",
      "Train Epoch: 9 [73600/143337] Loss: 0.398875 Acc: 0.9062\n",
      "Train Epoch: 9 [76800/143337] Loss: 0.118478 Acc: 0.9688\n",
      "Train Epoch: 9 [80000/143337] Loss: 0.250969 Acc: 0.9375\n",
      "Train Epoch: 9 [83200/143337] Loss: 0.245765 Acc: 0.9375\n",
      "Train Epoch: 9 [86400/143337] Loss: 0.429988 Acc: 0.8750\n",
      "Train Epoch: 9 [89600/143337] Loss: 0.164616 Acc: 0.9375\n",
      "Train Epoch: 9 [92800/143337] Loss: 0.239092 Acc: 0.9062\n",
      "Train Epoch: 9 [96000/143337] Loss: 0.066303 Acc: 1.0000\n",
      "Train Epoch: 9 [99200/143337] Loss: 0.210114 Acc: 0.9062\n",
      "Train Epoch: 9 [102400/143337] Loss: 0.148845 Acc: 0.9062\n",
      "Train Epoch: 9 [105600/143337] Loss: 0.270286 Acc: 0.8750\n",
      "Train Epoch: 9 [108800/143337] Loss: 0.176885 Acc: 0.9375\n",
      "Train Epoch: 9 [112000/143337] Loss: 0.231259 Acc: 0.9062\n",
      "Train Epoch: 9 [115200/143337] Loss: 0.080549 Acc: 0.9688\n",
      "Train Epoch: 9 [118400/143337] Loss: 0.198974 Acc: 0.9375\n",
      "Train Epoch: 9 [121600/143337] Loss: 0.160564 Acc: 0.9375\n",
      "Train Epoch: 9 [124800/143337] Loss: 0.213340 Acc: 0.9688\n",
      "Train Epoch: 9 [128000/143337] Loss: 0.249122 Acc: 0.9375\n",
      "Train Epoch: 9 [131200/143337] Loss: 0.033974 Acc: 1.0000\n",
      "Train Epoch: 9 [134400/143337] Loss: 0.483893 Acc: 0.8125\n",
      "Train Epoch: 9 [137600/143337] Loss: 0.034675 Acc: 1.0000\n",
      "Train Epoch: 9 [140800/143337] Loss: 0.091853 Acc: 0.9688\n",
      "Elapsed 436.05s, 43.60 s/epoch, 0.01 s/batch, ets 436.05s\n",
      "\n",
      "Test set: Average loss: 0.2718, Accuracy: 34761/37712 (92%)\n",
      "\n",
      "Train Epoch: 10 [3200/143337] Loss: 0.366634 Acc: 0.9062\n",
      "Train Epoch: 10 [6400/143337] Loss: 0.085457 Acc: 1.0000\n",
      "Train Epoch: 10 [9600/143337] Loss: 0.081944 Acc: 0.9688\n",
      "Train Epoch: 10 [12800/143337] Loss: 0.211795 Acc: 0.9062\n",
      "Train Epoch: 10 [16000/143337] Loss: 0.240481 Acc: 0.9375\n",
      "Train Epoch: 10 [19200/143337] Loss: 0.103640 Acc: 0.9688\n",
      "Train Epoch: 10 [22400/143337] Loss: 0.082095 Acc: 0.9688\n",
      "Train Epoch: 10 [25600/143337] Loss: 0.197718 Acc: 0.8750\n",
      "Train Epoch: 10 [28800/143337] Loss: 0.091008 Acc: 1.0000\n",
      "Train Epoch: 10 [32000/143337] Loss: 0.015303 Acc: 1.0000\n",
      "Train Epoch: 10 [35200/143337] Loss: 0.051909 Acc: 1.0000\n",
      "Train Epoch: 10 [38400/143337] Loss: 0.239104 Acc: 0.8750\n",
      "Train Epoch: 10 [41600/143337] Loss: 0.091470 Acc: 0.9688\n",
      "Train Epoch: 10 [44800/143337] Loss: 0.169285 Acc: 0.9688\n",
      "Train Epoch: 10 [48000/143337] Loss: 0.120942 Acc: 0.9688\n",
      "Train Epoch: 10 [51200/143337] Loss: 0.202196 Acc: 0.9062\n",
      "Train Epoch: 10 [54400/143337] Loss: 0.321245 Acc: 0.8750\n",
      "Train Epoch: 10 [57600/143337] Loss: 0.205770 Acc: 0.9688\n",
      "Train Epoch: 10 [60800/143337] Loss: 0.102431 Acc: 0.9688\n",
      "Train Epoch: 10 [64000/143337] Loss: 0.189352 Acc: 0.9375\n",
      "Train Epoch: 10 [67200/143337] Loss: 0.443074 Acc: 0.8750\n",
      "Train Epoch: 10 [70400/143337] Loss: 0.095368 Acc: 0.9688\n",
      "Train Epoch: 10 [73600/143337] Loss: 0.152721 Acc: 0.9062\n",
      "Train Epoch: 10 [76800/143337] Loss: 0.057158 Acc: 1.0000\n",
      "Train Epoch: 10 [80000/143337] Loss: 0.087590 Acc: 1.0000\n",
      "Train Epoch: 10 [83200/143337] Loss: 0.093832 Acc: 1.0000\n",
      "Train Epoch: 10 [86400/143337] Loss: 0.228038 Acc: 0.9062\n",
      "Train Epoch: 10 [89600/143337] Loss: 0.153236 Acc: 0.9688\n",
      "Train Epoch: 10 [92800/143337] Loss: 0.228670 Acc: 0.9062\n",
      "Train Epoch: 10 [96000/143337] Loss: 0.150335 Acc: 0.9688\n",
      "Train Epoch: 10 [99200/143337] Loss: 0.236672 Acc: 0.8750\n",
      "Train Epoch: 10 [102400/143337] Loss: 0.192705 Acc: 0.9375\n",
      "Train Epoch: 10 [105600/143337] Loss: 0.212576 Acc: 0.9375\n",
      "Train Epoch: 10 [108800/143337] Loss: 0.057166 Acc: 1.0000\n",
      "Train Epoch: 10 [112000/143337] Loss: 0.016930 Acc: 1.0000\n",
      "Train Epoch: 10 [115200/143337] Loss: 0.176965 Acc: 0.9688\n",
      "Train Epoch: 10 [118400/143337] Loss: 0.273768 Acc: 0.9062\n",
      "Train Epoch: 10 [121600/143337] Loss: 0.177401 Acc: 0.9375\n",
      "Train Epoch: 10 [124800/143337] Loss: 0.166379 Acc: 0.9688\n",
      "Train Epoch: 10 [128000/143337] Loss: 0.025313 Acc: 1.0000\n",
      "Train Epoch: 10 [131200/143337] Loss: 0.378228 Acc: 0.8750\n",
      "Train Epoch: 10 [134400/143337] Loss: 0.286676 Acc: 0.9062\n",
      "Train Epoch: 10 [137600/143337] Loss: 0.223190 Acc: 0.9375\n",
      "Train Epoch: 10 [140800/143337] Loss: 0.251803 Acc: 0.9688\n",
      "Elapsed 478.96s, 43.54 s/epoch, 0.01 s/batch, ets 391.88s\n",
      "\n",
      "Test set: Average loss: 0.2819, Accuracy: 34607/37712 (92%)\n",
      "\n",
      "Train Epoch: 11 [3200/143337] Loss: 0.103577 Acc: 0.9688\n",
      "Train Epoch: 11 [6400/143337] Loss: 0.069809 Acc: 0.9688\n",
      "Train Epoch: 11 [9600/143337] Loss: 0.210930 Acc: 0.9375\n",
      "Train Epoch: 11 [12800/143337] Loss: 0.064933 Acc: 1.0000\n",
      "Train Epoch: 11 [16000/143337] Loss: 0.363638 Acc: 0.9062\n",
      "Train Epoch: 11 [19200/143337] Loss: 0.410501 Acc: 0.9375\n",
      "Train Epoch: 11 [22400/143337] Loss: 0.073252 Acc: 0.9688\n",
      "Train Epoch: 11 [25600/143337] Loss: 0.171987 Acc: 0.9375\n",
      "Train Epoch: 11 [28800/143337] Loss: 0.175567 Acc: 0.9688\n",
      "Train Epoch: 11 [32000/143337] Loss: 0.103931 Acc: 0.9688\n",
      "Train Epoch: 11 [35200/143337] Loss: 0.187279 Acc: 0.9375\n",
      "Train Epoch: 11 [38400/143337] Loss: 0.092255 Acc: 0.9688\n",
      "Train Epoch: 11 [41600/143337] Loss: 0.030636 Acc: 1.0000\n",
      "Train Epoch: 11 [44800/143337] Loss: 0.263502 Acc: 0.9062\n",
      "Train Epoch: 11 [48000/143337] Loss: 0.394929 Acc: 0.9062\n",
      "Train Epoch: 11 [51200/143337] Loss: 0.031458 Acc: 0.9688\n",
      "Train Epoch: 11 [54400/143337] Loss: 0.091136 Acc: 0.9688\n",
      "Train Epoch: 11 [57600/143337] Loss: 0.150182 Acc: 0.9375\n",
      "Train Epoch: 11 [60800/143337] Loss: 0.147787 Acc: 0.9688\n",
      "Train Epoch: 11 [64000/143337] Loss: 0.099967 Acc: 0.9375\n",
      "Train Epoch: 11 [67200/143337] Loss: 0.218452 Acc: 0.9688\n",
      "Train Epoch: 11 [70400/143337] Loss: 0.209491 Acc: 0.9375\n",
      "Train Epoch: 11 [73600/143337] Loss: 0.083268 Acc: 0.9375\n",
      "Train Epoch: 11 [76800/143337] Loss: 0.105197 Acc: 0.9688\n",
      "Train Epoch: 11 [80000/143337] Loss: 0.144590 Acc: 0.9375\n",
      "Train Epoch: 11 [83200/143337] Loss: 0.015842 Acc: 1.0000\n",
      "Train Epoch: 11 [86400/143337] Loss: 0.195888 Acc: 0.9375\n",
      "Train Epoch: 11 [89600/143337] Loss: 0.130964 Acc: 0.9375\n",
      "Train Epoch: 11 [92800/143337] Loss: 0.039021 Acc: 1.0000\n",
      "Train Epoch: 11 [96000/143337] Loss: 0.276714 Acc: 0.9375\n",
      "Train Epoch: 11 [99200/143337] Loss: 0.214309 Acc: 0.9062\n",
      "Train Epoch: 11 [102400/143337] Loss: 0.153431 Acc: 0.9375\n",
      "Train Epoch: 11 [105600/143337] Loss: 0.095914 Acc: 0.9688\n",
      "Train Epoch: 11 [108800/143337] Loss: 0.036240 Acc: 1.0000\n",
      "Train Epoch: 11 [112000/143337] Loss: 0.241156 Acc: 0.9375\n",
      "Train Epoch: 11 [115200/143337] Loss: 0.244160 Acc: 0.9375\n",
      "Train Epoch: 11 [118400/143337] Loss: 0.230898 Acc: 0.9375\n",
      "Train Epoch: 11 [121600/143337] Loss: 0.120954 Acc: 1.0000\n",
      "Train Epoch: 11 [124800/143337] Loss: 0.160811 Acc: 0.9375\n",
      "Train Epoch: 11 [128000/143337] Loss: 0.175912 Acc: 0.9062\n",
      "Train Epoch: 11 [131200/143337] Loss: 0.150858 Acc: 0.9688\n",
      "Train Epoch: 11 [134400/143337] Loss: 0.061946 Acc: 0.9688\n",
      "Train Epoch: 11 [137600/143337] Loss: 0.120495 Acc: 0.9688\n",
      "Train Epoch: 11 [140800/143337] Loss: 0.156867 Acc: 0.9375\n",
      "Elapsed 521.56s, 43.46 s/epoch, 0.01 s/batch, ets 347.70s\n",
      "\n",
      "Test set: Average loss: 0.2887, Accuracy: 34628/37712 (92%)\n",
      "\n",
      "Train Epoch: 12 [3200/143337] Loss: 0.112106 Acc: 0.9688\n",
      "Train Epoch: 12 [6400/143337] Loss: 0.569801 Acc: 0.8438\n",
      "Train Epoch: 12 [9600/143337] Loss: 0.180008 Acc: 0.9062\n",
      "Train Epoch: 12 [12800/143337] Loss: 0.097902 Acc: 1.0000\n",
      "Train Epoch: 12 [16000/143337] Loss: 0.122462 Acc: 0.9375\n",
      "Train Epoch: 12 [19200/143337] Loss: 0.161079 Acc: 0.9375\n",
      "Train Epoch: 12 [22400/143337] Loss: 0.093118 Acc: 0.9688\n",
      "Train Epoch: 12 [25600/143337] Loss: 0.056253 Acc: 0.9688\n",
      "Train Epoch: 12 [28800/143337] Loss: 0.128600 Acc: 0.9688\n",
      "Train Epoch: 12 [32000/143337] Loss: 0.131082 Acc: 0.9688\n",
      "Train Epoch: 12 [35200/143337] Loss: 0.107306 Acc: 0.9688\n",
      "Train Epoch: 12 [38400/143337] Loss: 0.015870 Acc: 1.0000\n",
      "Train Epoch: 12 [41600/143337] Loss: 0.021812 Acc: 1.0000\n",
      "Train Epoch: 12 [44800/143337] Loss: 0.118828 Acc: 0.9375\n",
      "Train Epoch: 12 [48000/143337] Loss: 0.323118 Acc: 0.9375\n",
      "Train Epoch: 12 [51200/143337] Loss: 0.213547 Acc: 0.9688\n",
      "Train Epoch: 12 [54400/143337] Loss: 0.177783 Acc: 0.8750\n",
      "Train Epoch: 12 [57600/143337] Loss: 0.122093 Acc: 0.9375\n",
      "Train Epoch: 12 [60800/143337] Loss: 0.110740 Acc: 0.9688\n",
      "Train Epoch: 12 [64000/143337] Loss: 0.113826 Acc: 0.9375\n",
      "Train Epoch: 12 [67200/143337] Loss: 0.051899 Acc: 1.0000\n",
      "Train Epoch: 12 [70400/143337] Loss: 0.207670 Acc: 0.9688\n",
      "Train Epoch: 12 [73600/143337] Loss: 0.067310 Acc: 1.0000\n",
      "Train Epoch: 12 [76800/143337] Loss: 0.120678 Acc: 0.9688\n",
      "Train Epoch: 12 [80000/143337] Loss: 0.033623 Acc: 1.0000\n",
      "Train Epoch: 12 [83200/143337] Loss: 0.411189 Acc: 0.8125\n",
      "Train Epoch: 12 [86400/143337] Loss: 0.570973 Acc: 0.8750\n",
      "Train Epoch: 12 [89600/143337] Loss: 0.418385 Acc: 0.9375\n",
      "Train Epoch: 12 [92800/143337] Loss: 0.099786 Acc: 0.9688\n",
      "Train Epoch: 12 [96000/143337] Loss: 0.249742 Acc: 0.9375\n",
      "Train Epoch: 12 [99200/143337] Loss: 0.183752 Acc: 0.9375\n",
      "Train Epoch: 12 [102400/143337] Loss: 0.343293 Acc: 0.9062\n",
      "Train Epoch: 12 [105600/143337] Loss: 0.069539 Acc: 1.0000\n",
      "Train Epoch: 12 [108800/143337] Loss: 0.019076 Acc: 1.0000\n",
      "Train Epoch: 12 [112000/143337] Loss: 0.127567 Acc: 0.9688\n",
      "Train Epoch: 12 [115200/143337] Loss: 0.092806 Acc: 0.9688\n",
      "Train Epoch: 12 [118400/143337] Loss: 0.024556 Acc: 1.0000\n",
      "Train Epoch: 12 [121600/143337] Loss: 0.163227 Acc: 0.9375\n",
      "Train Epoch: 12 [124800/143337] Loss: 0.268803 Acc: 0.9062\n",
      "Train Epoch: 12 [128000/143337] Loss: 0.104501 Acc: 0.9688\n",
      "Train Epoch: 12 [131200/143337] Loss: 0.169440 Acc: 0.9375\n",
      "Train Epoch: 12 [134400/143337] Loss: 0.013449 Acc: 1.0000\n",
      "Train Epoch: 12 [137600/143337] Loss: 0.083992 Acc: 0.9688\n",
      "Train Epoch: 12 [140800/143337] Loss: 0.230479 Acc: 0.9062\n",
      "Elapsed 563.92s, 43.38 s/epoch, 0.01 s/batch, ets 303.65s\n",
      "\n",
      "Test set: Average loss: 0.2818, Accuracy: 34690/37712 (92%)\n",
      "\n",
      "Train Epoch: 13 [3200/143337] Loss: 0.364451 Acc: 0.9375\n",
      "Train Epoch: 13 [6400/143337] Loss: 0.080006 Acc: 0.9688\n",
      "Train Epoch: 13 [9600/143337] Loss: 0.256189 Acc: 0.9688\n",
      "Train Epoch: 13 [12800/143337] Loss: 0.128135 Acc: 0.9688\n",
      "Train Epoch: 13 [16000/143337] Loss: 0.220297 Acc: 0.9375\n",
      "Train Epoch: 13 [19200/143337] Loss: 0.087811 Acc: 0.9688\n",
      "Train Epoch: 13 [22400/143337] Loss: 0.173354 Acc: 0.9375\n",
      "Train Epoch: 13 [25600/143337] Loss: 0.035801 Acc: 1.0000\n",
      "Train Epoch: 13 [28800/143337] Loss: 0.301186 Acc: 0.9062\n",
      "Train Epoch: 13 [32000/143337] Loss: 0.233970 Acc: 0.9375\n",
      "Train Epoch: 13 [35200/143337] Loss: 0.300063 Acc: 0.9062\n",
      "Train Epoch: 13 [38400/143337] Loss: 0.158941 Acc: 0.9375\n",
      "Train Epoch: 13 [41600/143337] Loss: 0.149727 Acc: 0.9688\n",
      "Train Epoch: 13 [44800/143337] Loss: 0.312143 Acc: 0.9375\n",
      "Train Epoch: 13 [48000/143337] Loss: 0.405735 Acc: 0.8750\n",
      "Train Epoch: 13 [51200/143337] Loss: 0.133724 Acc: 0.9688\n",
      "Train Epoch: 13 [54400/143337] Loss: 0.114594 Acc: 0.9688\n",
      "Train Epoch: 13 [57600/143337] Loss: 0.254060 Acc: 0.9375\n",
      "Train Epoch: 13 [60800/143337] Loss: 0.156036 Acc: 0.9375\n",
      "Train Epoch: 13 [64000/143337] Loss: 0.050517 Acc: 1.0000\n",
      "Train Epoch: 13 [67200/143337] Loss: 0.243805 Acc: 0.9062\n",
      "Train Epoch: 13 [70400/143337] Loss: 0.168314 Acc: 0.9688\n",
      "Train Epoch: 13 [73600/143337] Loss: 0.450528 Acc: 0.8438\n",
      "Train Epoch: 13 [76800/143337] Loss: 0.072277 Acc: 1.0000\n",
      "Train Epoch: 13 [80000/143337] Loss: 0.102080 Acc: 0.9688\n",
      "Train Epoch: 13 [83200/143337] Loss: 0.760702 Acc: 0.8438\n",
      "Train Epoch: 13 [86400/143337] Loss: 0.099872 Acc: 0.9375\n",
      "Train Epoch: 13 [89600/143337] Loss: 0.175506 Acc: 0.9688\n",
      "Train Epoch: 13 [92800/143337] Loss: 0.222007 Acc: 0.9062\n",
      "Train Epoch: 13 [96000/143337] Loss: 0.380661 Acc: 0.8750\n",
      "Train Epoch: 13 [99200/143337] Loss: 0.092160 Acc: 0.9688\n",
      "Train Epoch: 13 [102400/143337] Loss: 0.709475 Acc: 0.8750\n",
      "Train Epoch: 13 [105600/143337] Loss: 0.238008 Acc: 0.9062\n",
      "Train Epoch: 13 [108800/143337] Loss: 0.066116 Acc: 1.0000\n",
      "Train Epoch: 13 [112000/143337] Loss: 0.764128 Acc: 0.9062\n",
      "Train Epoch: 13 [115200/143337] Loss: 0.101344 Acc: 0.9688\n",
      "Train Epoch: 13 [118400/143337] Loss: 0.084071 Acc: 0.9688\n",
      "Train Epoch: 13 [121600/143337] Loss: 0.248061 Acc: 0.9688\n",
      "Train Epoch: 13 [124800/143337] Loss: 0.200136 Acc: 0.9375\n",
      "Train Epoch: 13 [128000/143337] Loss: 0.157742 Acc: 0.9375\n",
      "Train Epoch: 13 [131200/143337] Loss: 0.159117 Acc: 0.9688\n",
      "Train Epoch: 13 [134400/143337] Loss: 0.080331 Acc: 1.0000\n",
      "Train Epoch: 13 [137600/143337] Loss: 0.066128 Acc: 1.0000\n",
      "Train Epoch: 13 [140800/143337] Loss: 0.332748 Acc: 0.8750\n",
      "Elapsed 606.21s, 43.30 s/epoch, 0.01 s/batch, ets 259.80s\n",
      "\n",
      "Test set: Average loss: 0.2647, Accuracy: 34865/37712 (92%)\n",
      "\n",
      "Train Epoch: 14 [3200/143337] Loss: 0.201403 Acc: 0.9375\n",
      "Train Epoch: 14 [6400/143337] Loss: 0.182728 Acc: 0.9688\n",
      "Train Epoch: 14 [9600/143337] Loss: 0.424074 Acc: 0.9062\n",
      "Train Epoch: 14 [12800/143337] Loss: 0.048280 Acc: 1.0000\n",
      "Train Epoch: 14 [16000/143337] Loss: 0.219718 Acc: 0.9375\n",
      "Train Epoch: 14 [19200/143337] Loss: 0.173580 Acc: 0.9375\n",
      "Train Epoch: 14 [22400/143337] Loss: 0.082367 Acc: 0.9688\n",
      "Train Epoch: 14 [25600/143337] Loss: 0.246106 Acc: 0.9375\n",
      "Train Epoch: 14 [28800/143337] Loss: 0.034531 Acc: 1.0000\n",
      "Train Epoch: 14 [32000/143337] Loss: 0.301173 Acc: 0.9375\n",
      "Train Epoch: 14 [35200/143337] Loss: 0.065220 Acc: 1.0000\n",
      "Train Epoch: 14 [38400/143337] Loss: 0.310823 Acc: 0.9375\n",
      "Train Epoch: 14 [41600/143337] Loss: 0.091371 Acc: 0.9688\n",
      "Train Epoch: 14 [44800/143337] Loss: 0.040178 Acc: 1.0000\n",
      "Train Epoch: 14 [48000/143337] Loss: 0.195061 Acc: 0.9062\n",
      "Train Epoch: 14 [51200/143337] Loss: 0.159579 Acc: 0.9688\n",
      "Train Epoch: 14 [54400/143337] Loss: 0.182023 Acc: 0.9688\n",
      "Train Epoch: 14 [57600/143337] Loss: 0.301751 Acc: 0.9062\n",
      "Train Epoch: 14 [60800/143337] Loss: 0.210435 Acc: 0.9375\n",
      "Train Epoch: 14 [64000/143337] Loss: 0.147763 Acc: 0.9375\n",
      "Train Epoch: 14 [67200/143337] Loss: 0.027810 Acc: 1.0000\n",
      "Train Epoch: 14 [70400/143337] Loss: 0.050596 Acc: 1.0000\n",
      "Train Epoch: 14 [73600/143337] Loss: 0.189565 Acc: 0.9375\n",
      "Train Epoch: 14 [76800/143337] Loss: 0.104295 Acc: 0.9375\n",
      "Train Epoch: 14 [80000/143337] Loss: 0.097323 Acc: 0.9375\n",
      "Train Epoch: 14 [83200/143337] Loss: 0.264818 Acc: 0.9062\n",
      "Train Epoch: 14 [86400/143337] Loss: 0.251234 Acc: 0.9375\n",
      "Train Epoch: 14 [89600/143337] Loss: 0.506636 Acc: 0.9688\n",
      "Train Epoch: 14 [92800/143337] Loss: 0.037991 Acc: 1.0000\n",
      "Train Epoch: 14 [96000/143337] Loss: 0.042255 Acc: 1.0000\n",
      "Train Epoch: 14 [99200/143337] Loss: 0.240908 Acc: 0.9062\n",
      "Train Epoch: 14 [102400/143337] Loss: 0.143443 Acc: 0.9375\n",
      "Train Epoch: 14 [105600/143337] Loss: 0.074196 Acc: 0.9688\n",
      "Train Epoch: 14 [108800/143337] Loss: 0.061286 Acc: 0.9688\n",
      "Train Epoch: 14 [112000/143337] Loss: 0.043935 Acc: 1.0000\n",
      "Train Epoch: 14 [115200/143337] Loss: 0.137252 Acc: 0.9375\n",
      "Train Epoch: 14 [118400/143337] Loss: 0.392154 Acc: 0.8750\n",
      "Train Epoch: 14 [121600/143337] Loss: 0.197372 Acc: 0.9375\n",
      "Train Epoch: 14 [124800/143337] Loss: 0.157566 Acc: 0.9375\n",
      "Train Epoch: 14 [128000/143337] Loss: 0.173231 Acc: 0.9688\n",
      "Train Epoch: 14 [131200/143337] Loss: 0.110928 Acc: 0.9375\n",
      "Train Epoch: 14 [134400/143337] Loss: 0.073970 Acc: 0.9688\n",
      "Train Epoch: 14 [137600/143337] Loss: 0.033696 Acc: 1.0000\n",
      "Train Epoch: 14 [140800/143337] Loss: 0.077913 Acc: 1.0000\n",
      "Elapsed 648.32s, 43.22 s/epoch, 0.01 s/batch, ets 216.11s\n",
      "\n",
      "Test set: Average loss: 0.2651, Accuracy: 34847/37712 (92%)\n",
      "\n",
      "Train Epoch: 15 [3200/143337] Loss: 0.134583 Acc: 0.9375\n",
      "Train Epoch: 15 [6400/143337] Loss: 0.167194 Acc: 0.9375\n",
      "Train Epoch: 15 [9600/143337] Loss: 0.047034 Acc: 1.0000\n",
      "Train Epoch: 15 [12800/143337] Loss: 0.009948 Acc: 1.0000\n",
      "Train Epoch: 15 [16000/143337] Loss: 0.113686 Acc: 0.9688\n",
      "Train Epoch: 15 [19200/143337] Loss: 0.091527 Acc: 0.9688\n",
      "Train Epoch: 15 [22400/143337] Loss: 0.353231 Acc: 0.9062\n",
      "Train Epoch: 15 [25600/143337] Loss: 0.058515 Acc: 1.0000\n",
      "Train Epoch: 15 [28800/143337] Loss: 0.052373 Acc: 1.0000\n",
      "Train Epoch: 15 [32000/143337] Loss: 0.149096 Acc: 0.9375\n",
      "Train Epoch: 15 [35200/143337] Loss: 0.118740 Acc: 0.9375\n",
      "Train Epoch: 15 [38400/143337] Loss: 0.030477 Acc: 1.0000\n",
      "Train Epoch: 15 [41600/143337] Loss: 0.110156 Acc: 0.9688\n",
      "Train Epoch: 15 [44800/143337] Loss: 0.355020 Acc: 0.8750\n",
      "Train Epoch: 15 [48000/143337] Loss: 0.094008 Acc: 0.9688\n",
      "Train Epoch: 15 [51200/143337] Loss: 0.228086 Acc: 0.8750\n",
      "Train Epoch: 15 [54400/143337] Loss: 0.053024 Acc: 1.0000\n",
      "Train Epoch: 15 [57600/143337] Loss: 0.015422 Acc: 1.0000\n",
      "Train Epoch: 15 [60800/143337] Loss: 0.340592 Acc: 0.9375\n",
      "Train Epoch: 15 [64000/143337] Loss: 0.221995 Acc: 0.9375\n",
      "Train Epoch: 15 [67200/143337] Loss: 0.030286 Acc: 1.0000\n",
      "Train Epoch: 15 [70400/143337] Loss: 0.118119 Acc: 0.9688\n",
      "Train Epoch: 15 [73600/143337] Loss: 0.094910 Acc: 0.9688\n",
      "Train Epoch: 15 [76800/143337] Loss: 0.014458 Acc: 1.0000\n",
      "Train Epoch: 15 [80000/143337] Loss: 0.048745 Acc: 1.0000\n",
      "Train Epoch: 15 [83200/143337] Loss: 0.152239 Acc: 0.9688\n",
      "Train Epoch: 15 [86400/143337] Loss: 0.107493 Acc: 0.9375\n",
      "Train Epoch: 15 [89600/143337] Loss: 0.154859 Acc: 0.9062\n",
      "Train Epoch: 15 [92800/143337] Loss: 0.164204 Acc: 0.9375\n",
      "Train Epoch: 15 [96000/143337] Loss: 0.262855 Acc: 0.8750\n",
      "Train Epoch: 15 [99200/143337] Loss: 0.318830 Acc: 0.9062\n",
      "Train Epoch: 15 [102400/143337] Loss: 0.030035 Acc: 1.0000\n",
      "Train Epoch: 15 [105600/143337] Loss: 0.083861 Acc: 0.9688\n",
      "Train Epoch: 15 [108800/143337] Loss: 0.181361 Acc: 0.9062\n",
      "Train Epoch: 15 [112000/143337] Loss: 0.060226 Acc: 1.0000\n",
      "Train Epoch: 15 [115200/143337] Loss: 0.038854 Acc: 1.0000\n",
      "Train Epoch: 15 [118400/143337] Loss: 0.062020 Acc: 0.9688\n",
      "Train Epoch: 15 [121600/143337] Loss: 0.063511 Acc: 1.0000\n",
      "Train Epoch: 15 [124800/143337] Loss: 0.175103 Acc: 0.9375\n",
      "Train Epoch: 15 [128000/143337] Loss: 0.099565 Acc: 0.9688\n",
      "Train Epoch: 15 [131200/143337] Loss: 0.221263 Acc: 0.9688\n",
      "Train Epoch: 15 [134400/143337] Loss: 0.382190 Acc: 0.9062\n",
      "Train Epoch: 15 [137600/143337] Loss: 0.030741 Acc: 1.0000\n",
      "Train Epoch: 15 [140800/143337] Loss: 0.113185 Acc: 0.9688\n",
      "Elapsed 690.85s, 43.18 s/epoch, 0.01 s/batch, ets 172.71s\n",
      "\n",
      "Test set: Average loss: 0.2829, Accuracy: 34610/37712 (92%)\n",
      "\n",
      "Train Epoch: 16 [3200/143337] Loss: 0.202268 Acc: 0.9375\n",
      "Train Epoch: 16 [6400/143337] Loss: 0.126616 Acc: 0.9375\n",
      "Train Epoch: 16 [9600/143337] Loss: 0.044673 Acc: 1.0000\n",
      "Train Epoch: 16 [12800/143337] Loss: 0.293277 Acc: 0.9062\n",
      "Train Epoch: 16 [16000/143337] Loss: 0.171760 Acc: 0.9375\n",
      "Train Epoch: 16 [19200/143337] Loss: 0.018535 Acc: 1.0000\n",
      "Train Epoch: 16 [22400/143337] Loss: 0.300607 Acc: 0.8750\n",
      "Train Epoch: 16 [25600/143337] Loss: 0.178645 Acc: 0.9688\n",
      "Train Epoch: 16 [28800/143337] Loss: 0.045891 Acc: 1.0000\n",
      "Train Epoch: 16 [32000/143337] Loss: 0.064134 Acc: 0.9688\n",
      "Train Epoch: 16 [35200/143337] Loss: 0.118441 Acc: 0.9375\n",
      "Train Epoch: 16 [38400/143337] Loss: 0.105942 Acc: 0.9688\n",
      "Train Epoch: 16 [41600/143337] Loss: 0.109148 Acc: 0.9688\n",
      "Train Epoch: 16 [44800/143337] Loss: 0.066816 Acc: 0.9688\n",
      "Train Epoch: 16 [48000/143337] Loss: 0.058763 Acc: 0.9688\n",
      "Train Epoch: 16 [51200/143337] Loss: 0.113989 Acc: 0.9688\n",
      "Train Epoch: 16 [54400/143337] Loss: 0.115115 Acc: 0.9062\n",
      "Train Epoch: 16 [57600/143337] Loss: 0.191651 Acc: 0.9688\n",
      "Train Epoch: 16 [60800/143337] Loss: 0.221031 Acc: 0.9375\n",
      "Train Epoch: 16 [64000/143337] Loss: 0.099669 Acc: 0.9375\n",
      "Train Epoch: 16 [67200/143337] Loss: 0.305765 Acc: 0.9375\n",
      "Train Epoch: 16 [70400/143337] Loss: 0.063886 Acc: 1.0000\n",
      "Train Epoch: 16 [73600/143337] Loss: 0.029550 Acc: 1.0000\n",
      "Train Epoch: 16 [76800/143337] Loss: 0.235616 Acc: 0.9375\n",
      "Train Epoch: 16 [80000/143337] Loss: 0.022300 Acc: 1.0000\n",
      "Train Epoch: 16 [83200/143337] Loss: 0.308958 Acc: 0.8125\n",
      "Train Epoch: 16 [86400/143337] Loss: 0.270942 Acc: 0.9062\n",
      "Train Epoch: 16 [89600/143337] Loss: 0.069659 Acc: 0.9688\n",
      "Train Epoch: 16 [92800/143337] Loss: 0.019987 Acc: 1.0000\n",
      "Train Epoch: 16 [96000/143337] Loss: 0.027629 Acc: 1.0000\n",
      "Train Epoch: 16 [99200/143337] Loss: 0.035126 Acc: 1.0000\n",
      "Train Epoch: 16 [102400/143337] Loss: 0.127916 Acc: 0.9688\n",
      "Train Epoch: 16 [105600/143337] Loss: 0.439163 Acc: 0.9062\n",
      "Train Epoch: 16 [108800/143337] Loss: 0.238337 Acc: 0.9062\n",
      "Train Epoch: 16 [112000/143337] Loss: 0.119826 Acc: 0.9688\n",
      "Train Epoch: 16 [115200/143337] Loss: 0.027590 Acc: 1.0000\n",
      "Train Epoch: 16 [118400/143337] Loss: 0.033828 Acc: 1.0000\n",
      "Train Epoch: 16 [121600/143337] Loss: 0.096523 Acc: 0.9688\n",
      "Train Epoch: 16 [124800/143337] Loss: 0.053391 Acc: 1.0000\n",
      "Train Epoch: 16 [128000/143337] Loss: 0.206845 Acc: 0.9688\n",
      "Train Epoch: 16 [131200/143337] Loss: 0.141346 Acc: 0.9688\n",
      "Train Epoch: 16 [134400/143337] Loss: 0.117189 Acc: 0.9688\n",
      "Train Epoch: 16 [137600/143337] Loss: 0.133463 Acc: 0.9375\n",
      "Train Epoch: 16 [140800/143337] Loss: 0.020206 Acc: 1.0000\n",
      "Elapsed 733.54s, 43.15 s/epoch, 0.01 s/batch, ets 129.45s\n",
      "\n",
      "Test set: Average loss: 0.2533, Accuracy: 35016/37712 (93%)\n",
      "\n",
      "Train Epoch: 17 [3200/143337] Loss: 0.032369 Acc: 1.0000\n",
      "Train Epoch: 17 [6400/143337] Loss: 0.160656 Acc: 0.9688\n",
      "Train Epoch: 17 [9600/143337] Loss: 0.084532 Acc: 1.0000\n",
      "Train Epoch: 17 [12800/143337] Loss: 0.104724 Acc: 0.9688\n",
      "Train Epoch: 17 [16000/143337] Loss: 0.227344 Acc: 0.9062\n",
      "Train Epoch: 17 [19200/143337] Loss: 0.287106 Acc: 0.8750\n",
      "Train Epoch: 17 [22400/143337] Loss: 0.229707 Acc: 0.9375\n",
      "Train Epoch: 17 [25600/143337] Loss: 0.110762 Acc: 0.9688\n",
      "Train Epoch: 17 [28800/143337] Loss: 0.127501 Acc: 0.9688\n",
      "Train Epoch: 17 [32000/143337] Loss: 0.019518 Acc: 1.0000\n",
      "Train Epoch: 17 [35200/143337] Loss: 0.144034 Acc: 0.9375\n",
      "Train Epoch: 17 [38400/143337] Loss: 0.110251 Acc: 0.9688\n",
      "Train Epoch: 17 [41600/143337] Loss: 0.101582 Acc: 1.0000\n",
      "Train Epoch: 17 [44800/143337] Loss: 0.224265 Acc: 0.9688\n",
      "Train Epoch: 17 [48000/143337] Loss: 0.076812 Acc: 0.9688\n",
      "Train Epoch: 17 [51200/143337] Loss: 0.033488 Acc: 1.0000\n",
      "Train Epoch: 17 [54400/143337] Loss: 0.051952 Acc: 0.9688\n",
      "Train Epoch: 17 [57600/143337] Loss: 0.250767 Acc: 0.8750\n",
      "Train Epoch: 17 [60800/143337] Loss: 1.214628 Acc: 0.8750\n",
      "Train Epoch: 17 [64000/143337] Loss: 0.092286 Acc: 0.9688\n",
      "Train Epoch: 17 [67200/143337] Loss: 0.067789 Acc: 0.9688\n",
      "Train Epoch: 17 [70400/143337] Loss: 0.176850 Acc: 0.9688\n",
      "Train Epoch: 17 [73600/143337] Loss: 0.173930 Acc: 0.9688\n",
      "Train Epoch: 17 [76800/143337] Loss: 0.246747 Acc: 0.8750\n",
      "Train Epoch: 17 [80000/143337] Loss: 0.181675 Acc: 0.9375\n",
      "Train Epoch: 17 [83200/143337] Loss: 0.144490 Acc: 0.9375\n",
      "Train Epoch: 17 [86400/143337] Loss: 0.238129 Acc: 0.9375\n",
      "Train Epoch: 17 [89600/143337] Loss: 0.337314 Acc: 0.9062\n",
      "Train Epoch: 17 [92800/143337] Loss: 0.126122 Acc: 0.9688\n",
      "Train Epoch: 17 [96000/143337] Loss: 0.074235 Acc: 0.9688\n",
      "Train Epoch: 17 [99200/143337] Loss: 0.057379 Acc: 1.0000\n",
      "Train Epoch: 17 [102400/143337] Loss: 0.086198 Acc: 0.9688\n",
      "Train Epoch: 17 [105600/143337] Loss: 0.206745 Acc: 0.9062\n",
      "Train Epoch: 17 [108800/143337] Loss: 0.108112 Acc: 0.9688\n",
      "Train Epoch: 17 [112000/143337] Loss: 0.119151 Acc: 0.9375\n",
      "Train Epoch: 17 [115200/143337] Loss: 0.138657 Acc: 0.9688\n",
      "Train Epoch: 17 [118400/143337] Loss: 0.051993 Acc: 1.0000\n",
      "Train Epoch: 17 [121600/143337] Loss: 0.031309 Acc: 1.0000\n",
      "Train Epoch: 17 [124800/143337] Loss: 0.037244 Acc: 0.9688\n",
      "Train Epoch: 17 [128000/143337] Loss: 0.100190 Acc: 0.9688\n",
      "Train Epoch: 17 [131200/143337] Loss: 0.069640 Acc: 1.0000\n",
      "Train Epoch: 17 [134400/143337] Loss: 0.218782 Acc: 0.9375\n",
      "Train Epoch: 17 [137600/143337] Loss: 0.042028 Acc: 1.0000\n",
      "Train Epoch: 17 [140800/143337] Loss: 0.113343 Acc: 1.0000\n",
      "Elapsed 776.33s, 43.13 s/epoch, 0.01 s/batch, ets 86.26s\n",
      "\n",
      "Test set: Average loss: 0.2913, Accuracy: 34583/37712 (92%)\n",
      "\n",
      "Train Epoch: 18 [3200/143337] Loss: 0.638038 Acc: 0.9062\n",
      "Train Epoch: 18 [6400/143337] Loss: 0.035170 Acc: 1.0000\n",
      "Train Epoch: 18 [9600/143337] Loss: 0.080722 Acc: 0.9688\n",
      "Train Epoch: 18 [12800/143337] Loss: 0.081675 Acc: 0.9688\n",
      "Train Epoch: 18 [16000/143337] Loss: 0.095923 Acc: 0.9688\n",
      "Train Epoch: 18 [19200/143337] Loss: 0.086833 Acc: 0.9688\n",
      "Train Epoch: 18 [22400/143337] Loss: 0.144247 Acc: 0.9375\n",
      "Train Epoch: 18 [25600/143337] Loss: 0.077567 Acc: 0.9688\n",
      "Train Epoch: 18 [28800/143337] Loss: 0.026626 Acc: 0.9688\n",
      "Train Epoch: 18 [32000/143337] Loss: 0.082900 Acc: 0.9375\n",
      "Train Epoch: 18 [35200/143337] Loss: 0.156988 Acc: 0.9375\n",
      "Train Epoch: 18 [38400/143337] Loss: 0.035268 Acc: 1.0000\n",
      "Train Epoch: 18 [41600/143337] Loss: 0.047456 Acc: 0.9688\n",
      "Train Epoch: 18 [44800/143337] Loss: 0.046150 Acc: 1.0000\n",
      "Train Epoch: 18 [48000/143337] Loss: 0.015183 Acc: 1.0000\n",
      "Train Epoch: 18 [51200/143337] Loss: 0.249306 Acc: 0.9375\n",
      "Train Epoch: 18 [54400/143337] Loss: 0.088507 Acc: 0.9688\n",
      "Train Epoch: 18 [57600/143337] Loss: 0.250655 Acc: 0.8750\n",
      "Train Epoch: 18 [60800/143337] Loss: 0.108482 Acc: 0.9375\n",
      "Train Epoch: 18 [64000/143337] Loss: 0.248960 Acc: 0.9688\n",
      "Train Epoch: 18 [67200/143337] Loss: 0.033055 Acc: 1.0000\n",
      "Train Epoch: 18 [70400/143337] Loss: 0.143436 Acc: 0.9375\n",
      "Train Epoch: 18 [73600/143337] Loss: 0.180419 Acc: 0.9062\n",
      "Train Epoch: 18 [76800/143337] Loss: 0.322354 Acc: 0.9062\n",
      "Train Epoch: 18 [80000/143337] Loss: 0.171975 Acc: 0.9375\n",
      "Train Epoch: 18 [83200/143337] Loss: 0.197152 Acc: 0.9062\n",
      "Train Epoch: 18 [86400/143337] Loss: 0.065311 Acc: 0.9688\n",
      "Train Epoch: 18 [89600/143337] Loss: 0.047301 Acc: 1.0000\n",
      "Train Epoch: 18 [92800/143337] Loss: 0.174211 Acc: 0.9375\n",
      "Train Epoch: 18 [96000/143337] Loss: 0.013823 Acc: 1.0000\n",
      "Train Epoch: 18 [99200/143337] Loss: 0.030929 Acc: 1.0000\n",
      "Train Epoch: 18 [102400/143337] Loss: 0.031748 Acc: 1.0000\n",
      "Train Epoch: 18 [105600/143337] Loss: 0.224632 Acc: 0.9688\n",
      "Train Epoch: 18 [108800/143337] Loss: 0.113624 Acc: 0.9688\n",
      "Train Epoch: 18 [112000/143337] Loss: 0.111227 Acc: 0.9688\n",
      "Train Epoch: 18 [115200/143337] Loss: 0.164608 Acc: 0.9062\n",
      "Train Epoch: 18 [118400/143337] Loss: 0.313485 Acc: 0.8750\n",
      "Train Epoch: 18 [121600/143337] Loss: 0.017695 Acc: 1.0000\n",
      "Train Epoch: 18 [124800/143337] Loss: 0.055862 Acc: 1.0000\n",
      "Train Epoch: 18 [128000/143337] Loss: 0.110778 Acc: 0.9688\n",
      "Train Epoch: 18 [131200/143337] Loss: 0.406738 Acc: 0.9062\n",
      "Train Epoch: 18 [134400/143337] Loss: 0.213452 Acc: 0.9375\n",
      "Train Epoch: 18 [137600/143337] Loss: 0.063478 Acc: 0.9688\n",
      "Train Epoch: 18 [140800/143337] Loss: 0.046573 Acc: 1.0000\n",
      "Elapsed 818.33s, 43.07 s/epoch, 0.01 s/batch, ets 43.07s\n",
      "\n",
      "Test set: Average loss: 0.2626, Accuracy: 34913/37712 (93%)\n",
      "\n",
      "Train Epoch: 19 [3200/143337] Loss: 0.227502 Acc: 0.9062\n",
      "Train Epoch: 19 [6400/143337] Loss: 0.419398 Acc: 0.9375\n",
      "Train Epoch: 19 [9600/143337] Loss: 0.101525 Acc: 0.9375\n",
      "Train Epoch: 19 [12800/143337] Loss: 0.075564 Acc: 0.9688\n",
      "Train Epoch: 19 [16000/143337] Loss: 0.053377 Acc: 1.0000\n",
      "Train Epoch: 19 [19200/143337] Loss: 0.019432 Acc: 1.0000\n",
      "Train Epoch: 19 [22400/143337] Loss: 0.053491 Acc: 1.0000\n",
      "Train Epoch: 19 [25600/143337] Loss: 0.063265 Acc: 0.9688\n",
      "Train Epoch: 19 [28800/143337] Loss: 0.013819 Acc: 1.0000\n",
      "Train Epoch: 19 [32000/143337] Loss: 0.232335 Acc: 0.9375\n",
      "Train Epoch: 19 [35200/143337] Loss: 0.097530 Acc: 0.9688\n",
      "Train Epoch: 19 [38400/143337] Loss: 0.085923 Acc: 0.9688\n",
      "Train Epoch: 19 [41600/143337] Loss: 0.208179 Acc: 0.9062\n",
      "Train Epoch: 19 [44800/143337] Loss: 0.075926 Acc: 0.9688\n",
      "Train Epoch: 19 [48000/143337] Loss: 0.074446 Acc: 0.9688\n",
      "Train Epoch: 19 [51200/143337] Loss: 0.304100 Acc: 0.9062\n",
      "Train Epoch: 19 [54400/143337] Loss: 0.132722 Acc: 0.9688\n",
      "Train Epoch: 19 [57600/143337] Loss: 0.235424 Acc: 0.9375\n",
      "Train Epoch: 19 [60800/143337] Loss: 0.122058 Acc: 0.9688\n",
      "Train Epoch: 19 [64000/143337] Loss: 0.094922 Acc: 0.9688\n",
      "Train Epoch: 19 [67200/143337] Loss: 0.234556 Acc: 0.9062\n",
      "Train Epoch: 19 [70400/143337] Loss: 0.033697 Acc: 1.0000\n",
      "Train Epoch: 19 [73600/143337] Loss: 0.023659 Acc: 1.0000\n",
      "Train Epoch: 19 [76800/143337] Loss: 0.318403 Acc: 0.9688\n",
      "Train Epoch: 19 [80000/143337] Loss: 0.190180 Acc: 0.9375\n",
      "Train Epoch: 19 [83200/143337] Loss: 0.071837 Acc: 0.9688\n",
      "Train Epoch: 19 [86400/143337] Loss: 0.107691 Acc: 0.9375\n",
      "Train Epoch: 19 [89600/143337] Loss: 0.187730 Acc: 0.9375\n",
      "Train Epoch: 19 [92800/143337] Loss: 0.116070 Acc: 0.9688\n",
      "Train Epoch: 19 [96000/143337] Loss: 0.284255 Acc: 0.9375\n",
      "Train Epoch: 19 [99200/143337] Loss: 0.061567 Acc: 0.9688\n",
      "Train Epoch: 19 [102400/143337] Loss: 0.128170 Acc: 0.9688\n",
      "Train Epoch: 19 [105600/143337] Loss: 0.166261 Acc: 0.9375\n",
      "Train Epoch: 19 [108800/143337] Loss: 0.039648 Acc: 0.9688\n",
      "Train Epoch: 19 [112000/143337] Loss: 0.446723 Acc: 0.9062\n",
      "Train Epoch: 19 [115200/143337] Loss: 0.020454 Acc: 1.0000\n",
      "Train Epoch: 19 [118400/143337] Loss: 0.135100 Acc: 0.9375\n",
      "Train Epoch: 19 [121600/143337] Loss: 0.046612 Acc: 1.0000\n",
      "Train Epoch: 19 [124800/143337] Loss: 0.094001 Acc: 0.9688\n",
      "Train Epoch: 19 [128000/143337] Loss: 0.181297 Acc: 0.9375\n",
      "Train Epoch: 19 [131200/143337] Loss: 0.057326 Acc: 1.0000\n",
      "Train Epoch: 19 [134400/143337] Loss: 0.130657 Acc: 0.9688\n",
      "Train Epoch: 19 [137600/143337] Loss: 0.113001 Acc: 0.9688\n",
      "Train Epoch: 19 [140800/143337] Loss: 0.216571 Acc: 0.9375\n",
      "Elapsed 860.05s, 43.00 s/epoch, 0.01 s/batch, ets 0.00s\n",
      "\n",
      "Test set: Average loss: 0.2637, Accuracy: 34900/37712 (93%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "t_begin = time.time()\n",
    "#for save best model\n",
    "best_model = copy.deepcopy(model)\n",
    "for epoch in range(epochs_count):\n",
    "\n",
    "    train_loss, train_acc = train(model, optimizer, train_loader, epoch)\n",
    "\n",
    "    epoch_train_loss = np.append(epoch_train_loss, [train_loss])\n",
    "\n",
    "    epoch_train_acc = np.append(epoch_train_acc, [train_acc])\n",
    "\n",
    "    elapsed_time = time.time() - t_begin\n",
    "    speed_epoch = elapsed_time / (epoch + 1)\n",
    "    speed_batch = speed_epoch / len(train_loader)\n",
    "    eta = speed_epoch * epochs_count - elapsed_time\n",
    "\n",
    "    print(\n",
    "        \"Elapsed {:.2f}s, {:.2f} s/epoch, {:.2f} s/batch, ets {:.2f}s\".format(\n",
    "            elapsed_time, speed_epoch, speed_batch, eta\n",
    "        )\n",
    "    )\n",
    "\n",
    "    if epoch % test_interval == 0:\n",
    "        current_loss, current_accuracy = validate(model, test_loader)\n",
    "\n",
    "        epoch_test_loss = np.append(epoch_test_loss, [current_loss])\n",
    "\n",
    "        epoch_test_acc = np.append(epoch_test_acc, [current_accuracy])\n",
    "\n",
    "        if current_loss < best_loss:\n",
    "            best_loss = current_loss\n",
    "            best_model = copy.deepcopy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "LUMHs1Revk4O",
    "outputId": "c9019e24-6b9f-45a0-b99a-82d5cc55ebdb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 867.89, Best Loss: 0.253\n"
     ]
    }
   ],
   "source": [
    "print(\"Total time: {:.2f}, Best Loss: {:.3f}\".format(time.time() - t_begin, best_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "colab_type": "code",
    "id": "WeeFfy2I99GO",
    "outputId": "591aa333-93d5-4529-ea2f-eff8d3ff7f61"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmEAAAGDCAYAAABjkcdfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nOzdd5iU5fn28e+1S5Pee1mQkSYdFUVE\nbEFRUDRKYk/UqEnUaHwlKoRYEoxGDYYY0Z/Gji2KNdhFEwtgEEVUuhSlKU3qsvf7xzXLDsvusmVm\nn9nd83MczzE7M8/MXPuwsCd3tRACIiIiIlK+MqIuQERERKQqUggTERERiYBCmIiIiEgEFMJERERE\nIqAQJiIiIhIBhTARERGRCCiEiQhmlmlmm82sfTLPjZKZdTazlKzBk/+9zexVMzszFXWY2Vgz+0dp\nXy8i6UshTKQCioeg3CPHzLYm3C8wDBQlhLArhFA3hPB1Ms9NV2b2upmNK+DxU81shZllluT9QgjH\nhRAeTUJdx5jZknzvfWMI4eKyvncBn3WBmb2d7PcVkeJTCBOpgOIhqG4IoS7wNXBSwmN7hQEzq1b+\nVaa1B4GzC3j8bOCREMKucq5HRKoghTCRSsjMbjKzJ8zscTPbBJxlZoea2Qdmtt7MvjGziWZWPX5+\nNTMLZpYVv/9I/PlXzGyTmb1vZh1Lem78+ePN7Csz22Bmd5nZf8zsvELqLk6NvzCzBWb2vZlNTHht\nppndYWbrzGwRMKyIS/QvoKWZHZbw+ibACcBD8fsjzGy2mW00s6/NbGwR1/u93O9pX3XEW6Dmxa/V\nQjO7IP54A+AFoH1Cq2bz+J/lPxNef4qZzY1fozfNrEvCc8vN7Eoz+zR+vR83s5pFXIfCvp+2Zvai\nmX1nZvPN7GcJzw00s4/j12WVmd0af7y2mT0W/77Xm9lHZta0pJ8tUpUohIlUXqcAjwENgCeAbOBy\noCkwCA8Hvyji9T8FxgKN8da2G0t6rpk1B54Ero5/7mLg4CLepzg1ngD0B/ri4fKY+OOXAMcBvYGD\ngNML+5AQwg/A08A5CQ+PBuaEEObG728GzgQaAicBl5vZiUXUnmtfdawChgP1gQuBu8ysVwhhQ/xz\nvk5o1Vyd+EIz6wY8DPwaaAa8DjyfG1TjTgeOBTrh16mgFr99eQL/s2oNnAH82cyGxJ+7C7g1hFAf\n6IxfR4DzgdpAW6AJcCmwrRSfLVJlKISJVF7vhRBeCCHkhBC2hhBmhBA+DCFkhxAWAZOBIUW8/ukQ\nwswQwk7gUaBPKc49EZgdQpgaf+4OYG1hb1LMGv8UQtgQQlgCvJ3wWacDd4QQlocQ1gETiqgXvEvy\n9ISWonPij+XW8mYIYW78+n0CTCmgloIUWUf8z2RRcG8CbwCDi/G+4EHx+XhtO+Pv3QA4JOGcO0MI\n38Y/+0WK/nPbS7wV82BgTAhhWwjhY+AB8sLcTiBmZk1CCJtCCB8mPN4U6BwfNzgzhLC5JJ8tUtUo\nhIlUXssS75hZVzN7ycy+NbONwA34L83CfJvw9RagbinObZ1YRwghAMsLe5Ni1liszwKWFlEvwDvA\nRuAkMzsAb1l7PKGWQ83sbTNbY2YbgAsKqKUgRdZhZiea2Yfxrr71eKtZcbvtWie+XwghB7+ebRLO\nKcmfW2GfsTbeWphracJnnA90B76MdzmeEH/8n3jL3JPmkxsmmMYiihRJIUyk8sq/LMI9wGd4S0V9\nYBxgKa7hG7x7CgAzM/YMDPmVpcZvgHYJ94tcQiMeCB/CW8DOBl4OISS20k0BngHahRAaAPcVs5ZC\n6zCz/fDuuz8BLUIIDYFXE953X0tZrAQ6JLxfBn59VxSjruJaCTQ1szoJj7XP/YwQwpchhNFAc+Av\nwDNmViuEsCOEMD6E0A04HO8OL/FMXZGqRCFMpOqoB2wAfoiPLSpqPFiyvAj0M7OT4q0il+NjmVJR\n45PAFWbWJj7I/ppivOYhfNzZz0joikyo5bsQwjYzG4h3BZa1jppADWANsCs+xuzohOdX4QGoXhHv\nPcLMjoyPA7sa2AR8WMj5+5JhZrUSjxDCYmAm8Eczq2lmffDWr0cAzOxsM2sab4XbgAfHHDM7yswO\njAfDjXj3ZE4p6xKpEhTCRKqOq4Bz8V/a9+CDr1MqhLAKH9h9O7AO2B/4H7A9BTXejY+v+hSYQd6A\n8aLqWwB8hIejl/I9fQnwJ/PZpdfiAahMdYQQ1gO/AZ4FvgNOw4Nq7vOf4a1vS+IzDJvnq3cufn3u\nxoPcMGBEfHxYaQwGtuY7wP/MYnjX5tPAtSGEt+PPnQDMi1+X24AzQgg78G7Mf+EBbC7eNflYKesS\nqRLMW+RFRFLPfBHUlcBpIYR3o65HRCRKagkTkZQys2Fm1jA+C3Es3k31UcRliYhETiFMRFLtcGAR\n3n32I+CUEEJh3ZEiIlWGuiNFREREIqCWMBEREZEIKISJiIiIRKDCrWbctGnTkJWVFXUZIiIiIvs0\na9astSGEAtdHrHAhLCsri5kzZ0ZdhoiIiMg+mVmhW6ipO1JEREQkAgphIiIiIhFQCBMRERGJQIUb\nEyYiIlJV7Ny5k+XLl7Nt27aoS5F9qFWrFm3btqV69erFfo1CmIiISJpavnw59erVIysrCzOLuhwp\nRAiBdevWsXz5cjp27Fjs16k7UkREJE1t27aNJk2aKIClOTOjSZMmJW6xVAgTERFJYwpgFUNp/pwU\nwkRERKRA69ev5+9//3upXnvCCSewfv36Yp8/fvx4brvttlJ9VkWlECYiIiIFKiqEZWdnF/nal19+\nmYYNG6airEpDIUxEREQKNGbMGBYuXEifPn24+uqrefvttxk8eDAjRoyge/fuAJx88sn079+fHj16\nMHny5N2vzcrKYu3atSxZsoRu3bpx4YUX0qNHD4477ji2bt1a5OfOnj2bgQMH0qtXL0455RS+//57\nACZOnEj37t3p1asXo0ePBuCdd96hT58+9OnTh759+7Jp06YUXY3k0+xIERGRiuCKK2D27OS+Z58+\ncOedhT49YcIEPvvsM2bHP/ftt9/m448/5rPPPts9C/D++++ncePGbN26lYMOOohTTz2VJk2a7PE+\n8+fP5/HHH+fee+/l9NNP55lnnuGss84q9HPPOecc7rrrLoYMGcK4ceP4wx/+wJ133smECRNYvHgx\nNWvW3N3VedtttzFp0iQGDRrE5s2bqVWrVlmvSrlRS1h+a9bA88/Dli1RVyIiIpJ2Dj744D2WYZg4\ncSK9e/dm4MCBLFu2jPnz5+/1mo4dO9KnTx8A+vfvz5IlSwp9/w0bNrB+/XqGDBkCwLnnnsv06dMB\n6NWrF2eeeSaPPPII1ap5O9KgQYO48sormThxIuvXr9/9eEVQcSotL++9B6NGwYwZMGBA1NWIiIi4\nIlqsylOdOnV2f/3222/z+uuv8/7771O7dm2OPPLIApdpqFmz5u6vMzMz99kdWZiXXnqJ6dOn88IL\nL3DzzTfz6aefMmbMGIYPH87LL7/MoEGDmDZtGl27di3V+5c3tYTlF4v5bQFJXkREpCqpV69ekWOs\nNmzYQKNGjahduzZffPEFH3zwQZk/s0GDBjRq1Ih3330XgIcffpghQ4aQk5PDsmXLGDp0KLfccgsb\nNmxg8+bNLFy4kJ49e3LNNddw0EEH8cUXX5S5hvKilrD89t/fbxcsiLYOERGRiDVp0oRBgwZx4IEH\ncvzxxzN8+PA9nh82bBj/+Mc/6NatG126dGHgwIFJ+dwHH3yQiy++mC1bttCpUyceeOABdu3axVln\nncWGDRsIIXDZZZfRsGFDxo4dy1tvvUVGRgY9evTg+OOPT0oN5cFCCFHXUCIDBgwIM2fOTO2HtG8P\nRx4JDz2U2s8REREpwrx58+jWrVvUZUgxFfTnZWazQggFjm9Sd2RBOndWd6SIiIiklEJYQWIxhTAR\nERFJKYWwgsRisG4dxBeHExEREUk2hbCCaIakiIiIpJhCWEEUwkRERCTFFMIK0qkTmCmEiYiISMoo\nhBWkVi1fpkIhTEREpETq1q0LwMqVKznttNMKPOfII49kX8tN3XnnnWxJ2ELwhBNO2L1fZFmMHz+e\n2267rczvkwwKYYXRDEkREZFSa926NU8//XSpX58/hL388ss0bNgwGaWlDYWwwuSGsAq2mK2IiEiy\njBkzhkmTJu2+n9uKtHnzZo4++mj69etHz549mTp16l6vXbJkCQceeCAAW7duZfTo0XTr1o1TTjll\nj70jL7nkEgYMGECPHj34/e9/D/im4CtXrmTo0KEMHToUgKysLNauXQvA7bffzoEHHsiBBx7InfE9\nNZcsWUK3bt248MIL6dGjB8cdd9w+96icPXs2AwcOpFevXpxyyil8H18VYeLEiXTv3p1evXoxevRo\nAN555x369OlDnz596Nu3b5HbORWXti0qTCwG69f7UhVNm0ZdjYiIVHFXXAGzZyf3Pfv0KXpf8DPO\nOIMrrriCX/7ylwA8+eSTTJs2jVq1avHss89Sv3591q5dy8CBAxkxYgRmVuD73H333dSuXZt58+Yx\nZ84c+vXrt/u5m2++mcaNG7Nr1y6OPvpo5syZw2WXXcbtt9/OW2+9RdN8v4NnzZrFAw88wIcffkgI\ngUMOOYQhQ4bQqFEj5s+fz+OPP869997L6aefzjPPPMNZZ51V6Pd3zjnncNdddzFkyBDGjRvHH/7w\nB+68804mTJjA4sWLqVmz5u4u0Ntuu41JkyYxaNAgNm/eTK1atYp7mQullrDCaIakiIhUcX379mX1\n6tWsXLmSTz75hEaNGtGuXTtCCFx77bX06tWLY445hhUrVrBq1apC32f69Om7w1CvXr3o1avX7uee\nfPJJ+vXrR9++fZk7dy6ff/55kTW99957nHLKKdSpU4e6desyatSo3Zt9d+zYkT59+gDQv39/lixZ\nUuj7bNiwgfXr1zNkyBAAzj33XKZPn767xjPPPJNHHnmEatW8vWrQoEFceeWVTJw4kfXr1+9+vCzU\nElaYxBB26KHR1iIiIlVeUS1WqfTjH/+Yp59+mm+//ZYzzjgDgEcffZQ1a9Ywa9YsqlevTlZWFtu2\nbSvxey9evJjbbruNGTNm0KhRI84777xSvU+umjVr7v46MzNzn92RhXnppZeYPn06L7zwAjfffDOf\nfvopY8aMYfjw4bz88ssMGjSIadOm0bVr11LXCmoJK1zHjpCRoZYwERGp0s444wymTJnC008/zY9/\n/GPAW5GaN29O9erVeeutt1i6dGmR73HEEUfw2GOPAfDZZ58xZ84cADZu3EidOnVo0KABq1at4pVX\nXtn9mnr16hU47mrw4ME899xzbNmyhR9++IFnn32WwYMHl/j7atCgAY0aNdrdivbwww8zZMgQcnJy\nWLZsGUOHDuWWW25hw4YNbN68mYULF9KzZ0+uueYaDjroIL744osSf2Z+agkrTI0akJWlECYiIlVa\njx492LRpE23atKFVq1YAnHnmmZx00kn07NmTAQMG7LNF6JJLLuH888+nW7dudOvWjf79+wPQu3dv\n+vbtS9euXWnXrh2DBg3a/ZqLLrqIYcOG0bp1a956663dj/fr14/zzjuPgw8+GIALLriAvn37Ftn1\nWJgHH3yQiy++mC1bttCpUyceeOABdu3axVlnncWGDRsIIXDZZZfRsGFDxo4dy1tvvUVGRgY9evTg\n+OOPL/Hn5Wehgs3+GzBgQNjX2iJJM2wYrFkDs2aVz+eJiIgkmDdvHt26dYu6DCmmgv68zGxWCGFA\nQeerO7IosRgsWKBlKkRERCTpFMKKEovBxo3eGiYiIiKSRAphRenc2W81LkxERESSTCGsKForTERE\nIlbRxm5XVaX5c1IIK0pWFmRmKoSJiEgkatWqxbp16xTE0lwIgXXr1pV4Ff2ULlFhZsOAvwKZwH0h\nhAn5nr8DGBq/WxtoHkJIn905q1f39cIUwkREJAJt27Zl+fLlrNHY5LRXq1Yt2rZtW6LXpCyEmVkm\nMAk4FlgOzDCz50MIu/cjCCH8JuH8XwN9U1VPqeVu5C0iIlLOqlevTseOHaMuQ1Ikld2RBwMLQgiL\nQgg7gCnAyCLO/wnweArrKZ3cEKamYBEREUmiVIawNsCyhPvL44/txcw6AB2BNwt5/iIzm2lmM8u9\nSTYWgx9+gG+/Ld/PFRERkUotXQbmjwaeDiHsKujJEMLkEMKAEMKAZs2alW9lmiEpIiIiKZDKELYC\naJdwv238sYKMJh27IkEhTERERFIilSFsBhAzs45mVgMPWs/nP8nMugKNgPdTWEvptW/vsyQVwkRE\nRCSJUhbCQgjZwK+AacA84MkQwlwzu8HMRiScOhqYEtJ1EZRq1aBTJ4UwERERSaqUrhMWQngZeDnf\nY+Py3R+fyhqSQstUiIiISJKly8D89BaLwYIFkJMTdSUiIiJSSSiEFUcsBlu3wsqVUVciIiIilYRC\nWHFohqSIiIgkmUJYcSiEiYiISJIphBVHu3ZQs6aPCxMRERFJAoWw4sjI0DIVIiIiklQKYcWlZSpE\nREQkiRTCiisWg4ULtUyFiIiIJIVCWHHFYrBtGyxfHnUlIiIiUgkohBWXZkiKiIhIEimEFZdCmIiI\niCSRQlhxtWkDtWophImIiEhSKIQVV0YGdO6sECYiIiJJoRBWElqmQkRERJJEIawkYjFYtAh27Yq6\nEhEREangFMJKIhaDHTvg66+jrkREREQqOIWwktAMSREREUkShbCSUAgTERGRJFEIK4lWraBOHYUw\nERERKTOFsJIw0zIVIiIikhQKYSWlZSpEREQkCRTCSioWg8WLITs76kpERESkAlMIK6nOnT2ALV0a\ndSUiIiJSgSmElZRmSIqIiEgSKISVlEKYiIiIJIFCWEm1aAF16yqEiYiISJkohJWUmWZIioiISJkp\nhJWGQpiIiIiUkUJYacRisGQJ7NwZdSUiIiJSQSmElUYsBrt2+XphIiIiIqWgEFYamiEpIiIiZaQQ\nVhoKYSIiIlJGCmGl0bQpNGigECYiIiKlphBWGlqmQkRERMpIIay0FMJERESkDBTCSisWg6+/hu3b\no65EREREKiCFsNKKxSAnBxYtiroSERERqYAUwkpLMyRFRESkDBTCSkshTERERMogpSHMzIaZ2Zdm\ntsDMxhRyzulm9rmZzTWzx1JZT1I1bgyNGsGCBVFXIiIiIhVQtVS9sZllApOAY4HlwAwzez6E8HnC\nOTHgd8CgEML3ZtY8VfWkhGZIioiISCmlsiXsYGBBCGFRCGEHMAUYme+cC4FJIYTvAUIIq1NYT/Ip\nhImIiEgppTKEtQGWJdxfHn8s0QHAAWb2HzP7wMyGFfRGZnaRmc00s5lr1qxJUbmlEIvBsmWwbVvU\nlYiIiEgFE/XA/GpADDgS+Alwr5k1zH9SCGFyCGFACGFAs2bNyrnEIsRiEAIsXBh1JSIiIlLBpDKE\nrQDaJdxvG38s0XLg+RDCzhDCYuArPJRVDJohKSIiIqWUyhA2A4iZWUczqwGMBp7Pd85zeCsYZtYU\n756sOKufKoSJiIhIKaUshIUQsoFfAdOAecCTIYS5ZnaDmY2InzYNWGdmnwNvAVeHENalqqaka9gQ\nmjZVCBMREZESS9kSFQAhhJeBl/M9Ni7h6wBcGT8qJs2QFBERkVKIemB+xacQJiIiIqWgEFZWsRis\nWAFbtkRdiYiIiFQgCmFllTs4X9sXiYiISAkohJWVZkiKiIhIKSiElZVCmIiIiJSCQlhZ1asHLVoo\nhImIiEiJKIQlg2ZIioiISAkphCWDQpiIiIiUkEJYMnTuDN9+C5s3R12JiIiIVBAKYcmgZSpERESk\nhBTCkkEzJEVERKSEFMKSoXNnv1UIExERkWJSCEuGunWhVSuFMBERESk2hbBk0QxJERERKQGFsGRR\nCBMREZESUAhLllgMVq+GjRujrkREREQqAIWwZNEMSRERESkBhbBkUQgTERGRElAIS5b99/dbhTAR\nEREpBoWwZKldG9q2VQgTERGRYlEIy+eHH+CeeyAnpxQv1gxJERERKSaFsHyefx4uvhieeaYUL1YI\nExERkWJSCMvn9NOhe3cYNw527Srhi2MxWLcOvv8+JbWJiIhI5aEQlk9mJtxwA3zxBTz6aAlfrBmS\nIiIiUkwKYQUYNQr69oXx42HHjhK8UCFMREREikkhrABmcNNNsHgxPPBACV7YqZO/eMGClNUmIiIi\nlYNCWCGOPx4OPRRuvBG2bSvmi2rVgnbt1BImIiIi+6QQVggzuPlmWLEC/vGPErxQMyRFRESkGBTC\nijB0KBx1FPzpT7B5czFfpBAmIiIixaAQtg833QSrV8NddxXzBbGYL1Gxbl1K6xIREZGKTSFsHw49\nFIYPhz//GdavL8YLNENSREREikEhrBhuvNED2O23F+NkhTAREREpBoWwYujbF047De64A9au3cfJ\nnTpBRoZCmIiIiBRJIayYbrgBtmyBW27Zx4k1akCHDgphIiIiUiSFsGLq1g3OPBP+9jf45pt9nKwZ\nkiIiIrIPCmEl8PvfQ3a2rx9WpNwQFkK51CUiIiIVj0JYCey/P/zsZzB5MixdWsSJsRhs3Ahr1pRb\nbSIiIlKxKISV0NixPu7+xhuLOEkzJEVERGQfFMJKqG1buPhi+Oc/i8hYCmEiIiKyDykNYWY2zMy+\nNLMFZjamgOfPM7M1ZjY7flyQynqS5Xe/g5o1Yfz4Qk7IyoLMTIUwERERKVTKQpiZZQKTgOOB7sBP\nzKx7Aac+EULoEz/uS1U9ydSiBVx2GTz+OHz2WQEnVK8OHTsqhImIiEihUtkSdjCwIISwKISwA5gC\njEzh55Wrq6+GevVg3LhCTtAyFSIiIlKEVIawNsCyhPvL44/ld6qZzTGzp82sXUFvZGYXmdlMM5u5\nJk1mHDZuDFddBc8+CzNnFnBC585apkJEREQKFfXA/BeArBBCL+A14MGCTgohTA4hDAghDGjWrFm5\nFliUK66AJk18xuReYjH44QdYtarc6xIREZH0l8oQtgJIbNlqG39stxDCuhDC9vjd+4D+Kawn6erX\nh2uugX//G957L9+TmiEpIiIiRUhlCJsBxMyso5nVAEYDzyeeYGatEu6OAOalsJ6U+OUvoWVLuO66\nfD2PCmEiIiJShJSFsBBCNvArYBoerp4MIcw1sxvMbET8tMvMbK6ZfQJcBpyXqnpSpXZtD2DTp8Pr\nryc80aEDVKumECYiIiIFslDBBo4PGDAgzCxwJHx0tm+HAw7wFrEPPgCz+BNdukDPnvD005HWJyIi\nItEws1khhAEFPRf1wPxKoWZNX6rio4/ghRcSntAyFSIiIlIIhbAkOeccX5Vi7FjIyYk/GIvBggVa\npkJERET2ohCWJNWrwx/+AHPmJPQ+xmKwZQusXBlpbSIiIpJ+FMKS6IwzoEcP75rMzkYzJEVERKRQ\nCmFJlJkJN94IX34Jjz6KQpiIiIgUSiEsyU4+Gfr3h/HjYUeLdlCjhkKYiIiI7EUhLMnM4KabYMkS\nuP/BTNh/f4UwERER2YtCWAr86EcwaJB3TW7t1EMhTERERPaiEJYCua1hK1fCP344CxYuTFi3QkRE\nREQhLGWOPBKOOQb+NOs4Nm/LhOXLoy5JRERE0ohCWArddBOs2bQfE7lMXZIiIiKyB4WwFDrkEDjp\nmC3cytWs/2Rp1OWIiIhIGlEIS7EbbqnFehrxl6faRV2KiIiIpJFihTAzu9zM6pv7PzP72MyOS3Vx\nlUGffhmcXv/f3DnjcNasiboaERERSRfFbQn7WQhhI3Ac0Ag4G5iQsqoqmT8c9CJbdtXglluirkRE\nRETSRXFDmMVvTwAeDiHMTXhM9qFrv9qcnfEokyYF7eUtIiIiQPFD2CwzexUPYdPMrB6gha+KKxZj\nXM54srPh5pujLkZERETSQXFD2M+BMcBBIYQtQHXg/JRVVdnEYnRiMRcMW8699/qWRiIiIlK1FTeE\nHQp8GUJYb2ZnAdcDG1JXViUTiwFw3cA3yciAG26IuB4RERGJXHFD2N3AFjPrDVwFLAQeSllVlU3r\n1lC7Nm3XzubSS+HBB+Grr6IuSkRERKJU3BCWHUIIwEjgbyGESUC91JVVyZhB584wfz5jxsB++8Hv\nfx91USIiIhKl4oawTWb2O3xpipfMLAMfFybFFYvB/Pk0bw6XXw5TpsCcOVEXJSIiIlEpbgg7A9iO\nrxf2LdAWuDVlVVVGsRgsWgTZ2fz2t9CgAYwbF3VRIiIiEpVihbB48HoUaGBmJwLbQggaE1YSsRhk\nZ8PSpTRqBL/9LUydCjNmRF2YiIiIRKG42xadDnwE/Bg4HfjQzE5LZWGVTnyGJPPnA94l2aSJh7Ht\n2yOsS0RERCJR3O7I6/A1ws4NIZwDHAyMTV1ZlVC+EFavHtx2G0yfDiefDFu2RFibiIiIlLvihrCM\nEMLqhPvrSvBaAWjRAurW3R3CAM47D+67D6ZNgxNOgE2boitPREREyle1Yp73bzObBjwev38G8HJq\nSqqkzHbPkEz0859D7dpw9tlwzDHwyivQuHFENYqIiEi5Ke7A/KuByUCv+DE5hHBNKgurlOJrheX3\nk5/AM8/A7NkwdCisWhVBbSIiIlKuit2lGEJ4JoRwZfx4NpVFVVqxmG8cuXPnXk+NHAkvvggLFsAR\nR8CyZeVfnoiIiJSfIkOYmW0ys40FHJvMbGN5FVlpxGKwa1ehO3gfeyy8+ip8+y0MHgwLF5ZveSIi\nIlJ+igxhIYR6IYT6BRz1Qgj1y6vISiPfDMmCDBoEb74Jmzd7EPv883KqTURERMqVZjiWp2KEMID+\n/eGddyAE75r8+ONyqE1ERETKlUJYeWrWDOrX32cIA+jRA959F+rU8cH6//1vOdQnIiIi5UYhrDwV\nskxFYTp3hvfe8yXGjj0WXn89xfWJiIhIuVEIK28lCGEA7dr5qvr77w/Dh8MLL6SwNhERESk3CmHl\nLRaDpUthx45iv6RlS3j7bejdG0aNgieeSF15IiIiUj4UwspbLAY5ObBoUYle1rixd0cedpgv7vp/\n/5ei+kRERKRcKISVt2LOkCxI/fq+rdFxx8EFF8Bf/5rk2kRERKTcpDSEmdkwM/vSzBaY2ZgizjvV\nzIKZDUhlPWmhDCEMfJ/JqVcEnE0AACAASURBVFPhlFPgiivgj39MYm0iIiJSblIWwswsE5gEHA90\nB35iZt0LOK8ecDnwYapqSStNmkCjRqUOYQA1a8KTT8JZZ8F118HvfudriomIiEjFUS2F730wsCCE\nsAjAzKYAI4H8a8DfCNwCXJ3CWtJLCWdIFqRaNXjwQahbFyZMgE2bYOJEyFAHs4iISIWQyl/ZbYDE\nbaiXxx/bzcz6Ae1CCC+lsI70k4QQBh64/v53+O1vYdIk+PnPITs7CfWJiIhIyqWyJaxIZpYB3A6c\nV4xzLwIuAmjfvn1qCysPsRg89hhs2wa1apXprczgz3+GevXg97+HH36ARx6BGjWSVKuIiIikRCpb\nwlYA7RLut40/lqsecCDwtpktAQYCzxc0OD+EMDmEMCCEMKBZs2YpLLmcxGI+iGvhwqS8nRmMGwd/\n+Qs89ZQP2t+6NSlvLSIiIimSyhA2A4iZWUczqwGMBp7PfTKEsCGE0DSEkBVCyAI+AEaEEGamsKb0\n0Lmz3yahSzLRlVfCPff4MhbDh/s4MREREUlPKQthIYRs4FfANGAe8GQIYa6Z3WBmI1L1uRVCGZep\nKMpFF8HDD/tWR8cdB99/n/SPEBERkSRI6ZiwEMLLwMv5HhtXyLlHprKWtNKokS9VsWBBSt7+zDN9\nPbHRo2HoUHj1VWjePCUfJSIiIqWkBQ2ikqQZkoU55RTf7Purr2DIEFixYt+vERERkfKjEBaVFIcw\n8O7IadM8gA0eDG+95RMyRUREJHoKYVGJxWD5ctiyJaUfM3gwvPEGbNgARx0FDRrAoEFwzTXeUrZu\nXUo/XkRERAqhEBaV3MH5SVqmoigHHeTDz557Di67zFfHuOMOGDECmjaFHj3gF7/wAf2LF2sLJBER\nkfIQ2WKtVV7iDMmePVP+cY0awciRfoCvIzZjBrz3nh9PPAGTJ/tzrVvD4Yd7i9nhh0OvXr5NkoiI\niCSPfrVGJYXLVBTHfvvBEUf4AbBrF8yd64HsP//x2yef9Ofq1oVDD/VAdvjhcMghUKdOJGWLiIhU\nGgphUalf39eNiCiE5ZeZ6S1evXrBpZf6Y19/nRfI3nsPxo/3rsrMTOjXLy+UDRoELVpEWr6IiEiF\nY6GCDQAaMGBAmDmzkiyqf/jhnmjeeSfqSopl/Xp4//28YPbhh3mzLTt3zgtlgwfDAQdEW6uIiEg6\nMLNZIYS9tmQEDcyP1oEHwkcf+eCsCqBhQzj+eLjpJnj7bZ9x+f77cOutPrj/hRfgggugSxcYNgw+\n/jjqikVERNKXWsKi9O23Ptjqhx88zey/f9QVlUkI8OWXPgvz1lvhu+/gtNPgxhuha9eoqxMRESl/\naglLVy1bwr//DTk53nS0Zk3UFZWJmYetMWNg0SIYO9Y3E+/RA372M1i6NOoKRURE0odCWNS6dPF+\nvOXL4cQTvVWsEmjQAG64wcPY5ZfDY4/5OLHLL4dVq6KuTkREJHoKYeng0ENhyhSYOdN33c7Ojrqi\npGneHG6/3SeBnnMOTJrkva7XX+8D/UVERKoqhbB0MXIk/O1v8OKLvkZEBRurty/t2sG998Lnn3uD\n3803Q8eOMGFCpWn8ExERKRGFsHRyySVw7bWeVm66KepqUuKAA7zR73//g8MOg9/9zpe3mDQJduyI\nujoREZHyoxCWbm66yfvtxo2DBx6IupqU6dMHXnoJ3n3XNw/41a98eNxDD/nq/SIiIpWdQli6MfOW\nsGOPhQsv9OmFldjhh/tata+84vtbnnuur9r/7LOVrkdWRERkDwph6ahGDXjmGU8jP/6xD9ivxMx8\nhY6ZM+Gpp7wlbNQoOPhgeO01hTEREamcFMLSVb163l/XtCkMH+5rPVRyGRm+uOtnn8H998Pq1XDc\ncXDUUfDBB1FXJyIiklwKYemsVStfzDU7u1Is5lpc1arB+efDV1/BX//qMyoPPdQnkH76adTViYiI\nJIdCWLrr2hWefx6WLYOTToItW6KuqNzUrAmXXQYLF/p8hXfegd694cwzYcGCqKsTEREpG4WwimDQ\nIF9y/qOP4Cc/qVSLuRZH3bpw3XXeI/v//p8P2u/WDS6+GJYs0ZgxERGpmLSBd0UyaZKv5fCLX8Dd\nd/uI9irom298sdfJk2HnTg9pnToVfHToALVqRV2xiIhUVUVt4F2tvIuRMvjlL32PyQkTfAn6666L\nuqJItGrlmwtcdZVvu7lokR/z58O0abB1a965ZtCmTeEhrXnzKptlRUQkYgphFc0f/+hB7PrrPV2c\nd17UFUWmY0cfM5YoBN8gPDeYJR6vvQYrVux5fu3ahQe0rCzYb79y+3ZERKSKUQiraMzg//4Pvv3W\nF3Nt1Qp+9KOoq0obZtCypR+HHbb389u2+TiygkLaG2/svY9l69Z5oax7dzjoIBgwAOrXL5dvR0RE\nKjGNCauoNm6EI47waYLvvAP9+0ddUYUXgq8CUlBAW7jQGyDBg16XLr6Y7EEH+W3v3j6bU0REJFFR\nY8IUwiqylSt9Aa3t2+H9971/TlJm3TqYMcOPjz7y21Wr/Lnq1T2I5Yaygw7y1UUyM6OtWUREoqUQ\nVpnNm+dLWDRrBv/5j6+wL+UiBG8dyw1kH33kWy9t2uTP163rDZSJLWbt22sigIhIVaIQVtm99x4c\ncwz06wevv+6jzSUSOTnw5Zd7tpbNng07dvjzzZp5GMsNZgcdpNwsIlKZKYRVBf/6l2+8OGKEb/6t\nfrC0sX27b7eU2GI2b17eIrMdO+7ZWtavH9SpE23NIiKSHAphVcVdd/maDZdc4gu7qt8rbW3aBLNm\n7dlitnSpP5eZCaeeCtdc44GsMvjmm7yZqyIiVUlRIUzbFlUmv/617+tz992+oKukrXr14Mgj4eqr\n4amnfNmMVavgxRfh8st93/b+/eHYY319swr2f6XdPvgARo/2tYV79ID//jfqikRE0odCWGXzpz/5\nDtfXXgsPPRR1NVICzZvD8OHwl7/A11/DLbfA3Llw3HG+NtkTT1SMbUN37oQpU2DgQJ+8+8orvttW\n48Zw9NEwdWrUFYqIpAeFsMomIwPuvx+OOgp+/nN49dWoK5JSaNDAGzUXL4b77vNFZEeP9vXJ7r57\nz62Z0sW6df5/gI4dfZ/5deu8h3z5crjzTm8F69ULRo2Ce+6JuloRkegphFVGNWr4QP3u3X1w0f/+\nF3VFUko1a3qW/vxz/yNt1gwuvdQ3Jr/pJvjuu6gr9Na6X/wC2rb1Bthu3XxPzy+/9BawevX8vGbN\n4M03YdgwuPhiGDeu4naziogkg0JYZdWggfcDNWoEJ5zgg46kwsrIgFNO8TV5337buyfHjvV1x668\nEpYtK996cnLg5Ze9q/TAA73n+6yzfBboa6/BiSd6zfnVqePdkT/7Gdx4o++8VRG6WEVEUkEhrDJr\n3dpHeG/b5s0P69ZFXZGUkRkMGeIB6JNPPJhNnOh7W557rrdKpdLmzT7xtls3H782dy7cfLOHwHvv\n9UC2L9WqeRfr9df7Nqgnn7z3np0iIlWBQlhl1707PP+8t4T17QsvvRR1RZIkvXrBww/7vpaXXgpP\nP+0h6KSTfP3eZFq61Gdytm3rXYwNGsCjj/qYtWuvLfmCs2beEnb33d5ge9RRvm+niEhVktIQZmbD\nzOxLM1tgZmMKeP5iM/vUzGab2Xtm1j2V9VRZgwfDW2/54JwTT4QzzoBvv426KkmSDh3gr3/1GZV/\n+IN3WQ4e7LtZTZ3qXYelEYKHudNO85a2O+6AH/3IB9h/+CH89Kc+/LAsLr7Y1xaeM8frXbSobO8n\nIlKRpCyEmVkmMAk4HugO/KSAkPVYCKFnCKEP8Gfg9lTVU+UdeqgP0L/xRnjuOe9Puu++0v+GlrTT\npIkPdv/6a5+VuHKld/UdeCA88EDe1kn7smMHPPKIr+A/eLAPpr/6am/1euIJ/1FK5jrAJ5/su22t\nXQuHHQYff5y89xYRSWepbAk7GFgQQlgUQtgBTAFGJp4QQtiYcLcOoLlSqVSjhg/EmTPH+7IuvBCG\nDvVpbFJp1K7tXYbz53uXYY0aPhC+Y0e47TbYuLHg161e7Rm9Qwc4+2wfp3X33T7ea8IEX3A1VQYN\n8v3na9b0MW+vvZa6zxIRSRepDGFtgMQ5W8vjj+3BzH5pZgvxlrDLUliP5OrSxbsn7703L5DdeGPx\nm0qkQqhWzbsM//c/n5/RpYu3aLVv7+O4Vq3y8+bM8WUw2rf3lrQ+ffz8uXO9u7C89rHs1s27Ujt1\n8gm9jzxSPp8rIhKVyAfmhxAmhRD2B64Bri/oHDO7yMxmmtnMNRq9mxwZGXDBBb6T9Cmn+G/fvn21\nr0wlZOZjud580/epPOYYb9nq0MG7HHv39hXuzz/f1yN75RU/v6AlJlKtdWuYPh0OP9xb4/78Z60l\nJiKVVyr/mV0BJHZgtI0/VpgpwMkFPRFCmBxCGBBCGNCsWbMklii0bOm/gV980XeVPvxwn2q3YUPU\nlUkKHHSQz6L84gtf0mLnTt8eadky73rs1i3qCn3m5b//7fNHrrkGrrhCQxdFpHKqlsL3ngHEzKwj\nHr5GAz9NPMHMYiGE+fG7w4H5SDSGD/dmkLFjfeGpqVPhb3/zVjKpdA44IL23DqpZEx57zFvG7rgD\nvvnGF4StVSvqykREkidlLWEhhGzgV8A0YB7wZAhhrpndYGYj4qf9yszmmtls4Erg3FTVI8VQt67/\nxvvgA99jZtQoD2ErimrAFEmNjAy4/XafTPDUU77e8Pr1UVclUrWEkLcjxjvvRF1N5WOhgg24GDBg\nQJg5c2bUZVR+O3f6b8Dx46F6dR9EdPHF0QwUkirvscfgvPN8csG//w1t9priU7nk5PjEiaVLfZ3l\npUth+3bIyso72rSBzMxo65TKbfVq3xf2uee8dXr7djj2WJ/HdcghUVdXcZjZrBDCgAKfUwiTIi1c\n6OHr9dd9gah774UePaKuSqqgN97whtmGDT2Ida/ASzvv2uUNzIkhK/Hrr7/2X3hFqVbNlw3p2HHP\ncJaV5Y+1aqWQJqX37LMewDZs8K3JLrnEhzD86U++pt9JJ3kY69076krTn0KYlE0Ivl7Ab37ji0xd\ncw1cd50G6Ei5mz0bjj/et0N94QWfR5KOduzwyQ75w1Xu7fLle29c3qKFz1jNytrzNveoUcPD2ZIl\nfixenPf1kiU+bi5R9eq+7Ej+cJb7datWatiWvW3YAJdd5l2Qffv6beKesJs2+bDhW2/1c884w3fq\n6NIluprTnUKYJMeaNXDVVb5h4QEHwOTJvrKmSDlavNjHhy1d6t2Uo0aVfw0h+I4Ec+fuHbCWLPHn\nEv9pNfPuw9xAlT9otW8P++1Xtpq2bfOQlj+c5R75dyqrUSMvpOVvTevdu/zWh6sstm71RYZfeMFb\nKK+4AurXj7qqknnjDV+qZuVKX0vw+usL35rs++/hL3+BO+/07/2cc+D3v/efH9mTQpgk16uvehfl\n4sW+yuett0KjRlFXJVVIbnfIhx/6JN5LL03dZ23f7hOHP/nEF7b95BM/1q3LOycz03/xFtSKlZXl\nG5+XdZ/Nstq6NS8kJh65oW316rxza9Xy9eRGjvTr3KJFJCWnvY0b4eWX4V//8tsffvAtejdt8m3E\nrrvOu/HSvdNgyxYYM8a3O+vSxVu/Dj64eK9dvdqHDP/97z6W8YILPLy1bp3amisShTBJvi1bfND+\n7bdD06a+g/Tppyd3U0GRImzZ4l0hL77o/2u/6aay//h9+21eyMoNXF98kdd1uN9+3jXTu7dvNNGz\np6/w37q1j9GqyLZs8ZC2YIEPAZ061e+bwcCBMGKEh7KuXav2X/O1a+H55z14vfaadz23aOF7oI4a\nBUce6T87117rz7dv7911Z5+dnmP0PvzQW7G++gouvxz++Eff+qykVqzwv4P33ed/F375Sx+5oqU9\niw5hhBAq1NG/f/8gaeTjj0Po3z8ECGH48BCWLIm6IqlCdu4M4cIL/cfvvPNC2LGjeK/bvj2ETz4J\n4aGHQrjqqhCOPTaE5s39fXKPtm39R/raa0OYMiWEefNCyM5O7feTTnJyQpg9O4Qbbsj7Kw4hxGIh\n/Pa3Ibz7btW5HsuXh3DXXSEMHRpCRoZfhw4dQvjNb4q+Dq+/HsKAAX5+jx4hTJ3q1zUdbN8ewnXX\n+ffTvn0Ib7yRnPddtCiEc8/1961bN4Trrw/h+++T894VFTAzFJJpIg9VJT0UwtLQzp0h3HFHCHXq\n+HH77SFs2xZ1VVJF5OSEMH68/2t2/PEhbNq05/OrV4fw2msh3HZbCOecE0Lv3iFUr54XKmrWDKFf\nvxDOPz+EO+8M4c03Q1i7NprvJZ0tWxbCpEkhHHdc3vVr1syv27PPhvDDD1FXmFzz54fw5z+HMHBg\n3s9Kt24eXGbNKn6YyskJ4cknPbxCCIcdFsL06amtfV8+/TSEPn3y/vOyfn3yP+Pzz0M4/XT/jIYN\nQ7j55r3/blYVRYUwdUdK8ixd6gMgXnnFB0Scey5ceKH3X4ik2L33+lDFfv3gqKPyuhMTZw22auVd\nibndib17+xiYit6VWN42bIBp07zL8qWX/H6tWr6G1MiRcOKJFW8cWQjw2Wfezfivf/nPD0D//nnr\nVpdlW6+dO+GBB3wUxzff+CYlf/yj/xyWl127fATJ9df7Ui+TJ/ufVyrNnu0bsbz4ondN/u53FWOc\nXDJpTJiUnxB8is099/gKf9nZPoPyoov8X7Kq9DdPyt0LL8Do0f4Lr3v3vQOXxqck386dvun61Kl+\nfP21jxk79NA9x5Glo5wcmDEjL3gtWOC1DxoEp57q47ySPdtvyxYfAD9hgofXM8+EG27wGaqptHCh\nL3j83nseKO+5p3z/PnzwgYex11/3mcJjx/pMzKgnrJQHhTCJxqpV8M9/+n+3Fi2Cxo3zWsfSYado\nqZS2bPGWrarwj3u6CcFbH59/3gPZxx/74wcc4GFs5Egf5B/lAPXsbHj3XQ9dzz7rA8qrVfPW01Gj\nvMaWLVNfx/ffexCbONFbqC65xGdTNm+e3M8Jwf8Jvuoq/z7vusu3IIpqcsXbb/v3+d//evAcP96D\naDpOWkgWhTCJVk4OvPmm/0vw7LP+r+Dgwb4c86mnqnVMpJJatiwvkL39treaNWvm3ZUjR3r3ZUEz\n8ULwfzZKexT0+oULPXhNnerLi9Sq5evNjRrl9US1ys6KFT578v77ffbtVVf5Ua9ect77ggt8h4lj\njvHPaNeu7O9bViF4Tddf70G9a1dvDTz11Mq5gLBCmKSPVavgwQc9kC1c6K1j55zj3ZVqHROptDZs\n8F+8U6f6mlobNnjrR/XqewemVKlf3wPXqFEewNJpQdovv/RQ8vTTvurP9df7GMeaNUv3flOm+Pp5\n27b5Uo6XXJJ+AScE/3/52LG+Fl+fPr4V0vDheS11IXh4T8aRnb33Y4MHF39NtNJSCJP0k5MDb72V\n1zqW+7fhoov8v0NlXT5cRNLWjh0+jiy3dSwjo2yH2b7PadIEjjii9KGmvMyY4Qunvvmmj0e74Qb4\n6U+L3123bp2Hryef9K7fBx/07uB0tmsXPP64d00uXOitozk5/rOxa1dqP3vCBF/PLJUUwiS9rV6d\n1zq2YIH3C+S2jlXkXZpFREohBB/APmaMd9f17OkzKRNbiAry0kve/bhunQea//f/KtbM3507fZvi\nOXO8hTRZR7VqhT+3335+m0oKYVIx5OT4f40nT/bBGzt3+jSlX/wCTjtNrWMiUqXk5MBTT3nX5IIF\nvmH9hAn+z2KiTZvgyit9tfoDD/Ttffv0iaZm2VtRISzNeoilSsvI8ClKU6b4iNJbb/VWsnPO8X1h\nLr/cd0wWEakCMjJ8a67PP4e7784LYiNG+Jpm4N26vXr5oPtrroGZMxXAKhK1hEl6C2HP1rEdO/y/\ngRddBD/+sVrHRKTK+OEH36b3llu89evII/2fx06dfERH/hYySQ9qCZOKywyGDvVRm8uXw223wZo1\nvt5Y69a+S+wbb3jXpYhIJVanjm8MvmiRL2Px0Uc+WmP2bAWwikotYVLxhOBt8Pfc4zMrt23zPTiG\nD/fFh4YNS84iOyIiaSwnJ/2WnZC9FdUSVoHmTYjEmflWSEOGePv8a6/54kMvvACPPupLpR99tO85\nMmJE+Sx/LSJSzhTAKj61hEnlkZ3te2E895yHskWL/PGBA72F7OST03cTOxERqZS0RIVUPSH4TMrc\nQJb7M3PAAR7Gcjex038lRUQkhRTCRJYv903snnvOV+rPzoYWLeCkkzyUHX209rAUEZGkUwgTSbR+\nPbzySt4mdps2+bSjYcO8hWz4cN/TUkREpIwUwkQKs327L7QzdaofK1f6Jm1HHOGBbORI38BNRESk\nFBTCRIojJwdmzcobR5a7On/v3t5lOXw49OtX/J10RUSkylMIEymNBQvyWsj+8x8PaQ0b+uKxRx/t\nR5cuRe+oKyIiVZpCmEhZrVnjK/O/8Qa8/josWeKPt2njYeyYY/y2detIyxQRkfSiECaSbIsWeRjL\nDWbr1vnj3brltZIdeaS3nImISJWlECaSSjk5MGdOXivZ9OmwZYuvQTZgQF4r2WGHaRkMEZEqRiFM\npDzt2AEffJAXyj78EHbt8gB2+OF53Zd9+2qQv4hIJacQJhKljRu9dSy36/LTT/3x3EH+uS1lBxyg\nQf4iIpWMQphIOlm1Ct5801vJXn8dvv7aH2/bNm882cEHQ+fOaikTEangFMJE0lUIsHBhXivZG2/A\nd9/5c7VrQ69e0KePH717Q8+evrq/iIhUCAphIhVFTo53V/7vfzB7dt6xYYM/b+bdlonBrE8faNlS\nXZkiImmoqBBWrbyLEZEiZGR4sOrdO++xELzLMjGUffQRPPFE3jnNm+8dzA44AKrpr7iISLrSv9Ai\n6c4MOnTwY+TIvMfXr/elMWbPhk8+8ds77/TZmeCzMXv23DOY9eoF9epF832IiMge1B0pUpns3Alf\nfLFnMJs9O28xWfAB//mDWbt26s4UEUkBjQkTqcpCgBUr9gxls2f73pi5GjTwMJZ4HHgg1K0bXd0i\nIpWAxoSJVGVmvvxF27YwfHje45s2+SSAOXP8+PRTePhhX9cs1/777x3OOnXysWsiIlImKQ1hZjYM\n+CuQCdwXQpiQ7/krgQuAbGAN8LMQwtJU1iQicfXq+VZKhx2W91juJIDcYJZ7TJ3qMzfBl87o2XPP\nYNazJzRqFM33ISJSQaWsO9LMMoGvgGOB5cAM4CchhM8TzhkKfBhC2GJmlwBHhhDOKOp91R0pEoGt\nW+Hzz/cMZp98sudYs3bt9m410wxNEaniouqOPBhYEEJYFC9iCjAS2B3CQghvJZz/AXBWCusRkdLa\nbz/o39+PXCHAt9/u3Wr26qs+QQCgRg3o0cMDWY8e0LUrdOniXZoKZyJSxaXyX8E2wLKE+8uBQ4o4\n/+fAKwU9YWYXARcBtG/fPln1iUhZmEGrVn786Ed5j+/YAV9+uXcwe/DBvHOqV/fxZl275gWz3Ft1\na4pIFZEW/xU1s7OAAcCQgp4PIUwGJoN3R5ZjaSJSUjVq+Bixnj3hzDPzHl+/3sPZF1/sefvSS3kt\nZ+ALz+YPZl27QlaW9tIUkUollSFsBdAu4X7b+GN7MLNjgOuAISGE7SmsR0Si1LAhHHKIH4mys2Hx\n4r0D2rPPwtq1eefVqAGx2N4BrUsXX2JDRKSCSWUImwHEzKwjHr5GAz9NPMHM+gL3AMNCCKtTWIuI\npKtq1TxcxWJw4ol7PrdunYey3GD2xRfw2Wfw3HOwa1feeS1b7tm12bmzjzvLyvLZnCIiaShlISyE\nkG1mvwKm4UtU3B9CmGtmNwAzQwjPA7cCdYGnzFfr/jqEMCJVNYlIBdOkyd7LaIB3Xy5alBfMckPa\nE0/A99/veW7Llh7Ico+OHfO+bt1aa56JSGS0Yr6IVB4heBfmokV7HosX++2yZXnrnYF3cWZlFR7S\n6teP7FsRkcpBK+aLSNVgBs2a+ZF/7Bn4zM2vv94zmOUeH364dytakyZ7hrLEoNaunc/yFBEpJYUw\nEak6atTw8WKdOxf8/Pff7xnOcr/++GP41798EkGuzEwPYllZfnTosOdt27YKaSJSJIUwEZFcjRr5\n0a/f3s/t2uUboefv6ly6FF57DVau9O7QXBkZ0KbN3uEs97Z9e6hZs5y+MRFJRxoTJiKSDDt2+Jiz\nJUs8mOW/Xb58zxmd4AvdFhbSOnTQzE6RSkBjwkREUq1GDd8FYP/9C34+O9tb0goKaTNmwDPP7Llo\nLfjYtsSWs3btvJsz92jZUts/iVRg+tsrIlIeqlXzQNWhQ8HP79rle3HmhrPEoPbpp76zwNate74m\nI8Nb0xKDWf6jdWsPiCKSdhTCRETSQWamjyFr0wYGDdr7+RB84sDy5QUfn38O06bB5s17v7ZFi7xQ\n1qbN3kGtTRt1fYpEQCFMRKQiMIPGjf3o1avw8zZuLDyoLVoE06fvvRQH+Psmtp61bp23QXvu0bKl\nZnyKJJFCmIhIZVK/PnTv7kdhfvjBx6cVFtZmzYLVq/ec7QkeBJs23TOYFRTWWrWCWrVS+32KVAIK\nYSIiVU2dOnDAAX4UJjvbg9jKlfDNN3lH4v3PPvNxbPlnfYIv9VFUUMt9rE6d1H2fImlOIUxERPZW\nrVpet2RRdu3yraIKCmm59995x7/OP/sTPIS1aAHNm/ttUV83bOitcSKVhEKYiIiUXmZmXkjq06fw\n80KA777bO6StWuUtbqtWwcKF8P77sGbN3l2h4OPRihPWWrTwbtPMzNR93yJJoBAmIiKpZ+Z7cTZp\nAj17Fn1ubutabjhLDGqJX3/6qX+9Y0fBn9e0aeFhLf+txrBJBBTCREQkvSS2ru0rsIUAGzbsO7B9\n9JF/vWlTwe9Tv37hAS1/iGvQQN2ikhQKYSIiUnGZ+Vixhg2LnmiQa8sW7+7MH9ISb7/6Ct59F9at\nK7hbtEaNogNbs2Z5O515kgAAClBJREFUy4k0buyhLSMj+d+7VHgKYSIiUnXUrl30zgWJsrP37BYt\nqIVt9WrvFl21quCJB5AXFBODWWFHo0Z7fq3dDio1hTAREZGCVKvmC9S2bLnvc3O7RVet8ha0777L\nO77/fs/7333nkxBynyuotS1X3bpFh7WmTb3lLfGoV0/dpRWEQpiIiEhZJXaLlkROju9ykD+kFRbg\nPv887+uCJiQA1Ky5ZzjL7SIt7NDSH5FRCBMREYlKRkZeeOvUqfivC8F3Pli71se4FXUsXOi3hU1K\nqFat4Ba1/EeTJn5ekybavipJFMJEREQqGjPvqqxbF7KyiveabdvyQtvq1YWHto8/9tv16wt/r/r1\nPZDlhrL8X+d/rEkTjW8rgEKYiIhIVVCrVt4m7cWxc+eeLW3r1vn93Nvcr1evhnnz/P7mzYW/X/36\nhYe0xK8bNfJzc49KHN4UwkRERGRv1avn7fVZXNu37x3SCgpua9fCF1/414V1k+aqUWPPUFavXuH3\ni3qubt2020VBIUxERESSo2bN4u05mmj7dp9okBvS1q/3yQobN3pAy/068f4338CXX+bd37q1eJ9V\np86eIe3Xv4azzy7d95oECmEiIiISnZo1S97ilt/OnXmBrLDgVtD9mjWT932UgkKYiIiIVGzVq+et\noVaBaB8FERERkQgohImIiIhEQCFMREREJAIKYSIiIiIRUAgTERERiYBCmIiIiEgEFMJEREREIqAQ\nJiIiIhIBhTARERGRCCiEiYiIiERAIUxEREQkAgphIiIiIhFQCBMRERGJgIUQoq6hRMxsDbA0xR/T\nFFib4s+oKHQt8uha5NG1cLoOeXQt8uha5NG1gA4hhGYFPVHhQlh5MLOZIYQBUdeRDnQt8uha5NG1\ncLoOeXQt8uha5NG1KJq6I0VEREQioBAmIiIiEgGFsIJNjrqANKJrkUfXIo+uhdN1yKNrkUfXIo+u\nRRE0JkxEREQkAmoJExEREYlAlQ5hZjbMzL40swVmNqaA52ua2RPx5z80s6zyrzL1zKydmb1lZp+b\n2Vwzu7yAc440sw1mNjt+jIui1vJgZkvM7NP49zmzgOfNzCbGfy7mmFm/KOpMJTPrkvBnPdvMNprZ\nFfnOqbQ/E2Z2v5mtNrPPEh5rbGavmdn8+G2jQl57bvyc+WZ2bvlVnRqFXItbzeyL+M//s2bWsJDX\nFvl3qaIp5FqMN7MVCX8PTijktUX+vqloCrkWTyRchyVmNruQ11aqn4syCSFUyQPIBBYCnYAawCdA\n93znXAr8I/71aOCJqOtO0bVoBfSLf10P+KqAa3Ek8GLUtZbT9VgCNC3i+ROAVwADBgIfRl1ziq9H\nJvAtvtZNlfiZAI4A+gGfJTz2Z2BM/OsxwC0FvK4xsCh+2yj+daOov58UXIvjgGrxr28p6FrEnyvy\n71JFOwq5FuOB3+7jdfv8fVPRjoKuRb7n/wKMqwo/F2U5qnJL2MHAghDCohDCDmAKMDLfOSOBB+Nf\nPw0cbWZWjjWWixDCNyGEj+NfbwLmAW2irSqtjQQeCu4DoKGZtYq6qBQ6GlgYQkj1IslpI4QwHfgu\n38OJ/x48CJxcwEt/BLwWQvguhPA98BowLGWFloOCrkUI4dUQQnb87gdA23IvLAKF/FwUR3F+31Qo\nRV2L+O/J0/9/e3cbIlUVx3H8+6u1JA3LHnzqUUsoqdYSKV2lJyQjpELIMtMMQlDCV0UYBL6LoF6E\nRfRotUT0YEkYmUZShGna+lSmvokUWyFCs0hK/724Z3IaZ3TS3bnN3N8HLnvnnLOz5x7OPfPfe+6d\nA7zZ0Eo1oSIHYcOAH8te7+TIwOOfMmnA2Quc1ZDa5SRNuY4GvqqSfZ2kDZI+kjSqoRVrrACWS1on\n6cEq+fX0nVYyjdqDaVH6BMCgiNid9n8CBlUpU7S+ATCb7MpwNcc6l1rFvDQ1+3KNaeqi9YsJQHdE\nbK+RX5R+cUxFDsKsgqT+wLvA/IjYV5G9nmw66irgGeD9RtevgToi4mpgMjBX0sS8K5QXSacAU4C3\nq2QXqU/8S2RzKoV/tFzSAuAvoLNGkSKcS88BI4B2YDfZNFzR3c3Rr4IVoV/UpchB2C7g/LLX56W0\nqmUktQEDgJ8bUrsGk9SHLADrjIj3KvMjYl9E7E/7y4A+ks5ucDUbIiJ2pZ97gCVkUwnl6uk7rWIy\nsD4iuiszitQnku7StHP6uadKmcL0DUmzgNuA6SkoPUId51LTi4juiDgYEYeAF6h+jEXqF23AncBb\ntcoUoV/Uq8hB2FrgUkkXp//2pwFLK8osBUpPN00FPq012DSzNH//EvBdRDxVo8zg0v1wksaS9Z2W\nC0gl9ZN0emmf7AbkzRXFlgL3packrwX2lk1TtZqa/9EWpU+UKR8PZgIfVCnzMTBJ0plpWmpSSmsp\nkm4BHgamRMTvNcrUcy41vYr7Qe+g+jHW83nTKm4GtkbEzmqZRekXdcv7yYA8N7Kn3LaRPbWyIKUt\nJBtYAPqSTcPsANYAw/Oucy+1QwfZ1MpGoCtttwJzgDmpzDxgC9lTPauBcXnXu5faYng6xg3peEv9\norwtBCxK/WYTMCbvevdSW/QjC6oGlKUVok+QBZ67gT/J7t95gOx+0JXAdmAFMDCVHQO8WPa7s9OY\nsQO4P+9j6aW22EF2j1NpvCg9RT4UWJb2q55LzbzVaIvX0ziwkSywGlLZFun1EZ83zbxVa4uU/mpp\njCgr29L94kQ2f2O+mZmZWQ6KPB1pZmZmlhsHYWZmZmY5cBBmZmZmlgMHYWZmZmY5cBBmZmZmlgMH\nYWZmgKTrJX2Ydz3MrDgchJmZmZnlwEGYmTUNSfdKWiOpS9Lzkk5O6fslPS1pi6SVks5J6e2SVqfF\nlZeUFleWdImkFWnx8fWSRqQ/0V/SO5K2SuosrQhQUYfPJD2R6rFN0oSU3lfSK5I2SfpG0g0NahYz\na1IOwsysKUi6DLgLGB8R7cBBYHrK7gd8HRGjgFXA4yn9NeCRiLiS7FvNS+mdwKLIFh8fR/bN3wCj\ngfnA5WTf7D2+RnXaImJsKlt6z7lka3tfQbbc02JJfU/sqM2slTkIM7NmcRNwDbBWUld6PTzlHeLw\ngsFvAB2SBgBnRMSqlL4YmJjWrRsWEUsAIuKPOLz+4ZqI2BnZYsxdwEU16lJa5H5dWZmO9LeJiK3A\nD8DI4z9cM2t1bXlXwMysTgIWR8SjdZQ93vXYDpTtH6T2GHmgjjJmZkflK2Fm1ixWAlMlnQsgaaCk\nC1PeScDUtH8P8EVE7AV+Kd2zBcwAVkXEr8BOSben9zlV0mk9UL/PSdOjkkYCFwDf98D7mlmLchBm\nZk0hIr4FHgOWS9oIfAIMSdm/AWMlbQZuBBam9JnAk6l8e1n6DOChlP4lMLgHqvgscJKkTWRTo7Mi\n4oCkoZKW9cD7m1mLUcTxXrU3M/t/kLQ/IvrnXQ8zs//CV8LMzMzMcuArYWZmZmY58JUwMzMzsxw4\nCDMzMzPLgYMwMzMzsxw4CDMzMzPLgYMwMzMzsxw4CDMzMzPLwd++Q3M9IQ5XlgAAAABJRU5ErkJg\ngg==\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot loss\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
    "x = range(len(epoch_train_loss))\n",
    "\n",
    "plt.plot(x, epoch_train_loss, 'r', label=\"train loss\")\n",
    "plt.plot(x, epoch_test_loss, 'b',label=\"validation loss\")\n",
    "\n",
    "plt.xlabel('epoch no.')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XtAuFBvK99I9"
   },
   "outputs": [],
   "source": [
    "#save model with best loss\n",
    "torch.save(best_model.state_dict(), '../content/gdrive/My Drive/models/MNIST_SVHM_aug_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "7TXaeqk499Kj",
    "outputId": "93289a9b-33eb-436a-b0d6-fe520b4f13d3"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWgAAAFlCAYAAADGe3ILAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFEVBA0BR/w+3yPveWL4lYwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class 7 with probability 0.9999438524246216\n"
     ]
    }
   ],
   "source": [
    "#download and test model\n",
    "model = LeNetBN()\n",
    "model.load_state_dict(torch.load('../content/gdrive/My Drive/models/MNIST_SVHM_aug_model.pth'))\n",
    "#print(model1)\n",
    "images, labels = next(iter(test_loader))\n",
    "plt.imshow(images[0][0],'gray')\n",
    "image = images[0,:]#.to(device)\n",
    "image = image[None]\n",
    "plt.show()\n",
    "score = model(image)\n",
    "prob = nn.functional.softmax(score[0], dim=0)\n",
    "y_pred =  prob.argmax()\n",
    "print(\"Predicted class {} with probability {}\".format(y_pred, prob[y_pred]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VHZWLiNkUHjk"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "test = pd.read_csv(\"../content/gdrive/My Drive/MNIST/test.csv\")\n",
    "test_image = test.loc[:,test.columns != \"label\"]\n",
    "test_dataset = torch.from_numpy(np.reshape(test_image.to_numpy().astype(np.uint8), (test_image.shape[0], 1, 28,28)))\n",
    "results = []\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for img in test_dataset:\n",
    "        img = transforms.ToPILImage()(img)\n",
    "        img = transforms.Resize((32, 32))(img)\n",
    "        img = transforms.ToTensor()(img)\n",
    "        img = transforms.Normalize((0.1307, ), (0.3081, ))(img)\n",
    "        test_im = img#.to(device)\n",
    "        test_im = test_im[None]\n",
    "        output = model(test_im)\n",
    "        prob = nn.functional.softmax(output[0], dim=0)\n",
    "        y_pred =  prob.argmax()\n",
    "        results.append( y_pred.cpu().data.numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "eA6dqcyzwMy6",
    "outputId": "a63f0e53-d002-46d6-c5c0-805d5887f785"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28000"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QhHO-Ii9wOpI"
   },
   "outputs": [],
   "source": [
    "predictions = np.array(results).flatten()\n",
    "submissions=pd.DataFrame({\"ImageId\": list(range(1,len(predictions)+1)),\n",
    "                         \"Label\": predictions})\n",
    "submissions.to_csv(\"../content/gdrive/My Drive/MNIST/my_submissions05.csv\", index=False, header=True)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "TrainDigitRecognizer_MNIST_SVHM_aug.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
