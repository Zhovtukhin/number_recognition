{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TrainDigitRecognizer_MNIST.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_fPhAC9u2FE",
        "colab_type": "code",
        "outputId": "1184ca33-786f-422c-f569-85db7184aaf4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#use Google Coolab with free GPU to train model\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWQfJ0MPvGXJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import time\n",
        "import copy\n",
        "\n",
        "from typing import Iterable\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MIBzDDPvI80",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#LeNet with Batch Normalization\n",
        "class LeNetBN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # convolution layers\n",
        "        self._body = nn.Sequential(\n",
        "            # First convolution Layer\n",
        "            # input size = (32, 32), output size = (28, 28)\n",
        "            nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5),\n",
        "            nn.BatchNorm2d(6),\n",
        "            # ReLU activation\n",
        "            nn.ReLU(inplace=True),\n",
        "            # Max pool 2-d\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            \n",
        "            # Second convolution layer\n",
        "            # input size = (14, 14), output size = (10, 10)\n",
        "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            # output size = (5, 5)\n",
        "        )\n",
        "        \n",
        "        # Fully connected layers\n",
        "        self._head = nn.Sequential(\n",
        "            # First fully connected layer\n",
        "            # in_features = total number of weight in last conv layer = 16 * 5 * 5\n",
        "            nn.Linear(in_features=16 * 5 * 5, out_features=120), \n",
        "            \n",
        "            # ReLU activation\n",
        "            nn.ReLU(inplace=True),\n",
        "            \n",
        "            # second fully connected layer\n",
        "            # in_features = output of last linear layer = 120 \n",
        "            nn.Linear(in_features=120, out_features=84), \n",
        "            \n",
        "            # ReLU activation\n",
        "            nn.ReLU(inplace=True),\n",
        "            \n",
        "            # Third fully connected layer. It is also output layer\n",
        "            # in_features = output of last linear layer = 84\n",
        "            # and out_features = number of classes = 10 (0-9)\n",
        "            nn.Linear(in_features=84, out_features=10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # apply feature extractor\n",
        "        x = self._body(x)\n",
        "        # flatten the output of conv layers\n",
        "        # dimension should be batch_size * number_of weight_in_last conv_layer\n",
        "        x = x.view(x.size()[0], -1)\n",
        "        # apply classification head\n",
        "        x = self._head(x)\n",
        "        return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0skH0fkYQSsj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "7e578c5e-bae6-426b-973a-83edce32f260"
      },
      "source": [
        "model = LeNetBN()\n",
        "print(model)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LeNetBN(\n",
            "  (_body): Sequential(\n",
            "    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (_head): Sequential(\n",
            "    (0): Linear(in_features=400, out_features=120, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Linear(in_features=120, out_features=84, bias=True)\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): Linear(in_features=84, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRAT2WWsvWQz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_test_transforms_MNIST = transforms.Compose([\n",
        "    #resize to (32,32)                                              \n",
        "    transforms.Resize((32, 32)),\n",
        "    # re-scale image tensor values between 0-1. image_tensor /= 255\n",
        "    transforms.ToTensor(),\n",
        "    # subtract mean (0.1307) and divide by variance (0.3081).\n",
        "    # This mean and variance is calculated on training data\n",
        "    transforms.Normalize((0.1307, ), (0.3081, ))\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Jzggwr8TOwo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#download data from Google Drive\n",
        "trainMNIST = datasets.MNIST('../content/gdrive/My Drive', train=True, download=False, transform=train_test_transforms_MNIST)\n",
        "testMNIST = datasets.MNIST('../content/gdrive/My Drive', train=False, download=False, transform=train_test_transforms_MNIST)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kk2gufzwQfgs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.manual_seed(42)\n",
        "#if torch.cuda.is_available():\n",
        "#    torch.backend.cudnn_benchmark_enabled = True\n",
        "#    torch.backend.cudnn.deterministic = True\n",
        "\n",
        "#training parameters\n",
        "batch_size = 32\n",
        "epochs_count = 20\n",
        "learning_rate = 0.01\n",
        "log_interval = 100\n",
        "test_interval = 1\n",
        "num_workers  = 10\n",
        "device = 'cuda'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhG-KqsEviSQ",
        "colab_type": "code",
        "outputId": "4704635a-7e6a-4f9d-bbd0-fb9080ef01ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "    print('GPU')\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "    batch_size = 16\n",
        "    num_workers = 0\n",
        "    epochs_count = 10\n",
        "    print('CPU')\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "#SGDoptimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "#array for plotting\n",
        "best_loss = torch.tensor(np.inf)\n",
        "\n",
        "# epoch train/test loss\n",
        "epoch_train_loss = np.array([])\n",
        "epoch_test_loss = np.array([])\n",
        "\n",
        "# epch train/test accuracy\n",
        "epoch_train_acc = np.array([])\n",
        "epoch_test_acc = np.array([])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOdWeCXxvkaC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train dataloader\n",
        "train_loader_MNIST = torch.utils.data.DataLoader(\n",
        "     trainMNIST,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=num_workers\n",
        ")\n",
        "# test dataloader\n",
        "test_loader_MNIST = torch.utils.data.DataLoader(\n",
        "    testMNIST,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=num_workers\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VE6H7m-svksn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, optimizer, train_loader, epoch_idx):\n",
        "    # change model in training mood\n",
        "    model.train()\n",
        "\n",
        "    # to get batch loss\n",
        "    batch_loss = np.array([])\n",
        "\n",
        "    # to get batch accuracy\n",
        "    batch_acc = np.array([])\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "\n",
        "        # clone target\n",
        "        indx_target = target.clone()\n",
        "        # send data to device (its is medatory if GPU has to be used)\n",
        "        data = data.to(device)\n",
        "        # send target to device\n",
        "        target = target.to(device)\n",
        "\n",
        "        # reset parameters gradient to zero\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward pass to the model\n",
        "        output = model(data)\n",
        "\n",
        "        # cross entropy loss\n",
        "        loss = F.cross_entropy(output, target)\n",
        "\n",
        "        # find gradients w.r.t training parameters\n",
        "        loss.backward()\n",
        "        # Update parameters using gardients\n",
        "        optimizer.step()\n",
        "\n",
        "        batch_loss = np.append(batch_loss, [loss.item()])\n",
        "\n",
        "        # Score to probability using softmax\n",
        "        prob = F.softmax(output, dim=1)\n",
        "\n",
        "        # get the index of the max probability\n",
        "        pred = prob.data.max(dim=1)[1]\n",
        "\n",
        "        # correct prediction\n",
        "        correct = pred.cpu().eq(indx_target).sum()\n",
        "\n",
        "        # accuracy\n",
        "        acc = float(correct) / float(len(data))\n",
        "\n",
        "        batch_acc = np.append(batch_acc, [acc])\n",
        "\n",
        "        if batch_idx % log_interval == 0 and batch_idx > 0:\n",
        "            print(\n",
        "                'Train Epoch: {} [{}/{}] Loss: {:.6f} Acc: {:.4f}'.format(\n",
        "                    epoch_idx, batch_idx * len(data), len(train_loader.dataset), loss.item(), acc\n",
        "                )\n",
        "            )\n",
        "\n",
        "    epoch_loss = batch_loss.mean()\n",
        "    epoch_acc = batch_acc.mean()\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "\n",
        "def validate(model, test_loader):\n",
        "    #\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    count_corect_predictions = 0\n",
        "    for data, target in test_loader:\n",
        "        indx_target = target.clone()\n",
        "        data = data.to(device)\n",
        "\n",
        "        target = target.to(device)\n",
        "\n",
        "        output = model(data)\n",
        "        # add loss for each mini batch\n",
        "        test_loss += F.cross_entropy(output, target).item()\n",
        "\n",
        "        # Score to probability using softmax\n",
        "        prob = F.softmax(output, dim=1)\n",
        "\n",
        "        # get the index of the max probability\n",
        "        pred = prob.data.max(dim=1)[1]\n",
        "\n",
        "        # add correct prediction count\n",
        "        count_corect_predictions += pred.cpu().eq(indx_target).sum()\n",
        "\n",
        "    # average over number of mini-batches\n",
        "    test_loss = test_loss / len(test_loader)\n",
        "\n",
        "    # average over number of dataset\n",
        "    accuracy = 100. * count_corect_predictions / len(test_loader.dataset)\n",
        "\n",
        "    print(\n",
        "        '\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "            test_loss, count_corect_predictions, len(test_loader.dataset), accuracy\n",
        "        )\n",
        "    )\n",
        "    return test_loss, accuracy / 100.0\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjythJS-vkv_",
        "colab_type": "code",
        "outputId": "073e0636-47ef-4f1c-b93e-113d0837929e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#train\n",
        "t_begin = time.time()\n",
        "#for save best model\n",
        "best_model = copy.deepcopy(model)\n",
        "for epoch in range(epochs_count):\n",
        "\n",
        "    train_loss, train_acc = train(model, optimizer, train_loader_MNIST, epoch)\n",
        "\n",
        "    epoch_train_loss = np.append(epoch_train_loss, [train_loss])\n",
        "\n",
        "    epoch_train_acc = np.append(epoch_train_acc, [train_acc])\n",
        "\n",
        "    elapsed_time = time.time() - t_begin\n",
        "    speed_epoch = elapsed_time / (epoch + 1)\n",
        "    speed_batch = speed_epoch / len(train_loader_MNIST)\n",
        "    eta = speed_epoch * epochs_count - elapsed_time\n",
        "\n",
        "    print(\n",
        "        \"Elapsed {:.2f}s, {:.2f} s/epoch, {:.2f} s/batch, ets {:.2f}s\".format(\n",
        "            elapsed_time, speed_epoch, speed_batch, eta\n",
        "        )\n",
        "    )\n",
        "\n",
        "    if epoch % test_interval == 0:\n",
        "        current_loss, current_accuracy = validate(model, test_loader_MNIST)\n",
        "\n",
        "        epoch_test_loss = np.append(epoch_test_loss, [current_loss])\n",
        "\n",
        "        epoch_test_acc = np.append(epoch_test_acc, [current_accuracy])\n",
        "\n",
        "        if current_loss < best_loss:\n",
        "            best_loss = current_loss\n",
        "            best_model = copy.deepcopy(model)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [3200/60000] Loss: 2.002036 Acc: 0.7188\n",
            "Train Epoch: 0 [6400/60000] Loss: 1.494526 Acc: 0.6250\n",
            "Train Epoch: 0 [9600/60000] Loss: 0.696404 Acc: 0.8750\n",
            "Train Epoch: 0 [12800/60000] Loss: 0.454495 Acc: 0.9062\n",
            "Train Epoch: 0 [16000/60000] Loss: 0.245746 Acc: 1.0000\n",
            "Train Epoch: 0 [19200/60000] Loss: 0.371234 Acc: 0.8750\n",
            "Train Epoch: 0 [22400/60000] Loss: 0.206877 Acc: 0.9062\n",
            "Train Epoch: 0 [25600/60000] Loss: 0.196510 Acc: 0.9375\n",
            "Train Epoch: 0 [28800/60000] Loss: 0.513706 Acc: 0.8750\n",
            "Train Epoch: 0 [32000/60000] Loss: 0.258200 Acc: 0.9688\n",
            "Train Epoch: 0 [35200/60000] Loss: 0.087188 Acc: 0.9688\n",
            "Train Epoch: 0 [38400/60000] Loss: 0.043744 Acc: 1.0000\n",
            "Train Epoch: 0 [41600/60000] Loss: 0.101799 Acc: 0.9688\n",
            "Train Epoch: 0 [44800/60000] Loss: 0.256225 Acc: 0.9375\n",
            "Train Epoch: 0 [48000/60000] Loss: 0.040485 Acc: 1.0000\n",
            "Train Epoch: 0 [51200/60000] Loss: 0.099987 Acc: 0.9688\n",
            "Train Epoch: 0 [54400/60000] Loss: 0.022102 Acc: 1.0000\n",
            "Train Epoch: 0 [57600/60000] Loss: 0.144105 Acc: 0.9375\n",
            "Elapsed 16.35s, 16.35 s/epoch, 0.01 s/batch, ets 310.65s\n",
            "\n",
            "Test set: Average loss: 0.0981, Accuracy: 9695/10000 (97%)\n",
            "\n",
            "Train Epoch: 1 [3200/60000] Loss: 0.055267 Acc: 1.0000\n",
            "Train Epoch: 1 [6400/60000] Loss: 0.131200 Acc: 0.9688\n",
            "Train Epoch: 1 [9600/60000] Loss: 0.020741 Acc: 1.0000\n",
            "Train Epoch: 1 [12800/60000] Loss: 0.258223 Acc: 0.9375\n",
            "Train Epoch: 1 [16000/60000] Loss: 0.069519 Acc: 0.9688\n",
            "Train Epoch: 1 [19200/60000] Loss: 0.131502 Acc: 0.9375\n",
            "Train Epoch: 1 [22400/60000] Loss: 0.047870 Acc: 0.9688\n",
            "Train Epoch: 1 [25600/60000] Loss: 0.044582 Acc: 1.0000\n",
            "Train Epoch: 1 [28800/60000] Loss: 0.045360 Acc: 1.0000\n",
            "Train Epoch: 1 [32000/60000] Loss: 0.280294 Acc: 0.9062\n",
            "Train Epoch: 1 [35200/60000] Loss: 0.115933 Acc: 0.9688\n",
            "Train Epoch: 1 [38400/60000] Loss: 0.080551 Acc: 0.9688\n",
            "Train Epoch: 1 [41600/60000] Loss: 0.017110 Acc: 1.0000\n",
            "Train Epoch: 1 [44800/60000] Loss: 0.074271 Acc: 0.9688\n",
            "Train Epoch: 1 [48000/60000] Loss: 0.047159 Acc: 1.0000\n",
            "Train Epoch: 1 [51200/60000] Loss: 0.074658 Acc: 1.0000\n",
            "Train Epoch: 1 [54400/60000] Loss: 0.143060 Acc: 0.9688\n",
            "Train Epoch: 1 [57600/60000] Loss: 0.107260 Acc: 0.9375\n",
            "Elapsed 35.02s, 17.51 s/epoch, 0.01 s/batch, ets 315.20s\n",
            "\n",
            "Test set: Average loss: 0.0586, Accuracy: 9834/10000 (98%)\n",
            "\n",
            "Train Epoch: 2 [3200/60000] Loss: 0.029068 Acc: 1.0000\n",
            "Train Epoch: 2 [6400/60000] Loss: 0.091652 Acc: 0.9688\n",
            "Train Epoch: 2 [9600/60000] Loss: 0.075267 Acc: 0.9688\n",
            "Train Epoch: 2 [12800/60000] Loss: 0.013261 Acc: 1.0000\n",
            "Train Epoch: 2 [16000/60000] Loss: 0.071480 Acc: 0.9688\n",
            "Train Epoch: 2 [19200/60000] Loss: 0.135591 Acc: 0.9375\n",
            "Train Epoch: 2 [22400/60000] Loss: 0.005401 Acc: 1.0000\n",
            "Train Epoch: 2 [25600/60000] Loss: 0.146385 Acc: 0.9688\n",
            "Train Epoch: 2 [28800/60000] Loss: 0.011300 Acc: 1.0000\n",
            "Train Epoch: 2 [32000/60000] Loss: 0.036480 Acc: 1.0000\n",
            "Train Epoch: 2 [35200/60000] Loss: 0.008284 Acc: 1.0000\n",
            "Train Epoch: 2 [38400/60000] Loss: 0.042415 Acc: 1.0000\n",
            "Train Epoch: 2 [41600/60000] Loss: 0.007154 Acc: 1.0000\n",
            "Train Epoch: 2 [44800/60000] Loss: 0.094506 Acc: 0.9688\n",
            "Train Epoch: 2 [48000/60000] Loss: 0.005186 Acc: 1.0000\n",
            "Train Epoch: 2 [51200/60000] Loss: 0.120780 Acc: 0.9375\n",
            "Train Epoch: 2 [54400/60000] Loss: 0.014953 Acc: 1.0000\n",
            "Train Epoch: 2 [57600/60000] Loss: 0.143678 Acc: 0.9062\n",
            "Elapsed 53.62s, 17.87 s/epoch, 0.01 s/batch, ets 303.85s\n",
            "\n",
            "Test set: Average loss: 0.0490, Accuracy: 9845/10000 (98%)\n",
            "\n",
            "Train Epoch: 3 [3200/60000] Loss: 0.071490 Acc: 1.0000\n",
            "Train Epoch: 3 [6400/60000] Loss: 0.194698 Acc: 0.9375\n",
            "Train Epoch: 3 [9600/60000] Loss: 0.011323 Acc: 1.0000\n",
            "Train Epoch: 3 [12800/60000] Loss: 0.026626 Acc: 1.0000\n",
            "Train Epoch: 3 [16000/60000] Loss: 0.023516 Acc: 1.0000\n",
            "Train Epoch: 3 [19200/60000] Loss: 0.099858 Acc: 0.9375\n",
            "Train Epoch: 3 [22400/60000] Loss: 0.038908 Acc: 1.0000\n",
            "Train Epoch: 3 [25600/60000] Loss: 0.053129 Acc: 1.0000\n",
            "Train Epoch: 3 [28800/60000] Loss: 0.056560 Acc: 0.9688\n",
            "Train Epoch: 3 [32000/60000] Loss: 0.061879 Acc: 0.9375\n",
            "Train Epoch: 3 [35200/60000] Loss: 0.039596 Acc: 1.0000\n",
            "Train Epoch: 3 [38400/60000] Loss: 0.069585 Acc: 0.9688\n",
            "Train Epoch: 3 [41600/60000] Loss: 0.049383 Acc: 1.0000\n",
            "Train Epoch: 3 [44800/60000] Loss: 0.032793 Acc: 1.0000\n",
            "Train Epoch: 3 [48000/60000] Loss: 0.052642 Acc: 1.0000\n",
            "Train Epoch: 3 [51200/60000] Loss: 0.029124 Acc: 1.0000\n",
            "Train Epoch: 3 [54400/60000] Loss: 0.024356 Acc: 1.0000\n",
            "Train Epoch: 3 [57600/60000] Loss: 0.005218 Acc: 1.0000\n",
            "Elapsed 72.29s, 18.07 s/epoch, 0.01 s/batch, ets 289.17s\n",
            "\n",
            "Test set: Average loss: 0.0474, Accuracy: 9853/10000 (99%)\n",
            "\n",
            "Train Epoch: 4 [3200/60000] Loss: 0.010802 Acc: 1.0000\n",
            "Train Epoch: 4 [6400/60000] Loss: 0.111839 Acc: 0.9688\n",
            "Train Epoch: 4 [9600/60000] Loss: 0.032513 Acc: 1.0000\n",
            "Train Epoch: 4 [12800/60000] Loss: 0.012216 Acc: 1.0000\n",
            "Train Epoch: 4 [16000/60000] Loss: 0.011015 Acc: 1.0000\n",
            "Train Epoch: 4 [19200/60000] Loss: 0.038034 Acc: 0.9688\n",
            "Train Epoch: 4 [22400/60000] Loss: 0.026124 Acc: 1.0000\n",
            "Train Epoch: 4 [25600/60000] Loss: 0.042876 Acc: 0.9688\n",
            "Train Epoch: 4 [28800/60000] Loss: 0.022058 Acc: 1.0000\n",
            "Train Epoch: 4 [32000/60000] Loss: 0.009208 Acc: 1.0000\n",
            "Train Epoch: 4 [35200/60000] Loss: 0.004146 Acc: 1.0000\n",
            "Train Epoch: 4 [38400/60000] Loss: 0.074899 Acc: 0.9375\n",
            "Train Epoch: 4 [41600/60000] Loss: 0.020987 Acc: 1.0000\n",
            "Train Epoch: 4 [44800/60000] Loss: 0.045082 Acc: 0.9688\n",
            "Train Epoch: 4 [48000/60000] Loss: 0.039980 Acc: 1.0000\n",
            "Train Epoch: 4 [51200/60000] Loss: 0.004836 Acc: 1.0000\n",
            "Train Epoch: 4 [54400/60000] Loss: 0.001188 Acc: 1.0000\n",
            "Train Epoch: 4 [57600/60000] Loss: 0.004253 Acc: 1.0000\n",
            "Elapsed 91.06s, 18.21 s/epoch, 0.01 s/batch, ets 273.17s\n",
            "\n",
            "Test set: Average loss: 0.0435, Accuracy: 9854/10000 (99%)\n",
            "\n",
            "Train Epoch: 5 [3200/60000] Loss: 0.014842 Acc: 1.0000\n",
            "Train Epoch: 5 [6400/60000] Loss: 0.021679 Acc: 1.0000\n",
            "Train Epoch: 5 [9600/60000] Loss: 0.005269 Acc: 1.0000\n",
            "Train Epoch: 5 [12800/60000] Loss: 0.006860 Acc: 1.0000\n",
            "Train Epoch: 5 [16000/60000] Loss: 0.019782 Acc: 1.0000\n",
            "Train Epoch: 5 [19200/60000] Loss: 0.007885 Acc: 1.0000\n",
            "Train Epoch: 5 [22400/60000] Loss: 0.020129 Acc: 1.0000\n",
            "Train Epoch: 5 [25600/60000] Loss: 0.003084 Acc: 1.0000\n",
            "Train Epoch: 5 [28800/60000] Loss: 0.200502 Acc: 0.9375\n",
            "Train Epoch: 5 [32000/60000] Loss: 0.021072 Acc: 1.0000\n",
            "Train Epoch: 5 [35200/60000] Loss: 0.194868 Acc: 0.8750\n",
            "Train Epoch: 5 [38400/60000] Loss: 0.014627 Acc: 1.0000\n",
            "Train Epoch: 5 [41600/60000] Loss: 0.013095 Acc: 1.0000\n",
            "Train Epoch: 5 [44800/60000] Loss: 0.016373 Acc: 1.0000\n",
            "Train Epoch: 5 [48000/60000] Loss: 0.001968 Acc: 1.0000\n",
            "Train Epoch: 5 [51200/60000] Loss: 0.010388 Acc: 1.0000\n",
            "Train Epoch: 5 [54400/60000] Loss: 0.049161 Acc: 1.0000\n",
            "Train Epoch: 5 [57600/60000] Loss: 0.023794 Acc: 1.0000\n",
            "Elapsed 110.07s, 18.34 s/epoch, 0.01 s/batch, ets 256.82s\n",
            "\n",
            "Test set: Average loss: 0.0449, Accuracy: 9851/10000 (99%)\n",
            "\n",
            "Train Epoch: 6 [3200/60000] Loss: 0.004705 Acc: 1.0000\n",
            "Train Epoch: 6 [6400/60000] Loss: 0.055444 Acc: 0.9688\n",
            "Train Epoch: 6 [9600/60000] Loss: 0.025631 Acc: 1.0000\n",
            "Train Epoch: 6 [12800/60000] Loss: 0.079872 Acc: 0.9688\n",
            "Train Epoch: 6 [16000/60000] Loss: 0.002347 Acc: 1.0000\n",
            "Train Epoch: 6 [19200/60000] Loss: 0.013573 Acc: 1.0000\n",
            "Train Epoch: 6 [22400/60000] Loss: 0.034265 Acc: 1.0000\n",
            "Train Epoch: 6 [25600/60000] Loss: 0.097146 Acc: 0.9688\n",
            "Train Epoch: 6 [28800/60000] Loss: 0.095239 Acc: 0.9688\n",
            "Train Epoch: 6 [32000/60000] Loss: 0.023363 Acc: 1.0000\n",
            "Train Epoch: 6 [35200/60000] Loss: 0.000894 Acc: 1.0000\n",
            "Train Epoch: 6 [38400/60000] Loss: 0.014567 Acc: 1.0000\n",
            "Train Epoch: 6 [41600/60000] Loss: 0.134676 Acc: 0.9375\n",
            "Train Epoch: 6 [44800/60000] Loss: 0.001425 Acc: 1.0000\n",
            "Train Epoch: 6 [48000/60000] Loss: 0.084990 Acc: 0.9688\n",
            "Train Epoch: 6 [51200/60000] Loss: 0.041190 Acc: 1.0000\n",
            "Train Epoch: 6 [54400/60000] Loss: 0.127058 Acc: 0.9688\n",
            "Train Epoch: 6 [57600/60000] Loss: 0.011628 Acc: 1.0000\n",
            "Elapsed 128.94s, 18.42 s/epoch, 0.01 s/batch, ets 239.46s\n",
            "\n",
            "Test set: Average loss: 0.0387, Accuracy: 9879/10000 (99%)\n",
            "\n",
            "Train Epoch: 7 [3200/60000] Loss: 0.006353 Acc: 1.0000\n",
            "Train Epoch: 7 [6400/60000] Loss: 0.009958 Acc: 1.0000\n",
            "Train Epoch: 7 [9600/60000] Loss: 0.011607 Acc: 1.0000\n",
            "Train Epoch: 7 [12800/60000] Loss: 0.005098 Acc: 1.0000\n",
            "Train Epoch: 7 [16000/60000] Loss: 0.023527 Acc: 1.0000\n",
            "Train Epoch: 7 [19200/60000] Loss: 0.052034 Acc: 0.9688\n",
            "Train Epoch: 7 [22400/60000] Loss: 0.009612 Acc: 1.0000\n",
            "Train Epoch: 7 [25600/60000] Loss: 0.028622 Acc: 1.0000\n",
            "Train Epoch: 7 [28800/60000] Loss: 0.022553 Acc: 1.0000\n",
            "Train Epoch: 7 [32000/60000] Loss: 0.006223 Acc: 1.0000\n",
            "Train Epoch: 7 [35200/60000] Loss: 0.036438 Acc: 1.0000\n",
            "Train Epoch: 7 [38400/60000] Loss: 0.003519 Acc: 1.0000\n",
            "Train Epoch: 7 [41600/60000] Loss: 0.002178 Acc: 1.0000\n",
            "Train Epoch: 7 [44800/60000] Loss: 0.009909 Acc: 1.0000\n",
            "Train Epoch: 7 [48000/60000] Loss: 0.001554 Acc: 1.0000\n",
            "Train Epoch: 7 [51200/60000] Loss: 0.002842 Acc: 1.0000\n",
            "Train Epoch: 7 [54400/60000] Loss: 0.008709 Acc: 1.0000\n",
            "Train Epoch: 7 [57600/60000] Loss: 0.001904 Acc: 1.0000\n",
            "Elapsed 147.52s, 18.44 s/epoch, 0.01 s/batch, ets 221.28s\n",
            "\n",
            "Test set: Average loss: 0.0350, Accuracy: 9883/10000 (99%)\n",
            "\n",
            "Train Epoch: 8 [3200/60000] Loss: 0.012254 Acc: 1.0000\n",
            "Train Epoch: 8 [6400/60000] Loss: 0.010214 Acc: 1.0000\n",
            "Train Epoch: 8 [9600/60000] Loss: 0.040045 Acc: 0.9688\n",
            "Train Epoch: 8 [12800/60000] Loss: 0.062300 Acc: 0.9688\n",
            "Train Epoch: 8 [16000/60000] Loss: 0.004119 Acc: 1.0000\n",
            "Train Epoch: 8 [19200/60000] Loss: 0.018467 Acc: 1.0000\n",
            "Train Epoch: 8 [22400/60000] Loss: 0.058802 Acc: 0.9688\n",
            "Train Epoch: 8 [25600/60000] Loss: 0.045086 Acc: 1.0000\n",
            "Train Epoch: 8 [28800/60000] Loss: 0.049795 Acc: 0.9688\n",
            "Train Epoch: 8 [32000/60000] Loss: 0.123730 Acc: 0.9688\n",
            "Train Epoch: 8 [35200/60000] Loss: 0.050301 Acc: 0.9688\n",
            "Train Epoch: 8 [38400/60000] Loss: 0.012914 Acc: 1.0000\n",
            "Train Epoch: 8 [41600/60000] Loss: 0.041668 Acc: 0.9688\n",
            "Train Epoch: 8 [44800/60000] Loss: 0.001802 Acc: 1.0000\n",
            "Train Epoch: 8 [48000/60000] Loss: 0.002538 Acc: 1.0000\n",
            "Train Epoch: 8 [51200/60000] Loss: 0.080702 Acc: 0.9688\n",
            "Train Epoch: 8 [54400/60000] Loss: 0.026628 Acc: 0.9688\n",
            "Train Epoch: 8 [57600/60000] Loss: 0.009328 Acc: 1.0000\n",
            "Elapsed 166.30s, 18.48 s/epoch, 0.01 s/batch, ets 203.26s\n",
            "\n",
            "Test set: Average loss: 0.0365, Accuracy: 9877/10000 (99%)\n",
            "\n",
            "Train Epoch: 9 [3200/60000] Loss: 0.007775 Acc: 1.0000\n",
            "Train Epoch: 9 [6400/60000] Loss: 0.022679 Acc: 1.0000\n",
            "Train Epoch: 9 [9600/60000] Loss: 0.002457 Acc: 1.0000\n",
            "Train Epoch: 9 [12800/60000] Loss: 0.001745 Acc: 1.0000\n",
            "Train Epoch: 9 [16000/60000] Loss: 0.065649 Acc: 0.9688\n",
            "Train Epoch: 9 [19200/60000] Loss: 0.000765 Acc: 1.0000\n",
            "Train Epoch: 9 [22400/60000] Loss: 0.002625 Acc: 1.0000\n",
            "Train Epoch: 9 [25600/60000] Loss: 0.000769 Acc: 1.0000\n",
            "Train Epoch: 9 [28800/60000] Loss: 0.029063 Acc: 0.9688\n",
            "Train Epoch: 9 [32000/60000] Loss: 0.003392 Acc: 1.0000\n",
            "Train Epoch: 9 [35200/60000] Loss: 0.016343 Acc: 1.0000\n",
            "Train Epoch: 9 [38400/60000] Loss: 0.216283 Acc: 0.9375\n",
            "Train Epoch: 9 [41600/60000] Loss: 0.011802 Acc: 1.0000\n",
            "Train Epoch: 9 [44800/60000] Loss: 0.006368 Acc: 1.0000\n",
            "Train Epoch: 9 [48000/60000] Loss: 0.000989 Acc: 1.0000\n",
            "Train Epoch: 9 [51200/60000] Loss: 0.002258 Acc: 1.0000\n",
            "Train Epoch: 9 [54400/60000] Loss: 0.016175 Acc: 1.0000\n",
            "Train Epoch: 9 [57600/60000] Loss: 0.028907 Acc: 1.0000\n",
            "Elapsed 184.82s, 18.48 s/epoch, 0.01 s/batch, ets 184.82s\n",
            "\n",
            "Test set: Average loss: 0.0356, Accuracy: 9890/10000 (99%)\n",
            "\n",
            "Train Epoch: 10 [3200/60000] Loss: 0.010395 Acc: 1.0000\n",
            "Train Epoch: 10 [6400/60000] Loss: 0.246870 Acc: 0.9688\n",
            "Train Epoch: 10 [9600/60000] Loss: 0.001822 Acc: 1.0000\n",
            "Train Epoch: 10 [12800/60000] Loss: 0.015570 Acc: 1.0000\n",
            "Train Epoch: 10 [16000/60000] Loss: 0.007092 Acc: 1.0000\n",
            "Train Epoch: 10 [19200/60000] Loss: 0.018185 Acc: 1.0000\n",
            "Train Epoch: 10 [22400/60000] Loss: 0.002089 Acc: 1.0000\n",
            "Train Epoch: 10 [25600/60000] Loss: 0.053165 Acc: 0.9688\n",
            "Train Epoch: 10 [28800/60000] Loss: 0.012419 Acc: 1.0000\n",
            "Train Epoch: 10 [32000/60000] Loss: 0.029312 Acc: 0.9688\n",
            "Train Epoch: 10 [35200/60000] Loss: 0.234220 Acc: 0.9375\n",
            "Train Epoch: 10 [38400/60000] Loss: 0.015716 Acc: 1.0000\n",
            "Train Epoch: 10 [41600/60000] Loss: 0.016835 Acc: 1.0000\n",
            "Train Epoch: 10 [44800/60000] Loss: 0.000649 Acc: 1.0000\n",
            "Train Epoch: 10 [48000/60000] Loss: 0.055691 Acc: 0.9688\n",
            "Train Epoch: 10 [51200/60000] Loss: 0.063764 Acc: 0.9688\n",
            "Train Epoch: 10 [54400/60000] Loss: 0.076217 Acc: 0.9375\n",
            "Train Epoch: 10 [57600/60000] Loss: 0.004242 Acc: 1.0000\n",
            "Elapsed 203.56s, 18.51 s/epoch, 0.01 s/batch, ets 166.55s\n",
            "\n",
            "Test set: Average loss: 0.0353, Accuracy: 9876/10000 (99%)\n",
            "\n",
            "Train Epoch: 11 [3200/60000] Loss: 0.008712 Acc: 1.0000\n",
            "Train Epoch: 11 [6400/60000] Loss: 0.028864 Acc: 1.0000\n",
            "Train Epoch: 11 [9600/60000] Loss: 0.008510 Acc: 1.0000\n",
            "Train Epoch: 11 [12800/60000] Loss: 0.010485 Acc: 1.0000\n",
            "Train Epoch: 11 [16000/60000] Loss: 0.012063 Acc: 1.0000\n",
            "Train Epoch: 11 [19200/60000] Loss: 0.041592 Acc: 0.9688\n",
            "Train Epoch: 11 [22400/60000] Loss: 0.009000 Acc: 1.0000\n",
            "Train Epoch: 11 [25600/60000] Loss: 0.003611 Acc: 1.0000\n",
            "Train Epoch: 11 [28800/60000] Loss: 0.005235 Acc: 1.0000\n",
            "Train Epoch: 11 [32000/60000] Loss: 0.015044 Acc: 1.0000\n",
            "Train Epoch: 11 [35200/60000] Loss: 0.016858 Acc: 1.0000\n",
            "Train Epoch: 11 [38400/60000] Loss: 0.035689 Acc: 1.0000\n",
            "Train Epoch: 11 [41600/60000] Loss: 0.008146 Acc: 1.0000\n",
            "Train Epoch: 11 [44800/60000] Loss: 0.130086 Acc: 0.9688\n",
            "Train Epoch: 11 [48000/60000] Loss: 0.023315 Acc: 1.0000\n",
            "Train Epoch: 11 [51200/60000] Loss: 0.006070 Acc: 1.0000\n",
            "Train Epoch: 11 [54400/60000] Loss: 0.006742 Acc: 1.0000\n",
            "Train Epoch: 11 [57600/60000] Loss: 0.007806 Acc: 1.0000\n",
            "Elapsed 222.31s, 18.53 s/epoch, 0.01 s/batch, ets 148.21s\n",
            "\n",
            "Test set: Average loss: 0.0385, Accuracy: 9870/10000 (99%)\n",
            "\n",
            "Train Epoch: 12 [3200/60000] Loss: 0.003046 Acc: 1.0000\n",
            "Train Epoch: 12 [6400/60000] Loss: 0.002520 Acc: 1.0000\n",
            "Train Epoch: 12 [9600/60000] Loss: 0.004477 Acc: 1.0000\n",
            "Train Epoch: 12 [12800/60000] Loss: 0.162066 Acc: 0.9375\n",
            "Train Epoch: 12 [16000/60000] Loss: 0.131704 Acc: 0.9688\n",
            "Train Epoch: 12 [19200/60000] Loss: 0.043669 Acc: 0.9688\n",
            "Train Epoch: 12 [22400/60000] Loss: 0.001512 Acc: 1.0000\n",
            "Train Epoch: 12 [25600/60000] Loss: 0.000699 Acc: 1.0000\n",
            "Train Epoch: 12 [28800/60000] Loss: 0.044431 Acc: 0.9688\n",
            "Train Epoch: 12 [32000/60000] Loss: 0.155777 Acc: 0.9375\n",
            "Train Epoch: 12 [35200/60000] Loss: 0.159456 Acc: 0.9375\n",
            "Train Epoch: 12 [38400/60000] Loss: 0.005706 Acc: 1.0000\n",
            "Train Epoch: 12 [41600/60000] Loss: 0.010469 Acc: 1.0000\n",
            "Train Epoch: 12 [44800/60000] Loss: 0.002058 Acc: 1.0000\n",
            "Train Epoch: 12 [48000/60000] Loss: 0.042579 Acc: 1.0000\n",
            "Train Epoch: 12 [51200/60000] Loss: 0.003646 Acc: 1.0000\n",
            "Train Epoch: 12 [54400/60000] Loss: 0.017733 Acc: 1.0000\n",
            "Train Epoch: 12 [57600/60000] Loss: 0.006691 Acc: 1.0000\n",
            "Elapsed 241.09s, 18.55 s/epoch, 0.01 s/batch, ets 129.82s\n",
            "\n",
            "Test set: Average loss: 0.0313, Accuracy: 9907/10000 (99%)\n",
            "\n",
            "Train Epoch: 13 [3200/60000] Loss: 0.017498 Acc: 1.0000\n",
            "Train Epoch: 13 [6400/60000] Loss: 0.035984 Acc: 0.9688\n",
            "Train Epoch: 13 [9600/60000] Loss: 0.072283 Acc: 0.9688\n",
            "Train Epoch: 13 [12800/60000] Loss: 0.003183 Acc: 1.0000\n",
            "Train Epoch: 13 [16000/60000] Loss: 0.029989 Acc: 1.0000\n",
            "Train Epoch: 13 [19200/60000] Loss: 0.001967 Acc: 1.0000\n",
            "Train Epoch: 13 [22400/60000] Loss: 0.010395 Acc: 1.0000\n",
            "Train Epoch: 13 [25600/60000] Loss: 0.020731 Acc: 1.0000\n",
            "Train Epoch: 13 [28800/60000] Loss: 0.124549 Acc: 0.9688\n",
            "Train Epoch: 13 [32000/60000] Loss: 0.044488 Acc: 0.9688\n",
            "Train Epoch: 13 [35200/60000] Loss: 0.015901 Acc: 1.0000\n",
            "Train Epoch: 13 [38400/60000] Loss: 0.033565 Acc: 0.9688\n",
            "Train Epoch: 13 [41600/60000] Loss: 0.000502 Acc: 1.0000\n",
            "Train Epoch: 13 [44800/60000] Loss: 0.012748 Acc: 1.0000\n",
            "Train Epoch: 13 [48000/60000] Loss: 0.004360 Acc: 1.0000\n",
            "Train Epoch: 13 [51200/60000] Loss: 0.000118 Acc: 1.0000\n",
            "Train Epoch: 13 [54400/60000] Loss: 0.003043 Acc: 1.0000\n",
            "Train Epoch: 13 [57600/60000] Loss: 0.001947 Acc: 1.0000\n",
            "Elapsed 259.79s, 18.56 s/epoch, 0.01 s/batch, ets 111.34s\n",
            "\n",
            "Test set: Average loss: 0.0324, Accuracy: 9887/10000 (99%)\n",
            "\n",
            "Train Epoch: 14 [3200/60000] Loss: 0.009652 Acc: 1.0000\n",
            "Train Epoch: 14 [6400/60000] Loss: 0.026008 Acc: 1.0000\n",
            "Train Epoch: 14 [9600/60000] Loss: 0.028297 Acc: 1.0000\n",
            "Train Epoch: 14 [12800/60000] Loss: 0.000832 Acc: 1.0000\n",
            "Train Epoch: 14 [16000/60000] Loss: 0.000505 Acc: 1.0000\n",
            "Train Epoch: 14 [19200/60000] Loss: 0.055938 Acc: 0.9688\n",
            "Train Epoch: 14 [22400/60000] Loss: 0.001710 Acc: 1.0000\n",
            "Train Epoch: 14 [25600/60000] Loss: 0.007717 Acc: 1.0000\n",
            "Train Epoch: 14 [28800/60000] Loss: 0.001756 Acc: 1.0000\n",
            "Train Epoch: 14 [32000/60000] Loss: 0.000957 Acc: 1.0000\n",
            "Train Epoch: 14 [35200/60000] Loss: 0.004912 Acc: 1.0000\n",
            "Train Epoch: 14 [38400/60000] Loss: 0.002805 Acc: 1.0000\n",
            "Train Epoch: 14 [41600/60000] Loss: 0.016009 Acc: 1.0000\n",
            "Train Epoch: 14 [44800/60000] Loss: 0.009507 Acc: 1.0000\n",
            "Train Epoch: 14 [48000/60000] Loss: 0.006116 Acc: 1.0000\n",
            "Train Epoch: 14 [51200/60000] Loss: 0.109383 Acc: 0.9688\n",
            "Train Epoch: 14 [54400/60000] Loss: 0.004712 Acc: 1.0000\n",
            "Train Epoch: 14 [57600/60000] Loss: 0.002611 Acc: 1.0000\n",
            "Elapsed 278.67s, 18.58 s/epoch, 0.01 s/batch, ets 92.89s\n",
            "\n",
            "Test set: Average loss: 0.0320, Accuracy: 9896/10000 (99%)\n",
            "\n",
            "Train Epoch: 15 [3200/60000] Loss: 0.000569 Acc: 1.0000\n",
            "Train Epoch: 15 [6400/60000] Loss: 0.001874 Acc: 1.0000\n",
            "Train Epoch: 15 [9600/60000] Loss: 0.006867 Acc: 1.0000\n",
            "Train Epoch: 15 [12800/60000] Loss: 0.005333 Acc: 1.0000\n",
            "Train Epoch: 15 [16000/60000] Loss: 0.026639 Acc: 1.0000\n",
            "Train Epoch: 15 [19200/60000] Loss: 0.054494 Acc: 0.9688\n",
            "Train Epoch: 15 [22400/60000] Loss: 0.000849 Acc: 1.0000\n",
            "Train Epoch: 15 [25600/60000] Loss: 0.000825 Acc: 1.0000\n",
            "Train Epoch: 15 [28800/60000] Loss: 0.005666 Acc: 1.0000\n",
            "Train Epoch: 15 [32000/60000] Loss: 0.005598 Acc: 1.0000\n",
            "Train Epoch: 15 [35200/60000] Loss: 0.006601 Acc: 1.0000\n",
            "Train Epoch: 15 [38400/60000] Loss: 0.012995 Acc: 1.0000\n",
            "Train Epoch: 15 [41600/60000] Loss: 0.023919 Acc: 1.0000\n",
            "Train Epoch: 15 [44800/60000] Loss: 0.003895 Acc: 1.0000\n",
            "Train Epoch: 15 [48000/60000] Loss: 0.021855 Acc: 1.0000\n",
            "Train Epoch: 15 [51200/60000] Loss: 0.002451 Acc: 1.0000\n",
            "Train Epoch: 15 [54400/60000] Loss: 0.009337 Acc: 1.0000\n",
            "Train Epoch: 15 [57600/60000] Loss: 0.004354 Acc: 1.0000\n",
            "Elapsed 297.57s, 18.60 s/epoch, 0.01 s/batch, ets 74.39s\n",
            "\n",
            "Test set: Average loss: 0.0313, Accuracy: 9893/10000 (99%)\n",
            "\n",
            "Train Epoch: 16 [3200/60000] Loss: 0.001876 Acc: 1.0000\n",
            "Train Epoch: 16 [6400/60000] Loss: 0.066986 Acc: 0.9688\n",
            "Train Epoch: 16 [9600/60000] Loss: 0.000286 Acc: 1.0000\n",
            "Train Epoch: 16 [12800/60000] Loss: 0.005060 Acc: 1.0000\n",
            "Train Epoch: 16 [16000/60000] Loss: 0.001978 Acc: 1.0000\n",
            "Train Epoch: 16 [19200/60000] Loss: 0.058130 Acc: 0.9688\n",
            "Train Epoch: 16 [22400/60000] Loss: 0.039023 Acc: 1.0000\n",
            "Train Epoch: 16 [25600/60000] Loss: 0.004147 Acc: 1.0000\n",
            "Train Epoch: 16 [28800/60000] Loss: 0.004798 Acc: 1.0000\n",
            "Train Epoch: 16 [32000/60000] Loss: 0.030475 Acc: 0.9688\n",
            "Train Epoch: 16 [35200/60000] Loss: 0.026155 Acc: 1.0000\n",
            "Train Epoch: 16 [38400/60000] Loss: 0.001432 Acc: 1.0000\n",
            "Train Epoch: 16 [41600/60000] Loss: 0.001513 Acc: 1.0000\n",
            "Train Epoch: 16 [44800/60000] Loss: 0.003392 Acc: 1.0000\n",
            "Train Epoch: 16 [48000/60000] Loss: 0.057031 Acc: 0.9688\n",
            "Train Epoch: 16 [51200/60000] Loss: 0.026706 Acc: 1.0000\n",
            "Train Epoch: 16 [54400/60000] Loss: 0.001526 Acc: 1.0000\n",
            "Train Epoch: 16 [57600/60000] Loss: 0.016968 Acc: 1.0000\n",
            "Elapsed 316.53s, 18.62 s/epoch, 0.01 s/batch, ets 55.86s\n",
            "\n",
            "Test set: Average loss: 0.0366, Accuracy: 9883/10000 (99%)\n",
            "\n",
            "Train Epoch: 17 [3200/60000] Loss: 0.002718 Acc: 1.0000\n",
            "Train Epoch: 17 [6400/60000] Loss: 0.000742 Acc: 1.0000\n",
            "Train Epoch: 17 [9600/60000] Loss: 0.007713 Acc: 1.0000\n",
            "Train Epoch: 17 [12800/60000] Loss: 0.031474 Acc: 0.9688\n",
            "Train Epoch: 17 [16000/60000] Loss: 0.000401 Acc: 1.0000\n",
            "Train Epoch: 17 [19200/60000] Loss: 0.005158 Acc: 1.0000\n",
            "Train Epoch: 17 [22400/60000] Loss: 0.011926 Acc: 1.0000\n",
            "Train Epoch: 17 [25600/60000] Loss: 0.103696 Acc: 0.9375\n",
            "Train Epoch: 17 [28800/60000] Loss: 0.015709 Acc: 1.0000\n",
            "Train Epoch: 17 [32000/60000] Loss: 0.000669 Acc: 1.0000\n",
            "Train Epoch: 17 [35200/60000] Loss: 0.003605 Acc: 1.0000\n",
            "Train Epoch: 17 [38400/60000] Loss: 0.010390 Acc: 1.0000\n",
            "Train Epoch: 17 [41600/60000] Loss: 0.032703 Acc: 0.9688\n",
            "Train Epoch: 17 [44800/60000] Loss: 0.002865 Acc: 1.0000\n",
            "Train Epoch: 17 [48000/60000] Loss: 0.058154 Acc: 0.9688\n",
            "Train Epoch: 17 [51200/60000] Loss: 0.003952 Acc: 1.0000\n",
            "Train Epoch: 17 [54400/60000] Loss: 0.001614 Acc: 1.0000\n",
            "Train Epoch: 17 [57600/60000] Loss: 0.043492 Acc: 0.9688\n",
            "Elapsed 335.55s, 18.64 s/epoch, 0.01 s/batch, ets 37.28s\n",
            "\n",
            "Test set: Average loss: 0.0299, Accuracy: 9901/10000 (99%)\n",
            "\n",
            "Train Epoch: 18 [3200/60000] Loss: 0.010867 Acc: 1.0000\n",
            "Train Epoch: 18 [6400/60000] Loss: 0.110817 Acc: 0.9688\n",
            "Train Epoch: 18 [9600/60000] Loss: 0.046769 Acc: 0.9688\n",
            "Train Epoch: 18 [12800/60000] Loss: 0.015373 Acc: 1.0000\n",
            "Train Epoch: 18 [16000/60000] Loss: 0.013281 Acc: 1.0000\n",
            "Train Epoch: 18 [19200/60000] Loss: 0.025249 Acc: 1.0000\n",
            "Train Epoch: 18 [22400/60000] Loss: 0.010737 Acc: 1.0000\n",
            "Train Epoch: 18 [25600/60000] Loss: 0.000904 Acc: 1.0000\n",
            "Train Epoch: 18 [28800/60000] Loss: 0.000663 Acc: 1.0000\n",
            "Train Epoch: 18 [32000/60000] Loss: 0.019853 Acc: 1.0000\n",
            "Train Epoch: 18 [35200/60000] Loss: 0.000923 Acc: 1.0000\n",
            "Train Epoch: 18 [38400/60000] Loss: 0.000849 Acc: 1.0000\n",
            "Train Epoch: 18 [41600/60000] Loss: 0.003485 Acc: 1.0000\n",
            "Train Epoch: 18 [44800/60000] Loss: 0.096084 Acc: 0.9688\n",
            "Train Epoch: 18 [48000/60000] Loss: 0.002666 Acc: 1.0000\n",
            "Train Epoch: 18 [51200/60000] Loss: 0.047171 Acc: 0.9688\n",
            "Train Epoch: 18 [54400/60000] Loss: 0.005707 Acc: 1.0000\n",
            "Train Epoch: 18 [57600/60000] Loss: 0.000810 Acc: 1.0000\n",
            "Elapsed 355.09s, 18.69 s/epoch, 0.01 s/batch, ets 18.69s\n",
            "\n",
            "Test set: Average loss: 0.0310, Accuracy: 9909/10000 (99%)\n",
            "\n",
            "Train Epoch: 19 [3200/60000] Loss: 0.002884 Acc: 1.0000\n",
            "Train Epoch: 19 [6400/60000] Loss: 0.016377 Acc: 1.0000\n",
            "Train Epoch: 19 [9600/60000] Loss: 0.000182 Acc: 1.0000\n",
            "Train Epoch: 19 [12800/60000] Loss: 0.008885 Acc: 1.0000\n",
            "Train Epoch: 19 [16000/60000] Loss: 0.000148 Acc: 1.0000\n",
            "Train Epoch: 19 [19200/60000] Loss: 0.000169 Acc: 1.0000\n",
            "Train Epoch: 19 [22400/60000] Loss: 0.034518 Acc: 0.9688\n",
            "Train Epoch: 19 [25600/60000] Loss: 0.021674 Acc: 1.0000\n",
            "Train Epoch: 19 [28800/60000] Loss: 0.002223 Acc: 1.0000\n",
            "Train Epoch: 19 [32000/60000] Loss: 0.002853 Acc: 1.0000\n",
            "Train Epoch: 19 [35200/60000] Loss: 0.002158 Acc: 1.0000\n",
            "Train Epoch: 19 [38400/60000] Loss: 0.000621 Acc: 1.0000\n",
            "Train Epoch: 19 [41600/60000] Loss: 0.001271 Acc: 1.0000\n",
            "Train Epoch: 19 [44800/60000] Loss: 0.002559 Acc: 1.0000\n",
            "Train Epoch: 19 [48000/60000] Loss: 0.106251 Acc: 0.9688\n",
            "Train Epoch: 19 [51200/60000] Loss: 0.056668 Acc: 0.9688\n",
            "Train Epoch: 19 [54400/60000] Loss: 0.000364 Acc: 1.0000\n",
            "Train Epoch: 19 [57600/60000] Loss: 0.000540 Acc: 1.0000\n",
            "Elapsed 374.50s, 18.73 s/epoch, 0.01 s/batch, ets 0.00s\n",
            "\n",
            "Test set: Average loss: 0.0343, Accuracy: 9890/10000 (99%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUMHs1Revk4O",
        "colab_type": "code",
        "outputId": "d614b3b3-f472-4498-8e57-13fe5808164a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"Total time: {:.2f}, Best Loss: {:.3f}\".format(time.time() - t_begin, best_loss))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total time: 377.19, Best Loss: 0.030\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeeFfy2I99GO",
        "colab_type": "code",
        "outputId": "0d50a2c5-ed8d-4015-b24d-d0f521440a3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        }
      },
      "source": [
        "# Plot loss\n",
        "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
        "x = range(len(epoch_train_loss))\n",
        "\n",
        "plt.plot(x, epoch_train_loss, 'r', label=\"train loss\")\n",
        "plt.plot(x, epoch_test_loss, 'b',label=\"validation loss\")\n",
        "\n",
        "plt.xlabel('epoch №.')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmEAAAGDCAYAAABjkcdfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtAuFBvK99I9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#save model with best loss\n",
        "torch.save(best_model.state_dict(), '../content/gdrive/My Drive/models/MNIST_model.pth')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TXaeqk499Kj",
        "colab_type": "code",
        "outputId": "a7b9c3a4-32cc-4bea-b5b3-b17c7f9e0dc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "#download and test model\n",
        "model = LeNetBN()\n",
        "model.load_state_dict(torch.load('../content/gdrive/My Drive/models/MNIST_model.pth'))\n",
        "#print(model1)\n",
        "images, labels = next(iter(test_loader_MNIST))\n",
        "plt.imshow(images[0][0],'gray')\n",
        "image = images[0,:]#.to(device)\n",
        "image = image[None]\n",
        "plt.show()\n",
        "score = model(image)\n",
        "prob = nn.functional.softmax(score[0], dim=0)\n",
        "y_pred =  prob.argmax()\n",
        "print(\"Predicted class {} with probability {}\".format(y_pred, prob[y_pred]))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWgAAAFlCAYAAADGe3ILAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nNAAURUEDQFH/D2X9+97QTtKtAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Predicted class 7 with probability 0.9999903440475464\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjaJm7q2eI6x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "test = pd.read_csv(\"../content/gdrive/My Drive/MNIST/test.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iImU4e_feOKr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_image = test.loc[:,test.columns != \"label\"]\n",
        "test_dataset = torch.from_numpy(np.reshape(test_image.to_numpy().astype(np.uint8), (test_image.shape[0], 1, 28,28)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lBFNCtyb5gB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results = []\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    for img in test_dataset:\n",
        "        img = transforms.ToPILImage()(img)\n",
        "        img = transforms.Resize((32, 32))(img)\n",
        "        img = transforms.ToTensor()(img)\n",
        "        img = transforms.Normalize((0.1307, ), (0.3081, ))(img)\n",
        "        test_im = img#.to(device)\n",
        "        test_im = test_im[None]\n",
        "        output = model(test_im)\n",
        "        prob = nn.functional.softmax(output[0], dim=0)\n",
        "        y_pred =  prob.argmax()\n",
        "        results.append( y_pred.cpu().data.numpy().tolist())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WJoHD0xcKVv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "12ce90c2-4eda-4658-e438-a77c918afbb6"
      },
      "source": [
        "len(results)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TJeDsB7cMZh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = np.array(results).flatten()\n",
        "submissions=pd.DataFrame({\"ImageId\": list(range(1,len(predictions)+1)),\n",
        "                         \"Label\": predictions})\n",
        "submissions.to_csv(\"../content/gdrive/My Drive/MNIST/my_submissions01.csv\", index=False, header=True)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}