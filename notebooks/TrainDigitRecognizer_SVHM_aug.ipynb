{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TrainDigitRecognizer_SVHM_aug.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_fPhAC9u2FE",
        "colab_type": "code",
        "outputId": "6f031266-9798-4361-b564-dedab2e6ffc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#use Google Coolab with free GPU to train model\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWQfJ0MPvGXJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import time\n",
        "import copy\n",
        "\n",
        "from typing import Iterable\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MIBzDDPvI80",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#LeNet with Batch Normalization\n",
        "class LeNetBN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # convolution layers\n",
        "        self._body = nn.Sequential(\n",
        "            # First convolution Layer\n",
        "            # input size = (32, 32), output size = (28, 28)\n",
        "            nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5),\n",
        "            nn.BatchNorm2d(6),\n",
        "            # ReLU activation\n",
        "            nn.ReLU(inplace=True),\n",
        "            # Max pool 2-d\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            \n",
        "            # Second convolution layer\n",
        "            # input size = (14, 14), output size = (10, 10)\n",
        "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            # output size = (5, 5)\n",
        "        )\n",
        "        \n",
        "        # Fully connected layers\n",
        "        self._head = nn.Sequential(\n",
        "            # First fully connected layer\n",
        "            # in_features = total number of weight in last conv layer = 16 * 5 * 5\n",
        "            nn.Linear(in_features=16 * 5 * 5, out_features=120), \n",
        "            \n",
        "            # ReLU activation\n",
        "            nn.ReLU(inplace=True),\n",
        "            \n",
        "            # second fully connected layer\n",
        "            # in_features = output of last linear layer = 120 \n",
        "            nn.Linear(in_features=120, out_features=84), \n",
        "            \n",
        "            # ReLU activation\n",
        "            nn.ReLU(inplace=True),\n",
        "            \n",
        "            # Third fully connected layer. It is also output layer\n",
        "            # in_features = output of last linear layer = 84\n",
        "            # and out_features = number of classes = 10 (0-9)\n",
        "            nn.Linear(in_features=84, out_features=10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # apply feature extractor\n",
        "        x = self._body(x)\n",
        "        # flatten the output of conv layers\n",
        "        # dimension should be batch_size * number_of weight_in_last conv_layer\n",
        "        x = x.view(x.size()[0], -1)\n",
        "        # apply classification head\n",
        "        x = self._head(x)\n",
        "        return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0skH0fkYQSsj",
        "colab_type": "code",
        "outputId": "7aede183-d405-416f-cb83-3eff956fd6d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        }
      },
      "source": [
        "model = LeNetBN()\n",
        "print(model)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LeNetBN(\n",
            "  (_body): Sequential(\n",
            "    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (_head): Sequential(\n",
            "    (0): Linear(in_features=400, out_features=120, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Linear(in_features=120, out_features=84, bias=True)\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): Linear(in_features=84, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRAT2WWsvWQz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_test_transforms_SVHM = transforms.Compose([\n",
        "    #convert to grayscale                                             \n",
        "    transforms.Grayscale(1),\n",
        "    #re-scale image tensor values between 0-1. image_tensor /= 255\n",
        "    transforms.ToTensor(),\n",
        "    # subtract mean (0.3977) and divide by variance (0.2329).\n",
        "    # This mean and variance is calculated on training data \n",
        "    transforms.Normalize((0.3977, ), (0.2329, ))\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Jzggwr8TOwo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#download data from Google Drive\n",
        "trainSVHM = datasets.SVHN('../content/gdrive/My Drive/SVHN', split='train', download=False, transform=train_test_transforms_SVHM)\n",
        "testSVHM = datasets.SVHN('../content/gdrive/My Drive/SVHN', split='test', download=False, transform=train_test_transforms_SVHM)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eeWtl4sYIrNw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "768ed49d-2012-4fd7-ee39-195a50df485a"
      },
      "source": [
        "#generate diferent image of digits bby OpenCV\n",
        "train_aug = []\n",
        "for digit in range(10):\n",
        "    for font in [0,2,3,4,6,7]:\n",
        "        for scale in [0.8, 1.0]:\n",
        "            for color in [70, 100, 120, 160, 200, 220, 255]:\n",
        "                for thik in [0,1,2]:\n",
        "                    for line in [cv2.FILLED, cv2.LINE_4, cv2.LINE_8, cv2.LINE_AA]:\n",
        "                        img = np.zeros((32,32), dtype=np.uint8)\n",
        "                        img = cv2.putText(img, str(digit), (6,25), font, scale, color, thik, line)\n",
        "                        if(len(train_aug)==500):\n",
        "                          #for example\n",
        "                          plt.imshow(img,'gray');plt.show()\n",
        "                        train_aug.append((transforms.Normalize((0.3977, ), (0.2329, ))(transforms.ToTensor()(img)), digit)) #transform to tensor and normalize. put in list"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAMdklEQVR4nO3dX6hl5X3G8e9T/7QlCtHaDsNoarTS\nEEI6ikgKEmwgwXozCkUMFKYQOKFE0ItCJYVm2qukREOvLLZKpLSmtjZ1kFIzFYu5Mo52HGecJmoY\nicPRIdig3iQ1/nqx15Azw+yz9+y91t77nPf7gcNZe5111vrN4jz7fddae943VYWk7e+Xll2ApMUw\n7FIjDLvUCMMuNcKwS40w7FIjzp/nl5PcDPw1cB7wd1X11Qnb+5xPGlhV5WzrM+tz9iTnAT8APgu8\nATwHfL6qXt7kdwy7NLBxYZ+nG38D8GpV/bCqfgZ8C9gzx/4kDWiesO8CfrTh9RvdOkkraK5r9mkk\nWQPWhj6OpM3NE/YTwBUbXl/erTtNVT0APABes0vLNE83/jngmiQfTXIhcAewv5+yJPVt5pa9qt5P\ncifwJKNHbw9V1dHeKpPUq5kfvc10MLvx0uCGePQmaQsx7FIjDLvUCMMuNcKwS40w7FIjDLvUCMMu\nNcKwS40w7FIjDLvUiMH/P7tWw6pM85Wc9WPbWgBbdqkRhl1qhGGXGmHYpUYYdqkRhl1qhI/eVtSq\nPCrr2xD/Lh/nTceWXWqEYZcaYdilRhh2qRGGXWqEYZcaMdejtyTHgXeBnwPvV9X1fRQlqX99PGf/\nvar6cQ/7kTQgu/FSI+YNewHfSfJ8krU+CpI0jHm78TdW1YkkvwEcSPI/VfXMxg26NwHfCKQl623K\n5iT7gPeq6uubbLM9P/A9gO362fgh+Nn40/U+ZXOSDyW5+NQy8DngyKz7kzSsebrxO4Bvd++q5wP/\nWFX/0UtVjViV1nuIlnGR/7bNjmWr/wu9deOnOpjd+NMY9uG1GPbeu/GSthbDLjXCsEuNMOxSIwy7\n1AgHnBxYq3elxx1v0edj3PFavEtvyy41wrBLjTDsUiMMu9QIwy41wrvx28yq32XerL5VeXKxXdmy\nS40w7FIjDLvUCMMuNcKwS40w7FIjDLvUCMMuNcKwS40w7FIjDLvUCMMuNcKwS42YGPYkDyU5meTI\nhnWXJjmQ5JXu+yXDlilpXtO07N8Ebj5j3T3AU1V1DfBU91rSCpsY9m6+9bfPWL0HeLhbfhi4tee6\nJPVs1mv2HVW13i2/yWhGV0krbO6RaqqqNpudNckasDbvcSTNZ9aW/a0kOwG67yfHbVhVD1TV9VV1\n/YzHktSDWcO+H9jbLe8FHu+nHElDyaRB/pI8AtwEXAa8BXwF+DfgUeAjwOvA7VV15k28s+1rW44o\nuOiBEld9UMkhLPIcb/XzW1Vn/QdMDHufDHs/tvof4ywM+/TGhd1P0EmNMOxSIwy71AjDLjXCsEuN\nMOxSIwy71AjDLjXCsEuNMOxSIwy71AjDLjXCsEuNMOxSIwy71AjDLjXCsEuNMOxSIwy71AjDLjXC\nsEuNMOxSIwy71AjDLjXCsEuNmBj2JA8lOZnkyIZ1+5KcSHKo+7pl2DIlzWualv2bwM1nWf+Nqtrd\nff17v2VJ6tvEsFfVM8DESRslrbZ5rtnvTHK46+Zf0ltFkgYxa9jvB64GdgPrwL3jNkyyluRgkoMz\nHktSD6aasjnJlcATVfWJc/nZWbZ1yuYebPUphWfhlM3T63XK5iQ7N7y8DTgybltJq+H8SRskeQS4\nCbgsyRvAV4CbkuwGCjgOfHHAGiX1YKpufG8Hsxvfi63ezZyF3fjp9dqNl7T1GHapEYZdaoRhlxph\n2KVGGHapEYZdaoRhlxph2KVGGHapEYZdaoRhlxph2KVGGHapEYZdaoRhlxph2KVGGHapERPHoNNk\nmw1jNMRwSpvtcysPqeTQU8OyZZcaYdilRhh2qRGGXWqEYZcaYdilRkwMe5Irkjyd5OUkR5Pc1a2/\nNMmBJK903522WVphE6d/6iZx3FlVLyS5GHgeuBX4I+DtqvpqknuAS6rqTyfsa1tO/7QZp4aans/Z\n+zHz9E9VtV5VL3TL7wLHgF3AHuDhbrOHGb0BSFpR53TN3s3Ffi3wLLCjqta7H70J7Oi1Mkm9mvrj\nskkuAh4D7q6qdzZ2g6qqxnXRk6wBa/MWKmk+U03ZnOQC4Angyaq6r1v3feCmqlrvruv/q6p+e8J+\nvGYf2Fa+FvWavR8zX7NndFYeBI6dCnpnP7C3W94LPD5vkZKGM83d+BuB7wIvAR90q7/M6Lr9UeAj\nwOvA7VX19oR92bIPbCu3WLbs/RjXsk/Vje+LYR/eVv4jNuz9mLkbL2l7MOxSIwy71AjDLjXCsEuN\ncMDJbWbcHe1Vufu86KcT+gVbdqkRhl1qhGGXGmHYpUYYdqkRhl1qhI/eBrboeeBmOdYQj+VW5RHb\nqjxyXAW27FIjDLvUCMMuNcKwS40w7FIjvBu/RFvhTv1W4B336diyS40w7FIjDLvUCMMuNcKwS40w\n7FIjppnr7YokTyd5OcnRJHd16/clOZHkUPd1y/DlSprVNHO97QR2VtULSS4GngduBW4H3quqr099\nsAanf5rVVn/2vUg+Zz/duOmfJn6opqrWgfVu+d0kx4Bd/ZYnaWjndM2e5ErgWkYzuALcmeRwkoeS\nXNJzbZJ6NHXYk1wEPAbcXVXvAPcDVwO7GbX89475vbUkB5Mc7KFeSTOaasrmJBcATwBPVtV9Z/n5\nlcATVfWJCfvxQnRKXrNPz2v20808ZXNGZ/JB4NjGoHc37k65DTgyb5GShjPN3fgbge8CLwEfdKu/\nDHyeURe+gOPAF7ubeZvty+ZqSValp2ArPLxxLftU3fi+GPblMeztmLkbL2l7MOxSIwy71AjDLjXC\nsEuNcMDJRngXXLbsUiMMu9QIwy41wrBLjTDsUiMMu9QIwy41wrBLjTDsUiMMu9QIwy41wrBLjTDs\nUiMMu9QIwy41wrBLjTDsUiMMu9QIwy41Ypq53n4lyfeSvJjkaJK/6NZ/NMmzSV5N8k9JLhy+XEmz\nmqZl/ynwmar6HUZzu92c5FPA14BvVNVvAf8LfGG4MiXNa2LYa+S97uUF3VcBnwH+pVv/MHDrIBVK\n6sVU1+xJzktyCDgJHABeA35SVe93m7wB7BqmREl9mCrsVfXzqtoNXA7cAHxs2gMkWUtyMMnBGWuU\n1INzuhtfVT8BngZ+F/hwklOTTFwOnBjzOw9U1fVVdf1clUqayzR34389yYe75V8FPgscYxT6P+g2\n2ws8PlSRkuaXqtp8g+STjG7AncfozeHRqvrLJFcB3wIuBf4b+MOq+umEfW1+MElzq6qzzvU1Mex9\nMuzS8MaF3U/QSY0w7FIjDLvUCMMuNcKwS404f/Imvfox8Hq3fFn3etms43TWcbqtVsdvjvvBQh+9\nnXbg5OAqfKrOOqyjlTrsxkuNMOxSI5YZ9geWeOyNrON01nG6bVPH0q7ZJS2W3XipEUsJe5Kbk3y/\nG6zynmXU0NVxPMlLSQ4tcnCNJA8lOZnkyIZ1lyY5kOSV7vslS6pjX5IT3Tk5lOSWBdRxRZKnk7zc\nDWp6V7d+oedkkzoWek4GG+S1qhb6xei/yr4GXAVcCLwIfHzRdXS1HAcuW8JxPw1cBxzZsO6vgHu6\n5XuAry2pjn3Anyz4fOwEruuWLwZ+AHx80edkkzoWek6AABd1yxcAzwKfAh4F7ujW/w3wx+ey32W0\n7DcAr1bVD6vqZ4z+T/yeJdSxNFX1DPD2Gav3MBo3ABY0gOeYOhauqtar6oVu+V1Gg6PsYsHnZJM6\nFqpGeh/kdRlh3wX8aMPrZQ5WWcB3kjyfZG1JNZyyo6rWu+U3gR1LrOXOJIe7bv7glxMbJbkSuJZR\na7a0c3JGHbDgczLEIK+t36C7saquA34f+FKSTy+7IBi9szN6I1qG+4GrGc0RsA7cu6gDJ7kIeAy4\nu6re2fizRZ6Ts9Sx8HNScwzyOs4ywn4CuGLD67GDVQ6tqk50308C32Z0UpflrSQ7AbrvJ5dRRFW9\n1f2hfQD8LQs6J0kuYBSwf6iqf+1WL/ycnK2OZZ2T7tjnPMjrOMsI+3PANd2dxQuBO4D9iy4iyYeS\nXHxqGfgccGTz3xrUfkYDd8ISB/A8Fa7ObSzgnCQJ8CBwrKru2/CjhZ6TcXUs+pwMNsjrou4wnnG3\n8RZGdzpfA/5sSTVcxehJwIvA0UXWATzCqDv4f4yuvb4A/BrwFPAK8J/ApUuq4++Bl4DDjMK2cwF1\n3Mioi34YONR93bLoc7JJHQs9J8AnGQ3iepjRG8ufb/ib/R7wKvDPwC+fy379BJ3UiNZv0EnNMOxS\nIwy71AjDLjXCsEuNMOxSIwy71AjDLjXi/wFMHI0dWgioHgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJSPlc3NItZz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5cc344bf-6bed-4eda-86f5-734b70093937"
      },
      "source": [
        "test_aug = []\n",
        "for digit in range(10):\n",
        "    for scale in [0.8, 1.0]:\n",
        "        for color in [70, 100, 120, 160, 200, 220, 255]:\n",
        "            for thik in [0,1,2]:\n",
        "                for line in [cv2.FILLED, cv2.LINE_4, cv2.LINE_8, cv2.LINE_AA]:\n",
        "                    img = np.zeros((32,32), dtype=np.uint8)\n",
        "                    img = cv2.putText(img, str(digit), (6,25), 16, scale, color, thik, line)\n",
        "                    #plt.imshow(img,'gray');plt.show()\n",
        "                    test_aug.append((transforms.Normalize((0.3977, ), (0.2329, ))(transforms.ToTensor()(img)), digit))#transform to tensor and normalize. put in list\n",
        "print(len(test_aug))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1680\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kk2gufzwQfgs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.manual_seed(42)\n",
        "#if torch.cuda.is_available():\n",
        "#    torch.backend.cudnn_benchmark_enabled = True\n",
        "#    torch.backend.cudnn.deterministic = True\n",
        "\n",
        "#training parameters\n",
        "batch_size = 32\n",
        "epochs_count = 20\n",
        "learning_rate = 0.01\n",
        "log_interval = 100\n",
        "test_interval = 1\n",
        "num_workers  = 10\n",
        "device = 'cuda'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhG-KqsEviSQ",
        "colab_type": "code",
        "outputId": "2cbb0024-ab43-4eba-9c51-eac6167a3c1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "    print('GPU')\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "    batch_size = 16\n",
        "    num_workers = 0\n",
        "    epochs_count = 10\n",
        "    print('CPU')\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "#SGDoptimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "#array for plotting\n",
        "best_loss = torch.tensor(np.inf)\n",
        "\n",
        "# epoch train/test loss\n",
        "epoch_train_loss = np.array([])\n",
        "epoch_test_loss = np.array([])\n",
        "\n",
        "# epch train/test accuracy\n",
        "epoch_train_acc = np.array([])\n",
        "epoch_test_acc = np.array([])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGK49aIQSgb4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train dataloader\n",
        "train_loader_SVHM = torch.utils.data.DataLoader(\n",
        "     trainSVHM + train_aug,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=num_workers\n",
        ")\n",
        "# test dataloader\n",
        "test_loader_SVHM = torch.utils.data.DataLoader(\n",
        "    testSVHM + test_aug,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=num_workers\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VE6H7m-svksn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, optimizer, train_loader, epoch_idx):\n",
        "    # change model in training mood\n",
        "    model.train()\n",
        "\n",
        "    # to get batch loss\n",
        "    batch_loss = np.array([])\n",
        "\n",
        "    # to get batch accuracy\n",
        "    batch_acc = np.array([])\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "\n",
        "        # clone target\n",
        "        indx_target = target.clone()\n",
        "        # send data to device (its is medatory if GPU has to be used)\n",
        "        data = data.to(device)\n",
        "        # send target to device\n",
        "        target = target.to(device)\n",
        "\n",
        "        # reset parameters gradient to zero\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward pass to the model\n",
        "        output = model(data)\n",
        "\n",
        "        # cross entropy loss\n",
        "        loss = F.cross_entropy(output, target)\n",
        "\n",
        "        # find gradients w.r.t training parameters\n",
        "        loss.backward()\n",
        "        # Update parameters using gardients\n",
        "        optimizer.step()\n",
        "\n",
        "        batch_loss = np.append(batch_loss, [loss.item()])\n",
        "\n",
        "        # Score to probability using softmax\n",
        "        prob = F.softmax(output, dim=1)\n",
        "\n",
        "        # get the index of the max probability\n",
        "        pred = prob.data.max(dim=1)[1]\n",
        "\n",
        "        # correct prediction\n",
        "        correct = pred.cpu().eq(indx_target).sum()\n",
        "\n",
        "        # accuracy\n",
        "        acc = float(correct) / float(len(data))\n",
        "\n",
        "        batch_acc = np.append(batch_acc, [acc])\n",
        "\n",
        "        if batch_idx % log_interval == 0 and batch_idx > 0:\n",
        "            print(\n",
        "                'Train Epoch: {} [{}/{}] Loss: {:.6f} Acc: {:.4f}'.format(\n",
        "                    epoch_idx, batch_idx * len(data), len(train_loader.dataset), loss.item(), acc\n",
        "                )\n",
        "            )\n",
        "\n",
        "    epoch_loss = batch_loss.mean()\n",
        "    epoch_acc = batch_acc.mean()\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "\n",
        "def validate(model, test_loader):\n",
        "    #change to validate mode\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    count_corect_predictions = 0\n",
        "    for data, target in test_loader:\n",
        "        indx_target = target.clone()\n",
        "        data = data.to(device)\n",
        "\n",
        "        target = target.to(device)\n",
        "\n",
        "        output = model(data)\n",
        "        # add loss for each mini batch\n",
        "        test_loss += F.cross_entropy(output, target).item()\n",
        "\n",
        "        # Score to probability using softmax\n",
        "        prob = F.softmax(output, dim=1)\n",
        "\n",
        "        # get the index of the max probability\n",
        "        pred = prob.data.max(dim=1)[1]\n",
        "\n",
        "        # add correct prediction count\n",
        "        count_corect_predictions += pred.cpu().eq(indx_target).sum()\n",
        "\n",
        "    # average over number of mini-batches\n",
        "    test_loss = test_loss / len(test_loader)\n",
        "\n",
        "    # average over number of dataset\n",
        "    accuracy = 100. * count_corect_predictions / len(test_loader.dataset)\n",
        "\n",
        "    print(\n",
        "        '\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "            test_loss, count_corect_predictions, len(test_loader.dataset), accuracy\n",
        "        )\n",
        "    )\n",
        "    return test_loss, accuracy / 100.0\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjythJS-vkv_",
        "colab_type": "code",
        "outputId": "5dfc0f19-56cf-404d-df84-d759c18e87cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#train\n",
        "t_begin = time.time()\n",
        "#for save best model\n",
        "best_model = copy.deepcopy(model)\n",
        "for epoch in range(epochs_count):\n",
        "\n",
        "    train_loss, train_acc = train(model, optimizer, train_loader_SVHM, epoch)\n",
        "\n",
        "    epoch_train_loss = np.append(epoch_train_loss, [train_loss])\n",
        "\n",
        "    epoch_train_acc = np.append(epoch_train_acc, [train_acc])\n",
        "\n",
        "    elapsed_time = time.time() - t_begin\n",
        "    speed_epoch = elapsed_time / (epoch + 1)\n",
        "    speed_batch = speed_epoch / len(train_loader_SVHM)\n",
        "    eta = speed_epoch * epochs_count - elapsed_time\n",
        "\n",
        "    print(\n",
        "        \"Elapsed {:.2f}s, {:.2f} s/epoch, {:.2f} s/batch, ets {:.2f}s\".format(\n",
        "            elapsed_time, speed_epoch, speed_batch, eta\n",
        "        )\n",
        "    )\n",
        "\n",
        "    if epoch % test_interval == 0:\n",
        "        current_loss, current_accuracy = validate(model, test_loader_SVHM)\n",
        "\n",
        "        epoch_test_loss = np.append(epoch_test_loss, [current_loss])\n",
        "\n",
        "        epoch_test_acc = np.append(epoch_test_acc, [current_accuracy])\n",
        "\n",
        "        if current_loss < best_loss:\n",
        "            best_loss = current_loss\n",
        "            best_model = copy.deepcopy(model)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [3200/83337] Loss: 2.306003 Acc: 0.0938\n",
            "Train Epoch: 0 [6400/83337] Loss: 2.242829 Acc: 0.1875\n",
            "Train Epoch: 0 [9600/83337] Loss: 2.116178 Acc: 0.3125\n",
            "Train Epoch: 0 [12800/83337] Loss: 2.129138 Acc: 0.1875\n",
            "Train Epoch: 0 [16000/83337] Loss: 1.970820 Acc: 0.4062\n",
            "Train Epoch: 0 [19200/83337] Loss: 1.800282 Acc: 0.4688\n",
            "Train Epoch: 0 [22400/83337] Loss: 1.670614 Acc: 0.3750\n",
            "Train Epoch: 0 [25600/83337] Loss: 1.383132 Acc: 0.6250\n",
            "Train Epoch: 0 [28800/83337] Loss: 1.299796 Acc: 0.5625\n",
            "Train Epoch: 0 [32000/83337] Loss: 1.160228 Acc: 0.6875\n",
            "Train Epoch: 0 [35200/83337] Loss: 0.903981 Acc: 0.7500\n",
            "Train Epoch: 0 [38400/83337] Loss: 1.051351 Acc: 0.6875\n",
            "Train Epoch: 0 [41600/83337] Loss: 0.987173 Acc: 0.6562\n",
            "Train Epoch: 0 [44800/83337] Loss: 0.885238 Acc: 0.7188\n",
            "Train Epoch: 0 [48000/83337] Loss: 1.033064 Acc: 0.7188\n",
            "Train Epoch: 0 [51200/83337] Loss: 1.011109 Acc: 0.6875\n",
            "Train Epoch: 0 [54400/83337] Loss: 0.648465 Acc: 0.7812\n",
            "Train Epoch: 0 [57600/83337] Loss: 0.429743 Acc: 0.8750\n",
            "Train Epoch: 0 [60800/83337] Loss: 0.654756 Acc: 0.7188\n",
            "Train Epoch: 0 [64000/83337] Loss: 0.632817 Acc: 0.8125\n",
            "Train Epoch: 0 [67200/83337] Loss: 0.610809 Acc: 0.7812\n",
            "Train Epoch: 0 [70400/83337] Loss: 0.957340 Acc: 0.7188\n",
            "Train Epoch: 0 [73600/83337] Loss: 0.466330 Acc: 0.8438\n",
            "Train Epoch: 0 [76800/83337] Loss: 0.515645 Acc: 0.8438\n",
            "Train Epoch: 0 [80000/83337] Loss: 0.693421 Acc: 0.7812\n",
            "Train Epoch: 0 [83200/83337] Loss: 0.761194 Acc: 0.7500\n",
            "Elapsed 19.07s, 19.07 s/epoch, 0.01 s/batch, ets 362.29s\n",
            "\n",
            "Test set: Average loss: 0.7982, Accuracy: 21403/27712 (77%)\n",
            "\n",
            "Train Epoch: 1 [3200/83337] Loss: 0.821590 Acc: 0.7812\n",
            "Train Epoch: 1 [6400/83337] Loss: 0.577136 Acc: 0.8125\n",
            "Train Epoch: 1 [9600/83337] Loss: 0.453983 Acc: 0.9062\n",
            "Train Epoch: 1 [12800/83337] Loss: 0.427172 Acc: 0.8438\n",
            "Train Epoch: 1 [16000/83337] Loss: 0.634370 Acc: 0.8750\n",
            "Train Epoch: 1 [19200/83337] Loss: 0.778770 Acc: 0.7500\n",
            "Train Epoch: 1 [22400/83337] Loss: 1.120974 Acc: 0.7500\n",
            "Train Epoch: 1 [25600/83337] Loss: 0.354480 Acc: 0.8750\n",
            "Train Epoch: 1 [28800/83337] Loss: 0.442174 Acc: 0.8750\n",
            "Train Epoch: 1 [32000/83337] Loss: 0.949153 Acc: 0.8438\n",
            "Train Epoch: 1 [35200/83337] Loss: 0.700837 Acc: 0.8438\n",
            "Train Epoch: 1 [38400/83337] Loss: 0.627920 Acc: 0.8438\n",
            "Train Epoch: 1 [41600/83337] Loss: 0.851802 Acc: 0.7812\n",
            "Train Epoch: 1 [44800/83337] Loss: 0.491906 Acc: 0.8438\n",
            "Train Epoch: 1 [48000/83337] Loss: 0.183512 Acc: 0.9375\n",
            "Train Epoch: 1 [51200/83337] Loss: 0.363598 Acc: 0.8750\n",
            "Train Epoch: 1 [54400/83337] Loss: 0.457520 Acc: 0.7812\n",
            "Train Epoch: 1 [57600/83337] Loss: 0.601346 Acc: 0.8125\n",
            "Train Epoch: 1 [60800/83337] Loss: 0.463771 Acc: 0.8750\n",
            "Train Epoch: 1 [64000/83337] Loss: 0.794171 Acc: 0.8125\n",
            "Train Epoch: 1 [67200/83337] Loss: 0.603664 Acc: 0.8438\n",
            "Train Epoch: 1 [70400/83337] Loss: 0.321105 Acc: 0.9375\n",
            "Train Epoch: 1 [73600/83337] Loss: 0.392982 Acc: 0.8750\n",
            "Train Epoch: 1 [76800/83337] Loss: 0.254135 Acc: 0.9375\n",
            "Train Epoch: 1 [80000/83337] Loss: 0.156948 Acc: 1.0000\n",
            "Train Epoch: 1 [83200/83337] Loss: 0.348690 Acc: 0.9062\n",
            "Elapsed 44.11s, 22.06 s/epoch, 0.01 s/batch, ets 397.03s\n",
            "\n",
            "Test set: Average loss: 0.5664, Accuracy: 23216/27712 (84%)\n",
            "\n",
            "Train Epoch: 2 [3200/83337] Loss: 0.333454 Acc: 0.9062\n",
            "Train Epoch: 2 [6400/83337] Loss: 0.286214 Acc: 0.9375\n",
            "Train Epoch: 2 [9600/83337] Loss: 0.558759 Acc: 0.8125\n",
            "Train Epoch: 2 [12800/83337] Loss: 0.166418 Acc: 1.0000\n",
            "Train Epoch: 2 [16000/83337] Loss: 0.504107 Acc: 0.8125\n",
            "Train Epoch: 2 [19200/83337] Loss: 0.690383 Acc: 0.8750\n",
            "Train Epoch: 2 [22400/83337] Loss: 0.217383 Acc: 0.9688\n",
            "Train Epoch: 2 [25600/83337] Loss: 0.382414 Acc: 0.9062\n",
            "Train Epoch: 2 [28800/83337] Loss: 0.259967 Acc: 0.9062\n",
            "Train Epoch: 2 [32000/83337] Loss: 0.461093 Acc: 0.9062\n",
            "Train Epoch: 2 [35200/83337] Loss: 0.289987 Acc: 0.9062\n",
            "Train Epoch: 2 [38400/83337] Loss: 0.527616 Acc: 0.8750\n",
            "Train Epoch: 2 [41600/83337] Loss: 0.634393 Acc: 0.7812\n",
            "Train Epoch: 2 [44800/83337] Loss: 0.644980 Acc: 0.8125\n",
            "Train Epoch: 2 [48000/83337] Loss: 0.261805 Acc: 0.9062\n",
            "Train Epoch: 2 [51200/83337] Loss: 0.724797 Acc: 0.8438\n",
            "Train Epoch: 2 [54400/83337] Loss: 0.877348 Acc: 0.9062\n",
            "Train Epoch: 2 [57600/83337] Loss: 0.357659 Acc: 0.8750\n",
            "Train Epoch: 2 [60800/83337] Loss: 0.374833 Acc: 0.8750\n",
            "Train Epoch: 2 [64000/83337] Loss: 0.351692 Acc: 0.8750\n",
            "Train Epoch: 2 [67200/83337] Loss: 0.600941 Acc: 0.8438\n",
            "Train Epoch: 2 [70400/83337] Loss: 0.489538 Acc: 0.9062\n",
            "Train Epoch: 2 [73600/83337] Loss: 0.435084 Acc: 0.9062\n",
            "Train Epoch: 2 [76800/83337] Loss: 0.298720 Acc: 0.8438\n",
            "Train Epoch: 2 [80000/83337] Loss: 0.477608 Acc: 0.9062\n",
            "Train Epoch: 2 [83200/83337] Loss: 0.323897 Acc: 0.9062\n",
            "Elapsed 70.81s, 23.60 s/epoch, 0.01 s/batch, ets 401.23s\n",
            "\n",
            "Test set: Average loss: 0.4906, Accuracy: 23742/27712 (86%)\n",
            "\n",
            "Train Epoch: 3 [3200/83337] Loss: 0.161560 Acc: 0.9375\n",
            "Train Epoch: 3 [6400/83337] Loss: 0.430266 Acc: 0.8438\n",
            "Train Epoch: 3 [9600/83337] Loss: 0.522672 Acc: 0.8125\n",
            "Train Epoch: 3 [12800/83337] Loss: 0.378489 Acc: 0.9062\n",
            "Train Epoch: 3 [16000/83337] Loss: 0.375703 Acc: 0.9062\n",
            "Train Epoch: 3 [19200/83337] Loss: 0.183586 Acc: 0.9375\n",
            "Train Epoch: 3 [22400/83337] Loss: 0.271667 Acc: 0.8750\n",
            "Train Epoch: 3 [25600/83337] Loss: 0.314732 Acc: 0.9062\n",
            "Train Epoch: 3 [28800/83337] Loss: 0.258648 Acc: 0.9688\n",
            "Train Epoch: 3 [32000/83337] Loss: 0.421141 Acc: 0.8438\n",
            "Train Epoch: 3 [35200/83337] Loss: 0.337770 Acc: 0.8750\n",
            "Train Epoch: 3 [38400/83337] Loss: 0.268389 Acc: 0.9375\n",
            "Train Epoch: 3 [41600/83337] Loss: 0.702031 Acc: 0.8438\n",
            "Train Epoch: 3 [44800/83337] Loss: 0.376283 Acc: 0.9062\n",
            "Train Epoch: 3 [48000/83337] Loss: 0.396189 Acc: 0.8750\n",
            "Train Epoch: 3 [51200/83337] Loss: 0.258165 Acc: 0.9375\n",
            "Train Epoch: 3 [54400/83337] Loss: 0.875905 Acc: 0.8125\n",
            "Train Epoch: 3 [57600/83337] Loss: 0.638985 Acc: 0.7500\n",
            "Train Epoch: 3 [60800/83337] Loss: 0.476771 Acc: 0.8125\n",
            "Train Epoch: 3 [64000/83337] Loss: 0.171175 Acc: 0.9688\n",
            "Train Epoch: 3 [67200/83337] Loss: 0.553743 Acc: 0.7500\n",
            "Train Epoch: 3 [70400/83337] Loss: 0.354751 Acc: 0.8438\n",
            "Train Epoch: 3 [73600/83337] Loss: 0.323067 Acc: 0.8750\n",
            "Train Epoch: 3 [76800/83337] Loss: 0.229934 Acc: 0.9062\n",
            "Train Epoch: 3 [80000/83337] Loss: 0.262520 Acc: 0.9375\n",
            "Train Epoch: 3 [83200/83337] Loss: 0.765006 Acc: 0.8438\n",
            "Elapsed 97.89s, 24.47 s/epoch, 0.01 s/batch, ets 391.57s\n",
            "\n",
            "Test set: Average loss: 0.4637, Accuracy: 23960/27712 (86%)\n",
            "\n",
            "Train Epoch: 4 [3200/83337] Loss: 0.184884 Acc: 0.9688\n",
            "Train Epoch: 4 [6400/83337] Loss: 0.289796 Acc: 0.9688\n",
            "Train Epoch: 4 [9600/83337] Loss: 0.602770 Acc: 0.8125\n",
            "Train Epoch: 4 [12800/83337] Loss: 0.086289 Acc: 1.0000\n",
            "Train Epoch: 4 [16000/83337] Loss: 0.188485 Acc: 0.9375\n",
            "Train Epoch: 4 [19200/83337] Loss: 0.219094 Acc: 0.9375\n",
            "Train Epoch: 4 [22400/83337] Loss: 0.595930 Acc: 0.9062\n",
            "Train Epoch: 4 [25600/83337] Loss: 0.167707 Acc: 0.9688\n",
            "Train Epoch: 4 [28800/83337] Loss: 0.328425 Acc: 0.9375\n",
            "Train Epoch: 4 [32000/83337] Loss: 0.362431 Acc: 0.8438\n",
            "Train Epoch: 4 [35200/83337] Loss: 0.613078 Acc: 0.8750\n",
            "Train Epoch: 4 [38400/83337] Loss: 0.413886 Acc: 0.8438\n",
            "Train Epoch: 4 [41600/83337] Loss: 0.477976 Acc: 0.9062\n",
            "Train Epoch: 4 [44800/83337] Loss: 0.572027 Acc: 0.9062\n",
            "Train Epoch: 4 [48000/83337] Loss: 0.369081 Acc: 0.8750\n",
            "Train Epoch: 4 [51200/83337] Loss: 0.332542 Acc: 0.9062\n",
            "Train Epoch: 4 [54400/83337] Loss: 0.276845 Acc: 0.9375\n",
            "Train Epoch: 4 [57600/83337] Loss: 0.257009 Acc: 0.9062\n",
            "Train Epoch: 4 [60800/83337] Loss: 0.542947 Acc: 0.8750\n",
            "Train Epoch: 4 [64000/83337] Loss: 0.472174 Acc: 0.8125\n",
            "Train Epoch: 4 [67200/83337] Loss: 0.294410 Acc: 0.8750\n",
            "Train Epoch: 4 [70400/83337] Loss: 0.652682 Acc: 0.9375\n",
            "Train Epoch: 4 [73600/83337] Loss: 0.534735 Acc: 0.8438\n",
            "Train Epoch: 4 [76800/83337] Loss: 0.389279 Acc: 0.7812\n",
            "Train Epoch: 4 [80000/83337] Loss: 0.212327 Acc: 0.9688\n",
            "Train Epoch: 4 [83200/83337] Loss: 0.323891 Acc: 0.8750\n",
            "Elapsed 123.41s, 24.68 s/epoch, 0.01 s/batch, ets 370.22s\n",
            "\n",
            "Test set: Average loss: 0.4195, Accuracy: 24296/27712 (88%)\n",
            "\n",
            "Train Epoch: 5 [3200/83337] Loss: 0.406756 Acc: 0.8125\n",
            "Train Epoch: 5 [6400/83337] Loss: 0.747117 Acc: 0.8438\n",
            "Train Epoch: 5 [9600/83337] Loss: 0.569670 Acc: 0.8750\n",
            "Train Epoch: 5 [12800/83337] Loss: 0.828896 Acc: 0.8750\n",
            "Train Epoch: 5 [16000/83337] Loss: 0.586394 Acc: 0.8125\n",
            "Train Epoch: 5 [19200/83337] Loss: 0.303670 Acc: 0.9062\n",
            "Train Epoch: 5 [22400/83337] Loss: 0.639340 Acc: 0.8125\n",
            "Train Epoch: 5 [25600/83337] Loss: 0.238635 Acc: 0.8750\n",
            "Train Epoch: 5 [28800/83337] Loss: 0.281223 Acc: 0.9062\n",
            "Train Epoch: 5 [32000/83337] Loss: 0.161561 Acc: 0.9375\n",
            "Train Epoch: 5 [35200/83337] Loss: 0.144154 Acc: 0.9688\n",
            "Train Epoch: 5 [38400/83337] Loss: 0.229839 Acc: 0.9375\n",
            "Train Epoch: 5 [41600/83337] Loss: 0.358452 Acc: 0.9062\n",
            "Train Epoch: 5 [44800/83337] Loss: 0.087264 Acc: 0.9688\n",
            "Train Epoch: 5 [48000/83337] Loss: 0.431441 Acc: 0.8438\n",
            "Train Epoch: 5 [51200/83337] Loss: 0.263772 Acc: 0.9062\n",
            "Train Epoch: 5 [54400/83337] Loss: 0.197732 Acc: 0.9688\n",
            "Train Epoch: 5 [57600/83337] Loss: 0.194523 Acc: 0.9375\n",
            "Train Epoch: 5 [60800/83337] Loss: 0.197909 Acc: 0.9062\n",
            "Train Epoch: 5 [64000/83337] Loss: 0.160307 Acc: 1.0000\n",
            "Train Epoch: 5 [67200/83337] Loss: 0.198832 Acc: 0.9375\n",
            "Train Epoch: 5 [70400/83337] Loss: 0.511105 Acc: 0.9062\n",
            "Train Epoch: 5 [73600/83337] Loss: 0.152915 Acc: 0.9375\n",
            "Train Epoch: 5 [76800/83337] Loss: 0.181479 Acc: 0.9375\n",
            "Train Epoch: 5 [80000/83337] Loss: 0.641291 Acc: 0.7812\n",
            "Train Epoch: 5 [83200/83337] Loss: 0.112149 Acc: 0.9688\n",
            "Elapsed 149.01s, 24.83 s/epoch, 0.01 s/batch, ets 347.68s\n",
            "\n",
            "Test set: Average loss: 0.4144, Accuracy: 24308/27712 (88%)\n",
            "\n",
            "Train Epoch: 6 [3200/83337] Loss: 0.707088 Acc: 0.8125\n",
            "Train Epoch: 6 [6400/83337] Loss: 0.319349 Acc: 0.8750\n",
            "Train Epoch: 6 [9600/83337] Loss: 0.394905 Acc: 0.8438\n",
            "Train Epoch: 6 [12800/83337] Loss: 0.398787 Acc: 0.8438\n",
            "Train Epoch: 6 [16000/83337] Loss: 0.482238 Acc: 0.7812\n",
            "Train Epoch: 6 [19200/83337] Loss: 0.407761 Acc: 0.9062\n",
            "Train Epoch: 6 [22400/83337] Loss: 0.524735 Acc: 0.8125\n",
            "Train Epoch: 6 [25600/83337] Loss: 0.478225 Acc: 0.9062\n",
            "Train Epoch: 6 [28800/83337] Loss: 0.515205 Acc: 0.9062\n",
            "Train Epoch: 6 [32000/83337] Loss: 0.349669 Acc: 0.8750\n",
            "Train Epoch: 6 [35200/83337] Loss: 0.299201 Acc: 0.8750\n",
            "Train Epoch: 6 [38400/83337] Loss: 0.430630 Acc: 0.9062\n",
            "Train Epoch: 6 [41600/83337] Loss: 0.261361 Acc: 0.9062\n",
            "Train Epoch: 6 [44800/83337] Loss: 0.339287 Acc: 0.8750\n",
            "Train Epoch: 6 [48000/83337] Loss: 0.209541 Acc: 0.9688\n",
            "Train Epoch: 6 [51200/83337] Loss: 0.225633 Acc: 0.9375\n",
            "Train Epoch: 6 [54400/83337] Loss: 0.177460 Acc: 0.9375\n",
            "Train Epoch: 6 [57600/83337] Loss: 0.281034 Acc: 0.8750\n",
            "Train Epoch: 6 [60800/83337] Loss: 0.222658 Acc: 0.9375\n",
            "Train Epoch: 6 [64000/83337] Loss: 0.493458 Acc: 0.9062\n",
            "Train Epoch: 6 [67200/83337] Loss: 0.599128 Acc: 0.8750\n",
            "Train Epoch: 6 [70400/83337] Loss: 0.486473 Acc: 0.8438\n",
            "Train Epoch: 6 [73600/83337] Loss: 0.492061 Acc: 0.8438\n",
            "Train Epoch: 6 [76800/83337] Loss: 0.371975 Acc: 0.8750\n",
            "Train Epoch: 6 [80000/83337] Loss: 0.262041 Acc: 0.9375\n",
            "Train Epoch: 6 [83200/83337] Loss: 0.273662 Acc: 0.9375\n",
            "Elapsed 174.38s, 24.91 s/epoch, 0.01 s/batch, ets 323.85s\n",
            "\n",
            "Test set: Average loss: 0.3907, Accuracy: 24503/27712 (88%)\n",
            "\n",
            "Train Epoch: 7 [3200/83337] Loss: 0.349604 Acc: 0.9375\n",
            "Train Epoch: 7 [6400/83337] Loss: 0.164339 Acc: 0.9375\n",
            "Train Epoch: 7 [9600/83337] Loss: 0.173347 Acc: 0.9688\n",
            "Train Epoch: 7 [12800/83337] Loss: 0.200259 Acc: 0.9375\n",
            "Train Epoch: 7 [16000/83337] Loss: 0.460819 Acc: 0.8750\n",
            "Train Epoch: 7 [19200/83337] Loss: 0.431816 Acc: 0.8750\n",
            "Train Epoch: 7 [22400/83337] Loss: 0.160842 Acc: 0.9375\n",
            "Train Epoch: 7 [25600/83337] Loss: 0.448171 Acc: 0.8750\n",
            "Train Epoch: 7 [28800/83337] Loss: 0.120087 Acc: 0.9688\n",
            "Train Epoch: 7 [32000/83337] Loss: 0.328639 Acc: 0.9062\n",
            "Train Epoch: 7 [35200/83337] Loss: 0.583945 Acc: 0.7812\n",
            "Train Epoch: 7 [38400/83337] Loss: 0.174252 Acc: 0.9375\n",
            "Train Epoch: 7 [41600/83337] Loss: 0.327598 Acc: 0.9062\n",
            "Train Epoch: 7 [44800/83337] Loss: 0.182411 Acc: 0.9375\n",
            "Train Epoch: 7 [48000/83337] Loss: 0.336934 Acc: 0.8750\n",
            "Train Epoch: 7 [51200/83337] Loss: 0.711591 Acc: 0.8750\n",
            "Train Epoch: 7 [54400/83337] Loss: 0.173323 Acc: 0.9375\n",
            "Train Epoch: 7 [57600/83337] Loss: 0.301793 Acc: 0.9375\n",
            "Train Epoch: 7 [60800/83337] Loss: 0.409103 Acc: 0.8750\n",
            "Train Epoch: 7 [64000/83337] Loss: 0.359930 Acc: 0.8750\n",
            "Train Epoch: 7 [67200/83337] Loss: 0.301903 Acc: 0.9375\n",
            "Train Epoch: 7 [70400/83337] Loss: 0.119578 Acc: 0.9688\n",
            "Train Epoch: 7 [73600/83337] Loss: 0.137268 Acc: 0.9688\n",
            "Train Epoch: 7 [76800/83337] Loss: 0.203178 Acc: 0.9688\n",
            "Train Epoch: 7 [80000/83337] Loss: 0.133137 Acc: 0.9375\n",
            "Train Epoch: 7 [83200/83337] Loss: 0.171894 Acc: 0.9688\n",
            "Elapsed 199.66s, 24.96 s/epoch, 0.01 s/batch, ets 299.49s\n",
            "\n",
            "Test set: Average loss: 0.4045, Accuracy: 24380/27712 (88%)\n",
            "\n",
            "Train Epoch: 8 [3200/83337] Loss: 0.219364 Acc: 0.9062\n",
            "Train Epoch: 8 [6400/83337] Loss: 0.348682 Acc: 0.8438\n",
            "Train Epoch: 8 [9600/83337] Loss: 0.188659 Acc: 0.9688\n",
            "Train Epoch: 8 [12800/83337] Loss: 0.508886 Acc: 0.8750\n",
            "Train Epoch: 8 [16000/83337] Loss: 0.315789 Acc: 0.9062\n",
            "Train Epoch: 8 [19200/83337] Loss: 0.120032 Acc: 0.9375\n",
            "Train Epoch: 8 [22400/83337] Loss: 0.553600 Acc: 0.8438\n",
            "Train Epoch: 8 [25600/83337] Loss: 0.358125 Acc: 0.9375\n",
            "Train Epoch: 8 [28800/83337] Loss: 0.213629 Acc: 0.9062\n",
            "Train Epoch: 8 [32000/83337] Loss: 0.094661 Acc: 0.9688\n",
            "Train Epoch: 8 [35200/83337] Loss: 0.314603 Acc: 0.8438\n",
            "Train Epoch: 8 [38400/83337] Loss: 0.439937 Acc: 0.8125\n",
            "Train Epoch: 8 [41600/83337] Loss: 0.173258 Acc: 0.9375\n",
            "Train Epoch: 8 [44800/83337] Loss: 0.211003 Acc: 0.9375\n",
            "Train Epoch: 8 [48000/83337] Loss: 0.183860 Acc: 0.9688\n",
            "Train Epoch: 8 [51200/83337] Loss: 0.356307 Acc: 0.9062\n",
            "Train Epoch: 8 [54400/83337] Loss: 0.271294 Acc: 0.9062\n",
            "Train Epoch: 8 [57600/83337] Loss: 0.514458 Acc: 0.8438\n",
            "Train Epoch: 8 [60800/83337] Loss: 0.978345 Acc: 0.8125\n",
            "Train Epoch: 8 [64000/83337] Loss: 0.238584 Acc: 0.9062\n",
            "Train Epoch: 8 [67200/83337] Loss: 0.190997 Acc: 0.9688\n",
            "Train Epoch: 8 [70400/83337] Loss: 0.272799 Acc: 0.8750\n",
            "Train Epoch: 8 [73600/83337] Loss: 0.339412 Acc: 0.8750\n",
            "Train Epoch: 8 [76800/83337] Loss: 0.307351 Acc: 0.9062\n",
            "Train Epoch: 8 [80000/83337] Loss: 0.123548 Acc: 0.9688\n",
            "Train Epoch: 8 [83200/83337] Loss: 0.237275 Acc: 0.9375\n",
            "Elapsed 224.64s, 24.96 s/epoch, 0.01 s/batch, ets 274.56s\n",
            "\n",
            "Test set: Average loss: 0.3752, Accuracy: 24598/27712 (89%)\n",
            "\n",
            "Train Epoch: 9 [3200/83337] Loss: 0.556585 Acc: 0.9375\n",
            "Train Epoch: 9 [6400/83337] Loss: 0.204702 Acc: 0.9375\n",
            "Train Epoch: 9 [9600/83337] Loss: 0.153081 Acc: 0.9688\n",
            "Train Epoch: 9 [12800/83337] Loss: 0.549064 Acc: 0.8438\n",
            "Train Epoch: 9 [16000/83337] Loss: 0.170642 Acc: 0.9062\n",
            "Train Epoch: 9 [19200/83337] Loss: 0.341770 Acc: 0.9062\n",
            "Train Epoch: 9 [22400/83337] Loss: 0.301034 Acc: 0.8750\n",
            "Train Epoch: 9 [25600/83337] Loss: 0.219690 Acc: 0.9375\n",
            "Train Epoch: 9 [28800/83337] Loss: 0.126943 Acc: 0.9375\n",
            "Train Epoch: 9 [32000/83337] Loss: 0.182348 Acc: 0.9375\n",
            "Train Epoch: 9 [35200/83337] Loss: 0.448889 Acc: 0.9062\n",
            "Train Epoch: 9 [38400/83337] Loss: 0.459258 Acc: 0.8438\n",
            "Train Epoch: 9 [41600/83337] Loss: 0.294536 Acc: 0.9375\n",
            "Train Epoch: 9 [44800/83337] Loss: 0.125248 Acc: 0.9688\n",
            "Train Epoch: 9 [48000/83337] Loss: 0.166552 Acc: 0.9688\n",
            "Train Epoch: 9 [51200/83337] Loss: 0.547641 Acc: 0.9375\n",
            "Train Epoch: 9 [54400/83337] Loss: 0.161887 Acc: 0.9375\n",
            "Train Epoch: 9 [57600/83337] Loss: 0.240268 Acc: 0.9375\n",
            "Train Epoch: 9 [60800/83337] Loss: 0.153564 Acc: 0.9375\n",
            "Train Epoch: 9 [64000/83337] Loss: 0.364595 Acc: 0.9062\n",
            "Train Epoch: 9 [67200/83337] Loss: 0.242057 Acc: 0.9062\n",
            "Train Epoch: 9 [70400/83337] Loss: 0.365818 Acc: 0.9062\n",
            "Train Epoch: 9 [73600/83337] Loss: 0.498349 Acc: 0.8750\n",
            "Train Epoch: 9 [76800/83337] Loss: 0.088512 Acc: 0.9688\n",
            "Train Epoch: 9 [80000/83337] Loss: 0.354830 Acc: 0.8750\n",
            "Train Epoch: 9 [83200/83337] Loss: 0.266742 Acc: 0.8750\n",
            "Elapsed 249.98s, 25.00 s/epoch, 0.01 s/batch, ets 249.98s\n",
            "\n",
            "Test set: Average loss: 0.3479, Accuracy: 24902/27712 (90%)\n",
            "\n",
            "Train Epoch: 10 [3200/83337] Loss: 0.212282 Acc: 0.9062\n",
            "Train Epoch: 10 [6400/83337] Loss: 0.346800 Acc: 0.9062\n",
            "Train Epoch: 10 [9600/83337] Loss: 0.223724 Acc: 0.9062\n",
            "Train Epoch: 10 [12800/83337] Loss: 0.403371 Acc: 0.8750\n",
            "Train Epoch: 10 [16000/83337] Loss: 0.069448 Acc: 1.0000\n",
            "Train Epoch: 10 [19200/83337] Loss: 0.132848 Acc: 0.9688\n",
            "Train Epoch: 10 [22400/83337] Loss: 0.069965 Acc: 0.9688\n",
            "Train Epoch: 10 [25600/83337] Loss: 0.566296 Acc: 0.8750\n",
            "Train Epoch: 10 [28800/83337] Loss: 0.137604 Acc: 0.9375\n",
            "Train Epoch: 10 [32000/83337] Loss: 0.433379 Acc: 0.8438\n",
            "Train Epoch: 10 [35200/83337] Loss: 0.095796 Acc: 1.0000\n",
            "Train Epoch: 10 [38400/83337] Loss: 0.115685 Acc: 0.9688\n",
            "Train Epoch: 10 [41600/83337] Loss: 0.514295 Acc: 0.8750\n",
            "Train Epoch: 10 [44800/83337] Loss: 0.074304 Acc: 0.9688\n",
            "Train Epoch: 10 [48000/83337] Loss: 0.116739 Acc: 0.9688\n",
            "Train Epoch: 10 [51200/83337] Loss: 0.230714 Acc: 0.9375\n",
            "Train Epoch: 10 [54400/83337] Loss: 0.153894 Acc: 0.9688\n",
            "Train Epoch: 10 [57600/83337] Loss: 0.064162 Acc: 0.9688\n",
            "Train Epoch: 10 [60800/83337] Loss: 0.215009 Acc: 0.8750\n",
            "Train Epoch: 10 [64000/83337] Loss: 0.249519 Acc: 0.9062\n",
            "Train Epoch: 10 [67200/83337] Loss: 0.072417 Acc: 1.0000\n",
            "Train Epoch: 10 [70400/83337] Loss: 0.153828 Acc: 0.9375\n",
            "Train Epoch: 10 [73600/83337] Loss: 0.189000 Acc: 0.9062\n",
            "Train Epoch: 10 [76800/83337] Loss: 0.295252 Acc: 0.9062\n",
            "Train Epoch: 10 [80000/83337] Loss: 0.161314 Acc: 0.9375\n",
            "Train Epoch: 10 [83200/83337] Loss: 0.270616 Acc: 0.8750\n",
            "Elapsed 275.20s, 25.02 s/epoch, 0.01 s/batch, ets 225.16s\n",
            "\n",
            "Test set: Average loss: 0.3559, Accuracy: 24900/27712 (90%)\n",
            "\n",
            "Train Epoch: 11 [3200/83337] Loss: 0.199207 Acc: 0.9375\n",
            "Train Epoch: 11 [6400/83337] Loss: 0.471245 Acc: 0.9688\n",
            "Train Epoch: 11 [9600/83337] Loss: 0.136031 Acc: 0.9688\n",
            "Train Epoch: 11 [12800/83337] Loss: 0.100308 Acc: 1.0000\n",
            "Train Epoch: 11 [16000/83337] Loss: 0.152666 Acc: 0.9375\n",
            "Train Epoch: 11 [19200/83337] Loss: 0.677281 Acc: 0.8125\n",
            "Train Epoch: 11 [22400/83337] Loss: 0.162342 Acc: 0.9375\n",
            "Train Epoch: 11 [25600/83337] Loss: 0.212205 Acc: 0.8750\n",
            "Train Epoch: 11 [28800/83337] Loss: 0.230601 Acc: 0.9375\n",
            "Train Epoch: 11 [32000/83337] Loss: 0.254473 Acc: 0.8750\n",
            "Train Epoch: 11 [35200/83337] Loss: 0.218008 Acc: 0.9375\n",
            "Train Epoch: 11 [38400/83337] Loss: 0.227409 Acc: 0.9375\n",
            "Train Epoch: 11 [41600/83337] Loss: 0.181373 Acc: 0.9688\n",
            "Train Epoch: 11 [44800/83337] Loss: 0.146328 Acc: 0.9375\n",
            "Train Epoch: 11 [48000/83337] Loss: 0.250465 Acc: 0.9688\n",
            "Train Epoch: 11 [51200/83337] Loss: 0.249423 Acc: 0.8750\n",
            "Train Epoch: 11 [54400/83337] Loss: 0.575249 Acc: 0.8125\n",
            "Train Epoch: 11 [57600/83337] Loss: 0.292638 Acc: 0.9062\n",
            "Train Epoch: 11 [60800/83337] Loss: 0.272912 Acc: 0.9062\n",
            "Train Epoch: 11 [64000/83337] Loss: 0.291011 Acc: 0.9688\n",
            "Train Epoch: 11 [67200/83337] Loss: 0.180861 Acc: 0.9375\n",
            "Train Epoch: 11 [70400/83337] Loss: 0.298861 Acc: 0.9375\n",
            "Train Epoch: 11 [73600/83337] Loss: 0.065697 Acc: 1.0000\n",
            "Train Epoch: 11 [76800/83337] Loss: 0.297375 Acc: 0.9062\n",
            "Train Epoch: 11 [80000/83337] Loss: 0.136900 Acc: 0.9688\n",
            "Train Epoch: 11 [83200/83337] Loss: 0.185784 Acc: 0.9688\n",
            "Elapsed 300.30s, 25.03 s/epoch, 0.01 s/batch, ets 200.20s\n",
            "\n",
            "Test set: Average loss: 0.3465, Accuracy: 24882/27712 (90%)\n",
            "\n",
            "Train Epoch: 12 [3200/83337] Loss: 0.360679 Acc: 0.8438\n",
            "Train Epoch: 12 [6400/83337] Loss: 0.089810 Acc: 0.9688\n",
            "Train Epoch: 12 [9600/83337] Loss: 0.194343 Acc: 0.9375\n",
            "Train Epoch: 12 [12800/83337] Loss: 0.289364 Acc: 0.9062\n",
            "Train Epoch: 12 [16000/83337] Loss: 0.374378 Acc: 0.9375\n",
            "Train Epoch: 12 [19200/83337] Loss: 0.188329 Acc: 0.9688\n",
            "Train Epoch: 12 [22400/83337] Loss: 0.068032 Acc: 1.0000\n",
            "Train Epoch: 12 [25600/83337] Loss: 0.452726 Acc: 0.8438\n",
            "Train Epoch: 12 [28800/83337] Loss: 0.408154 Acc: 0.8750\n",
            "Train Epoch: 12 [32000/83337] Loss: 0.142617 Acc: 0.9688\n",
            "Train Epoch: 12 [35200/83337] Loss: 0.229621 Acc: 0.9375\n",
            "Train Epoch: 12 [38400/83337] Loss: 0.052969 Acc: 1.0000\n",
            "Train Epoch: 12 [41600/83337] Loss: 0.073503 Acc: 1.0000\n",
            "Train Epoch: 12 [44800/83337] Loss: 0.352697 Acc: 0.9062\n",
            "Train Epoch: 12 [48000/83337] Loss: 0.576376 Acc: 0.8750\n",
            "Train Epoch: 12 [51200/83337] Loss: 0.777294 Acc: 0.9062\n",
            "Train Epoch: 12 [54400/83337] Loss: 0.121016 Acc: 0.9375\n",
            "Train Epoch: 12 [57600/83337] Loss: 0.623234 Acc: 0.9375\n",
            "Train Epoch: 12 [60800/83337] Loss: 0.094513 Acc: 1.0000\n",
            "Train Epoch: 12 [64000/83337] Loss: 0.398736 Acc: 0.8438\n",
            "Train Epoch: 12 [67200/83337] Loss: 0.329633 Acc: 0.8750\n",
            "Train Epoch: 12 [70400/83337] Loss: 0.093777 Acc: 0.9688\n",
            "Train Epoch: 12 [73600/83337] Loss: 0.199375 Acc: 0.9688\n",
            "Train Epoch: 12 [76800/83337] Loss: 0.099990 Acc: 0.9688\n",
            "Train Epoch: 12 [80000/83337] Loss: 0.310340 Acc: 0.9062\n",
            "Train Epoch: 12 [83200/83337] Loss: 0.066531 Acc: 1.0000\n",
            "Elapsed 326.26s, 25.10 s/epoch, 0.01 s/batch, ets 175.68s\n",
            "\n",
            "Test set: Average loss: 0.3373, Accuracy: 24983/27712 (90%)\n",
            "\n",
            "Train Epoch: 13 [3200/83337] Loss: 0.130938 Acc: 0.9688\n",
            "Train Epoch: 13 [6400/83337] Loss: 0.064961 Acc: 0.9688\n",
            "Train Epoch: 13 [9600/83337] Loss: 0.170153 Acc: 0.9062\n",
            "Train Epoch: 13 [12800/83337] Loss: 0.598933 Acc: 0.9062\n",
            "Train Epoch: 13 [16000/83337] Loss: 0.133093 Acc: 0.9375\n",
            "Train Epoch: 13 [19200/83337] Loss: 0.441241 Acc: 0.8750\n",
            "Train Epoch: 13 [22400/83337] Loss: 0.450400 Acc: 0.9688\n",
            "Train Epoch: 13 [25600/83337] Loss: 0.082442 Acc: 0.9688\n",
            "Train Epoch: 13 [28800/83337] Loss: 0.308686 Acc: 0.9375\n",
            "Train Epoch: 13 [32000/83337] Loss: 0.257212 Acc: 0.9375\n",
            "Train Epoch: 13 [35200/83337] Loss: 0.123505 Acc: 0.9688\n",
            "Train Epoch: 13 [38400/83337] Loss: 0.062301 Acc: 1.0000\n",
            "Train Epoch: 13 [41600/83337] Loss: 0.144708 Acc: 0.9688\n",
            "Train Epoch: 13 [44800/83337] Loss: 0.285480 Acc: 0.9688\n",
            "Train Epoch: 13 [48000/83337] Loss: 0.327455 Acc: 0.8750\n",
            "Train Epoch: 13 [51200/83337] Loss: 0.266644 Acc: 0.8438\n",
            "Train Epoch: 13 [54400/83337] Loss: 0.235396 Acc: 0.8438\n",
            "Train Epoch: 13 [57600/83337] Loss: 0.109576 Acc: 0.9688\n",
            "Train Epoch: 13 [60800/83337] Loss: 0.230433 Acc: 0.9062\n",
            "Train Epoch: 13 [64000/83337] Loss: 0.261648 Acc: 0.8750\n",
            "Train Epoch: 13 [67200/83337] Loss: 0.379780 Acc: 0.9062\n",
            "Train Epoch: 13 [70400/83337] Loss: 0.297645 Acc: 0.9062\n",
            "Train Epoch: 13 [73600/83337] Loss: 0.804272 Acc: 0.8438\n",
            "Train Epoch: 13 [76800/83337] Loss: 0.215321 Acc: 0.9688\n",
            "Train Epoch: 13 [80000/83337] Loss: 0.120858 Acc: 0.9688\n",
            "Train Epoch: 13 [83200/83337] Loss: 0.372866 Acc: 0.8438\n",
            "Elapsed 351.10s, 25.08 s/epoch, 0.01 s/batch, ets 150.47s\n",
            "\n",
            "Test set: Average loss: 0.3405, Accuracy: 25014/27712 (90%)\n",
            "\n",
            "Train Epoch: 14 [3200/83337] Loss: 0.228174 Acc: 0.9375\n",
            "Train Epoch: 14 [6400/83337] Loss: 0.163962 Acc: 0.9375\n",
            "Train Epoch: 14 [9600/83337] Loss: 0.128567 Acc: 0.9688\n",
            "Train Epoch: 14 [12800/83337] Loss: 0.074391 Acc: 1.0000\n",
            "Train Epoch: 14 [16000/83337] Loss: 0.502006 Acc: 0.9062\n",
            "Train Epoch: 14 [19200/83337] Loss: 0.078699 Acc: 0.9688\n",
            "Train Epoch: 14 [22400/83337] Loss: 0.354195 Acc: 0.8750\n",
            "Train Epoch: 14 [25600/83337] Loss: 0.142950 Acc: 0.9688\n",
            "Train Epoch: 14 [28800/83337] Loss: 0.159662 Acc: 0.9375\n",
            "Train Epoch: 14 [32000/83337] Loss: 0.419626 Acc: 0.8438\n",
            "Train Epoch: 14 [35200/83337] Loss: 0.134500 Acc: 0.9375\n",
            "Train Epoch: 14 [38400/83337] Loss: 0.078340 Acc: 1.0000\n",
            "Train Epoch: 14 [41600/83337] Loss: 0.259475 Acc: 0.8750\n",
            "Train Epoch: 14 [44800/83337] Loss: 0.333692 Acc: 0.9375\n",
            "Train Epoch: 14 [48000/83337] Loss: 0.156989 Acc: 0.9688\n",
            "Train Epoch: 14 [51200/83337] Loss: 0.069881 Acc: 0.9688\n",
            "Train Epoch: 14 [54400/83337] Loss: 0.228865 Acc: 0.9375\n",
            "Train Epoch: 14 [57600/83337] Loss: 0.261837 Acc: 0.8750\n",
            "Train Epoch: 14 [60800/83337] Loss: 0.209261 Acc: 0.9062\n",
            "Train Epoch: 14 [64000/83337] Loss: 0.343090 Acc: 0.9375\n",
            "Train Epoch: 14 [67200/83337] Loss: 0.223094 Acc: 0.9375\n",
            "Train Epoch: 14 [70400/83337] Loss: 0.126176 Acc: 0.9375\n",
            "Train Epoch: 14 [73600/83337] Loss: 0.142282 Acc: 0.9375\n",
            "Train Epoch: 14 [76800/83337] Loss: 0.418963 Acc: 0.9062\n",
            "Train Epoch: 14 [80000/83337] Loss: 0.096734 Acc: 1.0000\n",
            "Train Epoch: 14 [83200/83337] Loss: 0.104986 Acc: 0.9688\n",
            "Elapsed 376.98s, 25.13 s/epoch, 0.01 s/batch, ets 125.66s\n",
            "\n",
            "Test set: Average loss: 0.3342, Accuracy: 25098/27712 (91%)\n",
            "\n",
            "Train Epoch: 15 [3200/83337] Loss: 0.251234 Acc: 0.9688\n",
            "Train Epoch: 15 [6400/83337] Loss: 0.427485 Acc: 0.8750\n",
            "Train Epoch: 15 [9600/83337] Loss: 0.223107 Acc: 0.9375\n",
            "Train Epoch: 15 [12800/83337] Loss: 0.088483 Acc: 0.9375\n",
            "Train Epoch: 15 [16000/83337] Loss: 0.104566 Acc: 0.9375\n",
            "Train Epoch: 15 [19200/83337] Loss: 0.081498 Acc: 0.9688\n",
            "Train Epoch: 15 [22400/83337] Loss: 0.213146 Acc: 0.9062\n",
            "Train Epoch: 15 [25600/83337] Loss: 0.196771 Acc: 0.9062\n",
            "Train Epoch: 15 [28800/83337] Loss: 0.289686 Acc: 0.9375\n",
            "Train Epoch: 15 [32000/83337] Loss: 0.144795 Acc: 0.9375\n",
            "Train Epoch: 15 [35200/83337] Loss: 0.096051 Acc: 1.0000\n",
            "Train Epoch: 15 [38400/83337] Loss: 0.105556 Acc: 0.9688\n",
            "Train Epoch: 15 [41600/83337] Loss: 0.142523 Acc: 0.9688\n",
            "Train Epoch: 15 [44800/83337] Loss: 0.342639 Acc: 0.9062\n",
            "Train Epoch: 15 [48000/83337] Loss: 0.088385 Acc: 0.9688\n",
            "Train Epoch: 15 [51200/83337] Loss: 0.187055 Acc: 0.9375\n",
            "Train Epoch: 15 [54400/83337] Loss: 0.046202 Acc: 0.9688\n",
            "Train Epoch: 15 [57600/83337] Loss: 0.135184 Acc: 0.9375\n",
            "Train Epoch: 15 [60800/83337] Loss: 0.325256 Acc: 0.9062\n",
            "Train Epoch: 15 [64000/83337] Loss: 0.233713 Acc: 0.9688\n",
            "Train Epoch: 15 [67200/83337] Loss: 0.385105 Acc: 0.9688\n",
            "Train Epoch: 15 [70400/83337] Loss: 0.053974 Acc: 1.0000\n",
            "Train Epoch: 15 [73600/83337] Loss: 0.235710 Acc: 0.9062\n",
            "Train Epoch: 15 [76800/83337] Loss: 0.184381 Acc: 0.9375\n",
            "Train Epoch: 15 [80000/83337] Loss: 0.025762 Acc: 1.0000\n",
            "Train Epoch: 15 [83200/83337] Loss: 0.317379 Acc: 0.8438\n",
            "Elapsed 401.75s, 25.11 s/epoch, 0.01 s/batch, ets 100.44s\n",
            "\n",
            "Test set: Average loss: 0.3363, Accuracy: 25112/27712 (91%)\n",
            "\n",
            "Train Epoch: 16 [3200/83337] Loss: 0.237145 Acc: 0.9688\n",
            "Train Epoch: 16 [6400/83337] Loss: 0.434769 Acc: 0.9375\n",
            "Train Epoch: 16 [9600/83337] Loss: 0.056050 Acc: 0.9688\n",
            "Train Epoch: 16 [12800/83337] Loss: 0.302623 Acc: 0.9375\n",
            "Train Epoch: 16 [16000/83337] Loss: 0.154209 Acc: 0.9062\n",
            "Train Epoch: 16 [19200/83337] Loss: 0.104926 Acc: 0.9688\n",
            "Train Epoch: 16 [22400/83337] Loss: 0.092265 Acc: 0.9688\n",
            "Train Epoch: 16 [25600/83337] Loss: 0.333594 Acc: 0.9375\n",
            "Train Epoch: 16 [28800/83337] Loss: 0.227903 Acc: 0.9375\n",
            "Train Epoch: 16 [32000/83337] Loss: 0.458679 Acc: 0.8750\n",
            "Train Epoch: 16 [35200/83337] Loss: 0.250159 Acc: 0.9062\n",
            "Train Epoch: 16 [38400/83337] Loss: 0.279396 Acc: 0.8750\n",
            "Train Epoch: 16 [41600/83337] Loss: 0.142795 Acc: 0.9375\n",
            "Train Epoch: 16 [44800/83337] Loss: 0.102092 Acc: 0.9688\n",
            "Train Epoch: 16 [48000/83337] Loss: 0.176815 Acc: 0.9375\n",
            "Train Epoch: 16 [51200/83337] Loss: 0.161686 Acc: 0.9688\n",
            "Train Epoch: 16 [54400/83337] Loss: 0.366923 Acc: 0.9062\n",
            "Train Epoch: 16 [57600/83337] Loss: 0.131286 Acc: 0.9375\n",
            "Train Epoch: 16 [60800/83337] Loss: 0.118172 Acc: 0.9688\n",
            "Train Epoch: 16 [64000/83337] Loss: 0.176594 Acc: 0.9062\n",
            "Train Epoch: 16 [67200/83337] Loss: 0.149825 Acc: 0.9375\n",
            "Train Epoch: 16 [70400/83337] Loss: 0.150097 Acc: 0.9688\n",
            "Train Epoch: 16 [73600/83337] Loss: 0.168759 Acc: 0.9375\n",
            "Train Epoch: 16 [76800/83337] Loss: 0.175259 Acc: 0.9688\n",
            "Train Epoch: 16 [80000/83337] Loss: 0.200986 Acc: 0.8750\n",
            "Train Epoch: 16 [83200/83337] Loss: 0.305697 Acc: 0.9062\n",
            "Elapsed 427.03s, 25.12 s/epoch, 0.01 s/batch, ets 75.36s\n",
            "\n",
            "Test set: Average loss: 0.3279, Accuracy: 25166/27712 (91%)\n",
            "\n",
            "Train Epoch: 17 [3200/83337] Loss: 0.147281 Acc: 0.9375\n",
            "Train Epoch: 17 [6400/83337] Loss: 0.083766 Acc: 0.9688\n",
            "Train Epoch: 17 [9600/83337] Loss: 0.187094 Acc: 0.9688\n",
            "Train Epoch: 17 [12800/83337] Loss: 0.605688 Acc: 0.9062\n",
            "Train Epoch: 17 [16000/83337] Loss: 0.237405 Acc: 0.9062\n",
            "Train Epoch: 17 [19200/83337] Loss: 0.137181 Acc: 0.9375\n",
            "Train Epoch: 17 [22400/83337] Loss: 0.213722 Acc: 0.9062\n",
            "Train Epoch: 17 [25600/83337] Loss: 0.077817 Acc: 0.9688\n",
            "Train Epoch: 17 [28800/83337] Loss: 0.260295 Acc: 0.8750\n",
            "Train Epoch: 17 [32000/83337] Loss: 0.131028 Acc: 0.9688\n",
            "Train Epoch: 17 [35200/83337] Loss: 0.577089 Acc: 0.9062\n",
            "Train Epoch: 17 [38400/83337] Loss: 0.248060 Acc: 0.9375\n",
            "Train Epoch: 17 [41600/83337] Loss: 0.051593 Acc: 1.0000\n",
            "Train Epoch: 17 [44800/83337] Loss: 0.131501 Acc: 0.9688\n",
            "Train Epoch: 17 [48000/83337] Loss: 0.193719 Acc: 0.9375\n",
            "Train Epoch: 17 [51200/83337] Loss: 0.084862 Acc: 1.0000\n",
            "Train Epoch: 17 [54400/83337] Loss: 0.222433 Acc: 0.9062\n",
            "Train Epoch: 17 [57600/83337] Loss: 0.180509 Acc: 0.9375\n",
            "Train Epoch: 17 [60800/83337] Loss: 0.223031 Acc: 0.9688\n",
            "Train Epoch: 17 [64000/83337] Loss: 0.365041 Acc: 0.9062\n",
            "Train Epoch: 17 [67200/83337] Loss: 0.459231 Acc: 0.9062\n",
            "Train Epoch: 17 [70400/83337] Loss: 0.475809 Acc: 0.9375\n",
            "Train Epoch: 17 [73600/83337] Loss: 0.088083 Acc: 0.9688\n",
            "Train Epoch: 17 [76800/83337] Loss: 0.340571 Acc: 0.9062\n",
            "Train Epoch: 17 [80000/83337] Loss: 0.105667 Acc: 0.9688\n",
            "Train Epoch: 17 [83200/83337] Loss: 0.173028 Acc: 0.9688\n",
            "Elapsed 451.89s, 25.11 s/epoch, 0.01 s/batch, ets 50.21s\n",
            "\n",
            "Test set: Average loss: 0.3283, Accuracy: 25126/27712 (91%)\n",
            "\n",
            "Train Epoch: 18 [3200/83337] Loss: 0.314687 Acc: 0.9062\n",
            "Train Epoch: 18 [6400/83337] Loss: 0.122412 Acc: 0.9688\n",
            "Train Epoch: 18 [9600/83337] Loss: 0.134462 Acc: 0.9688\n",
            "Train Epoch: 18 [12800/83337] Loss: 0.102729 Acc: 1.0000\n",
            "Train Epoch: 18 [16000/83337] Loss: 0.125997 Acc: 0.9375\n",
            "Train Epoch: 18 [19200/83337] Loss: 0.289940 Acc: 0.8750\n",
            "Train Epoch: 18 [22400/83337] Loss: 0.348216 Acc: 0.9375\n",
            "Train Epoch: 18 [25600/83337] Loss: 0.277365 Acc: 0.9062\n",
            "Train Epoch: 18 [28800/83337] Loss: 0.197779 Acc: 0.9375\n",
            "Train Epoch: 18 [32000/83337] Loss: 0.138110 Acc: 0.9688\n",
            "Train Epoch: 18 [35200/83337] Loss: 0.286964 Acc: 0.9375\n",
            "Train Epoch: 18 [38400/83337] Loss: 0.258957 Acc: 0.8750\n",
            "Train Epoch: 18 [41600/83337] Loss: 0.090191 Acc: 0.9688\n",
            "Train Epoch: 18 [44800/83337] Loss: 0.237567 Acc: 0.9062\n",
            "Train Epoch: 18 [48000/83337] Loss: 0.174408 Acc: 0.9688\n",
            "Train Epoch: 18 [51200/83337] Loss: 0.194952 Acc: 0.9688\n",
            "Train Epoch: 18 [54400/83337] Loss: 0.080813 Acc: 1.0000\n",
            "Train Epoch: 18 [57600/83337] Loss: 0.156950 Acc: 0.9375\n",
            "Train Epoch: 18 [60800/83337] Loss: 0.005366 Acc: 1.0000\n",
            "Train Epoch: 18 [64000/83337] Loss: 0.291108 Acc: 0.9062\n",
            "Train Epoch: 18 [67200/83337] Loss: 0.207017 Acc: 0.9375\n",
            "Train Epoch: 18 [70400/83337] Loss: 0.260927 Acc: 0.9375\n",
            "Train Epoch: 18 [73600/83337] Loss: 0.161502 Acc: 0.9688\n",
            "Train Epoch: 18 [76800/83337] Loss: 0.283334 Acc: 0.9062\n",
            "Train Epoch: 18 [80000/83337] Loss: 0.034227 Acc: 1.0000\n",
            "Train Epoch: 18 [83200/83337] Loss: 0.228341 Acc: 0.9375\n",
            "Elapsed 476.80s, 25.09 s/epoch, 0.01 s/batch, ets 25.09s\n",
            "\n",
            "Test set: Average loss: 0.3393, Accuracy: 25097/27712 (91%)\n",
            "\n",
            "Train Epoch: 19 [3200/83337] Loss: 0.097929 Acc: 1.0000\n",
            "Train Epoch: 19 [6400/83337] Loss: 0.118858 Acc: 0.9688\n",
            "Train Epoch: 19 [9600/83337] Loss: 0.235465 Acc: 0.9375\n",
            "Train Epoch: 19 [12800/83337] Loss: 0.223224 Acc: 0.9062\n",
            "Train Epoch: 19 [16000/83337] Loss: 0.283194 Acc: 0.9375\n",
            "Train Epoch: 19 [19200/83337] Loss: 0.047924 Acc: 1.0000\n",
            "Train Epoch: 19 [22400/83337] Loss: 0.082146 Acc: 0.9688\n",
            "Train Epoch: 19 [25600/83337] Loss: 0.280446 Acc: 0.9062\n",
            "Train Epoch: 19 [28800/83337] Loss: 0.077672 Acc: 1.0000\n",
            "Train Epoch: 19 [32000/83337] Loss: 0.164812 Acc: 0.9375\n",
            "Train Epoch: 19 [35200/83337] Loss: 0.326713 Acc: 0.9062\n",
            "Train Epoch: 19 [38400/83337] Loss: 0.223702 Acc: 0.9375\n",
            "Train Epoch: 19 [41600/83337] Loss: 0.268160 Acc: 0.9375\n",
            "Train Epoch: 19 [44800/83337] Loss: 0.135972 Acc: 0.9688\n",
            "Train Epoch: 19 [48000/83337] Loss: 0.087405 Acc: 1.0000\n",
            "Train Epoch: 19 [51200/83337] Loss: 0.185261 Acc: 0.9375\n",
            "Train Epoch: 19 [54400/83337] Loss: 0.212329 Acc: 0.8750\n",
            "Train Epoch: 19 [57600/83337] Loss: 0.118714 Acc: 0.9375\n",
            "Train Epoch: 19 [60800/83337] Loss: 0.073088 Acc: 0.9688\n",
            "Train Epoch: 19 [64000/83337] Loss: 0.215537 Acc: 0.9062\n",
            "Train Epoch: 19 [67200/83337] Loss: 0.277507 Acc: 0.9688\n",
            "Train Epoch: 19 [70400/83337] Loss: 0.141861 Acc: 0.9688\n",
            "Train Epoch: 19 [73600/83337] Loss: 0.117581 Acc: 0.9688\n",
            "Train Epoch: 19 [76800/83337] Loss: 0.043639 Acc: 1.0000\n",
            "Train Epoch: 19 [80000/83337] Loss: 0.177954 Acc: 0.9062\n",
            "Train Epoch: 19 [83200/83337] Loss: 0.230554 Acc: 0.9375\n",
            "Elapsed 502.21s, 25.11 s/epoch, 0.01 s/batch, ets 0.00s\n",
            "\n",
            "Test set: Average loss: 0.3346, Accuracy: 25197/27712 (91%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUMHs1Revk4O",
        "colab_type": "code",
        "outputId": "8e9a1487-4e89-4bb5-ddb3-933cebd5f32a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"Total time: {:.2f}, Best Loss: {:.3f}\".format(time.time() - t_begin, best_loss))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total time: 508.06, Best Loss: 0.328\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeeFfy2I99GO",
        "colab_type": "code",
        "outputId": "39223b44-304c-4ce7-f2ec-7e24ee7e22cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        }
      },
      "source": [
        "# Plot loss\n",
        "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
        "x = range(len(epoch_train_loss))\n",
        "\n",
        "plt.plot(x, epoch_train_loss, 'r', label=\"train loss\")\n",
        "plt.plot(x, epoch_test_loss, 'b',label=\"validation loss\")\n",
        "\n",
        "plt.xlabel('epoch no.')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmEAAAGDCAYAAABjkcdfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtAuFBvK99I9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#save model with best loss\n",
        "torch.save(best_model.state_dict(), '../content/gdrive/My Drive/models/SVHM_aug_model.pth')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TXaeqk499Kj",
        "colab_type": "code",
        "outputId": "7fbbb41d-e6a5-41b3-b3dd-846444ab58a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "#download and test model\n",
        "model = LeNetBN()\n",
        "model.load_state_dict(torch.load('../content/gdrive/My Drive/models/SVHM_aug_model.pth'))\n",
        "#print(model1)\n",
        "images, labels = next(iter(test_loader_SVHM))\n",
        "plt.imshow(images[0][0],'gray')\n",
        "image = images[0,:]#.to(device)\n",
        "image = image[None]\n",
        "plt.show()\n",
        "score = model(image)\n",
        "prob = nn.functional.softmax(score[0], dim=0)\n",
        "y_pred =  prob.argmax()\n",
        "print(\"Predicted class {} with probability {}\".format(y_pred, prob[y_pred]))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWgAAAFlCAYAAADGe3ILAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nyg8GwB4oAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Predicted class 5 with probability 0.9999010562896729\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHZWLiNkUHjk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "test = pd.read_csv(\"../content/gdrive/My Drive/MNIST/test.csv\")\n",
        "test_image = test.loc[:,test.columns != \"label\"]\n",
        "test_dataset = torch.from_numpy(np.reshape(test_image.to_numpy().astype(np.uint8), (test_image.shape[0], 1, 28,28)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyFHcxJyqDLR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results = []\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    for img in test_dataset:\n",
        "        img = transforms.ToPILImage()(img)\n",
        "        img = transforms.Resize((32, 32))(img)\n",
        "        img = transforms.ToTensor()(img)\n",
        "        img = transforms.Normalize((0.3977, ), (0.2329, ))(img)\n",
        "        test_im = img#.to(device)\n",
        "        test_im = test_im[None]\n",
        "        output = model(test_im)\n",
        "        prob = nn.functional.softmax(output[0], dim=0)\n",
        "        y_pred =  prob.argmax()\n",
        "        results.append( y_pred.cpu().data.numpy().tolist())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIaobUUtqPEC",
        "colab_type": "code",
        "outputId": "9038a0ca-3f98-40b2-dd5a-6961d6df8779",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(results)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cHoRVscqRDl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = np.array(results).flatten()\n",
        "submissions=pd.DataFrame({\"ImageId\": list(range(1,len(predictions)+1)),\n",
        "                         \"Label\": predictions})\n",
        "submissions.to_csv(\"../content/gdrive/My Drive/MNIST/my_submissions07.csv\", index=False, header=True)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}