{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TrainDigitRecognizer_SVHM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_fPhAC9u2FE",
        "colab_type": "code",
        "outputId": "d0a77227-d861-49ae-8e00-b0b1af162f96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#use Google Coolab with free GPU to train model\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWQfJ0MPvGXJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import time\n",
        "import copy\n",
        "\n",
        "from typing import Iterable\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MIBzDDPvI80",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#LeNet with Batch Normalization\n",
        "class LeNetBN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # convolution layers\n",
        "        self._body = nn.Sequential(\n",
        "            # First convolution Layer\n",
        "            # input size = (32, 32), output size = (28, 28)\n",
        "            nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5),\n",
        "            nn.BatchNorm2d(6),\n",
        "            # ReLU activation\n",
        "            nn.ReLU(inplace=True),\n",
        "            # Max pool 2-d\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            \n",
        "            # Second convolution layer\n",
        "            # input size = (14, 14), output size = (10, 10)\n",
        "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            # output size = (5, 5)\n",
        "        )\n",
        "        \n",
        "        # Fully connected layers\n",
        "        self._head = nn.Sequential(\n",
        "            # First fully connected layer\n",
        "            # in_features = total number of weight in last conv layer = 16 * 5 * 5\n",
        "            nn.Linear(in_features=16 * 5 * 5, out_features=120), \n",
        "            \n",
        "            # ReLU activation\n",
        "            nn.ReLU(inplace=True),\n",
        "            \n",
        "            # second fully connected layer\n",
        "            # in_features = output of last linear layer = 120 \n",
        "            nn.Linear(in_features=120, out_features=84), \n",
        "            \n",
        "            # ReLU activation\n",
        "            nn.ReLU(inplace=True),\n",
        "            \n",
        "            # Third fully connected layer. It is also output layer\n",
        "            # in_features = output of last linear layer = 84\n",
        "            # and out_features = number of classes = 10 (0-9)\n",
        "            nn.Linear(in_features=84, out_features=10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # apply feature extractor\n",
        "        x = self._body(x)\n",
        "        # flatten the output of conv layers\n",
        "        # dimension should be batch_size * number_of weight_in_last conv_layer\n",
        "        x = x.view(x.size()[0], -1)\n",
        "        # apply classification head\n",
        "        x = self._head(x)\n",
        "        return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0skH0fkYQSsj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "d50152c7-4b16-4fe0-f83b-5831736b61ef"
      },
      "source": [
        "model = LeNetBN()\n",
        "print(model)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LeNetBN(\n",
            "  (_body): Sequential(\n",
            "    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (_head): Sequential(\n",
            "    (0): Linear(in_features=400, out_features=120, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Linear(in_features=120, out_features=84, bias=True)\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): Linear(in_features=84, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRAT2WWsvWQz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_test_transforms_SVHM = transforms.Compose([\n",
        "    #convert to grayscale                                             \n",
        "    transforms.Grayscale(1),\n",
        "    #re-scale image tensor values between 0-1. image_tensor /= 255\n",
        "    transforms.ToTensor(),\n",
        "    # subtract mean (0.4514) and divide by variance (0.1993).\n",
        "    # This mean and variance is calculated on training data \n",
        "    transforms.Normalize((0.4514, ), (0.1993, ))\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Jzggwr8TOwo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#download data from Google Drive\n",
        "trainSVHM = datasets.SVHN('../content/gdrive/My Drive/SVHN', split='train', download=False, transform=train_test_transforms_SVHM)\n",
        "testSVHM = datasets.SVHN('../content/gdrive/My Drive/SVHN', split='test', download=False, transform=train_test_transforms_SVHM)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kk2gufzwQfgs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.manual_seed(42)\n",
        "#if torch.cuda.is_available():\n",
        "#    torch.backend.cudnn_benchmark_enabled = True\n",
        "#    torch.backend.cudnn.deterministic = True\n",
        "\n",
        "#training parameters\n",
        "batch_size = 32\n",
        "epochs_count = 20\n",
        "learning_rate = 0.01\n",
        "log_interval = 100\n",
        "test_interval = 1\n",
        "num_workers  = 10\n",
        "device = 'cuda'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhG-KqsEviSQ",
        "colab_type": "code",
        "outputId": "8cf3a5c4-457f-4ddf-fd3e-548bd9d903c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "    print('GPU')\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "    batch_size = 16\n",
        "    num_workers = 0\n",
        "    epochs_count = 10\n",
        "    print('CPU')\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "#SGDoptimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "#array for plotting\n",
        "best_loss = torch.tensor(np.inf)\n",
        "\n",
        "# epoch train/test loss\n",
        "epoch_train_loss = np.array([])\n",
        "epoch_test_loss = np.array([])\n",
        "\n",
        "# epch train/test accuracy\n",
        "epoch_train_acc = np.array([])\n",
        "epoch_test_acc = np.array([])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGK49aIQSgb4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train dataloader\n",
        "train_loader_SVHM = torch.utils.data.DataLoader(\n",
        "     trainSVHM,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=num_workers\n",
        ")\n",
        "# test dataloader\n",
        "test_loader_SVHM = torch.utils.data.DataLoader(\n",
        "    testSVHM,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=num_workers\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VE6H7m-svksn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, optimizer, train_loader, epoch_idx):\n",
        "    # change model in training mood\n",
        "    model.train()\n",
        "\n",
        "    # to get batch loss\n",
        "    batch_loss = np.array([])\n",
        "\n",
        "    # to get batch accuracy\n",
        "    batch_acc = np.array([])\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "\n",
        "        # clone target\n",
        "        indx_target = target.clone()\n",
        "        # send data to device (its is medatory if GPU has to be used)\n",
        "        data = data.to(device)\n",
        "        # send target to device\n",
        "        target = target.to(device)\n",
        "\n",
        "        # reset parameters gradient to zero\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward pass to the model\n",
        "        output = model(data)\n",
        "\n",
        "        # cross entropy loss\n",
        "        loss = F.cross_entropy(output, target)\n",
        "\n",
        "        # find gradients w.r.t training parameters\n",
        "        loss.backward()\n",
        "        # Update parameters using gardients\n",
        "        optimizer.step()\n",
        "\n",
        "        batch_loss = np.append(batch_loss, [loss.item()])\n",
        "\n",
        "        # Score to probability using softmax\n",
        "        prob = F.softmax(output, dim=1)\n",
        "\n",
        "        # get the index of the max probability\n",
        "        pred = prob.data.max(dim=1)[1]\n",
        "\n",
        "        # correct prediction\n",
        "        correct = pred.cpu().eq(indx_target).sum()\n",
        "\n",
        "        # accuracy\n",
        "        acc = float(correct) / float(len(data))\n",
        "\n",
        "        batch_acc = np.append(batch_acc, [acc])\n",
        "\n",
        "        if batch_idx % log_interval == 0 and batch_idx > 0:\n",
        "            print(\n",
        "                'Train Epoch: {} [{}/{}] Loss: {:.6f} Acc: {:.4f}'.format(\n",
        "                    epoch_idx, batch_idx * len(data), len(train_loader.dataset), loss.item(), acc\n",
        "                )\n",
        "            )\n",
        "\n",
        "    epoch_loss = batch_loss.mean()\n",
        "    epoch_acc = batch_acc.mean()\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "\n",
        "def validate(model, test_loader):\n",
        "    #change to validate mode\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    count_corect_predictions = 0\n",
        "    for data, target in test_loader:\n",
        "        indx_target = target.clone()\n",
        "        data = data.to(device)\n",
        "\n",
        "        target = target.to(device)\n",
        "\n",
        "        output = model(data)\n",
        "        # add loss for each mini batch\n",
        "        test_loss += F.cross_entropy(output, target).item()\n",
        "\n",
        "        # Score to probability using softmax\n",
        "        prob = F.softmax(output, dim=1)\n",
        "\n",
        "        # get the index of the max probability\n",
        "        pred = prob.data.max(dim=1)[1]\n",
        "\n",
        "        # add correct prediction count\n",
        "        count_corect_predictions += pred.cpu().eq(indx_target).sum()\n",
        "\n",
        "    # average over number of mini-batches\n",
        "    test_loss = test_loss / len(test_loader)\n",
        "\n",
        "    # average over number of dataset\n",
        "    accuracy = 100. * count_corect_predictions / len(test_loader.dataset)\n",
        "\n",
        "    print(\n",
        "        '\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "            test_loss, count_corect_predictions, len(test_loader.dataset), accuracy\n",
        "        )\n",
        "    )\n",
        "    return test_loss, accuracy / 100.0\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjythJS-vkv_",
        "colab_type": "code",
        "outputId": "10d09eee-3d21-42c8-f13e-5258816eda82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#train\n",
        "t_begin = time.time()\n",
        "#for save best model\n",
        "best_model = copy.deepcopy(model)\n",
        "for epoch in range(epochs_count):\n",
        "\n",
        "    train_loss, train_acc = train(model, optimizer, train_loader_SVHM, epoch)\n",
        "\n",
        "    epoch_train_loss = np.append(epoch_train_loss, [train_loss])\n",
        "\n",
        "    epoch_train_acc = np.append(epoch_train_acc, [train_acc])\n",
        "\n",
        "    elapsed_time = time.time() - t_begin\n",
        "    speed_epoch = elapsed_time / (epoch + 1)\n",
        "    speed_batch = speed_epoch / len(train_loader_SVHM)\n",
        "    eta = speed_epoch * epochs_count - elapsed_time\n",
        "\n",
        "    print(\n",
        "        \"Elapsed {:.2f}s, {:.2f} s/epoch, {:.2f} s/batch, ets {:.2f}s\".format(\n",
        "            elapsed_time, speed_epoch, speed_batch, eta\n",
        "        )\n",
        "    )\n",
        "\n",
        "    if epoch % test_interval == 0:\n",
        "        current_loss, current_accuracy = validate(model, test_loader_SVHM)\n",
        "\n",
        "        epoch_test_loss = np.append(epoch_test_loss, [current_loss])\n",
        "\n",
        "        epoch_test_acc = np.append(epoch_test_acc, [current_accuracy])\n",
        "\n",
        "        if current_loss < best_loss:\n",
        "            best_loss = current_loss\n",
        "            best_model = copy.deepcopy(model)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [3200/73257] Loss: 2.275573 Acc: 0.1875\n",
            "Train Epoch: 0 [6400/73257] Loss: 2.075103 Acc: 0.2500\n",
            "Train Epoch: 0 [9600/73257] Loss: 2.201768 Acc: 0.1875\n",
            "Train Epoch: 0 [12800/73257] Loss: 1.708190 Acc: 0.4062\n",
            "Train Epoch: 0 [16000/73257] Loss: 1.816186 Acc: 0.4062\n",
            "Train Epoch: 0 [19200/73257] Loss: 1.691095 Acc: 0.4375\n",
            "Train Epoch: 0 [22400/73257] Loss: 1.404497 Acc: 0.5938\n",
            "Train Epoch: 0 [25600/73257] Loss: 1.206809 Acc: 0.5312\n",
            "Train Epoch: 0 [28800/73257] Loss: 1.011987 Acc: 0.7500\n",
            "Train Epoch: 0 [32000/73257] Loss: 1.185546 Acc: 0.6250\n",
            "Train Epoch: 0 [35200/73257] Loss: 1.167590 Acc: 0.5625\n",
            "Train Epoch: 0 [38400/73257] Loss: 0.855498 Acc: 0.6875\n",
            "Train Epoch: 0 [41600/73257] Loss: 0.957826 Acc: 0.7812\n",
            "Train Epoch: 0 [44800/73257] Loss: 0.719213 Acc: 0.7188\n",
            "Train Epoch: 0 [48000/73257] Loss: 0.678024 Acc: 0.7812\n",
            "Train Epoch: 0 [51200/73257] Loss: 1.049357 Acc: 0.6562\n",
            "Train Epoch: 0 [54400/73257] Loss: 0.551264 Acc: 0.8125\n",
            "Train Epoch: 0 [57600/73257] Loss: 0.760486 Acc: 0.8438\n",
            "Train Epoch: 0 [60800/73257] Loss: 0.559004 Acc: 0.8438\n",
            "Train Epoch: 0 [64000/73257] Loss: 0.766487 Acc: 0.8438\n",
            "Train Epoch: 0 [67200/73257] Loss: 0.648456 Acc: 0.8125\n",
            "Train Epoch: 0 [70400/73257] Loss: 0.538981 Acc: 0.7812\n",
            "Elapsed 18.20s, 18.20 s/epoch, 0.01 s/batch, ets 345.74s\n",
            "\n",
            "Test set: Average loss: 0.7286, Accuracy: 20425/26032 (78%)\n",
            "\n",
            "Train Epoch: 1 [3200/73257] Loss: 1.013147 Acc: 0.6562\n",
            "Train Epoch: 1 [6400/73257] Loss: 0.330050 Acc: 0.9375\n",
            "Train Epoch: 1 [9600/73257] Loss: 0.397264 Acc: 0.9062\n",
            "Train Epoch: 1 [12800/73257] Loss: 0.479295 Acc: 0.8125\n",
            "Train Epoch: 1 [16000/73257] Loss: 0.566027 Acc: 0.7812\n",
            "Train Epoch: 1 [19200/73257] Loss: 0.572129 Acc: 0.7812\n",
            "Train Epoch: 1 [22400/73257] Loss: 0.489640 Acc: 0.8125\n",
            "Train Epoch: 1 [25600/73257] Loss: 0.427275 Acc: 0.8125\n",
            "Train Epoch: 1 [28800/73257] Loss: 0.667501 Acc: 0.7500\n",
            "Train Epoch: 1 [32000/73257] Loss: 0.878477 Acc: 0.7188\n",
            "Train Epoch: 1 [35200/73257] Loss: 0.243405 Acc: 0.9688\n",
            "Train Epoch: 1 [38400/73257] Loss: 0.558546 Acc: 0.8438\n",
            "Train Epoch: 1 [41600/73257] Loss: 0.704300 Acc: 0.7812\n",
            "Train Epoch: 1 [44800/73257] Loss: 0.507249 Acc: 0.9062\n",
            "Train Epoch: 1 [48000/73257] Loss: 0.374892 Acc: 0.8438\n",
            "Train Epoch: 1 [51200/73257] Loss: 0.660182 Acc: 0.8125\n",
            "Train Epoch: 1 [54400/73257] Loss: 0.517515 Acc: 0.8125\n",
            "Train Epoch: 1 [57600/73257] Loss: 0.422813 Acc: 0.8750\n",
            "Train Epoch: 1 [60800/73257] Loss: 0.527134 Acc: 0.8125\n",
            "Train Epoch: 1 [64000/73257] Loss: 0.462104 Acc: 0.8750\n",
            "Train Epoch: 1 [67200/73257] Loss: 0.245600 Acc: 0.9062\n",
            "Train Epoch: 1 [70400/73257] Loss: 0.366294 Acc: 0.9375\n",
            "Elapsed 41.61s, 20.80 s/epoch, 0.01 s/batch, ets 374.46s\n",
            "\n",
            "Test set: Average loss: 0.7057, Accuracy: 20469/26032 (79%)\n",
            "\n",
            "Train Epoch: 2 [3200/73257] Loss: 0.573339 Acc: 0.7812\n",
            "Train Epoch: 2 [6400/73257] Loss: 0.541385 Acc: 0.8125\n",
            "Train Epoch: 2 [9600/73257] Loss: 0.808619 Acc: 0.9062\n",
            "Train Epoch: 2 [12800/73257] Loss: 0.469903 Acc: 0.8438\n",
            "Train Epoch: 2 [16000/73257] Loss: 0.722284 Acc: 0.8125\n",
            "Train Epoch: 2 [19200/73257] Loss: 0.744104 Acc: 0.7188\n",
            "Train Epoch: 2 [22400/73257] Loss: 0.445767 Acc: 0.8750\n",
            "Train Epoch: 2 [25600/73257] Loss: 0.757421 Acc: 0.8438\n",
            "Train Epoch: 2 [28800/73257] Loss: 0.545550 Acc: 0.8125\n",
            "Train Epoch: 2 [32000/73257] Loss: 0.462002 Acc: 0.8125\n",
            "Train Epoch: 2 [35200/73257] Loss: 0.288384 Acc: 0.9062\n",
            "Train Epoch: 2 [38400/73257] Loss: 0.561822 Acc: 0.8750\n",
            "Train Epoch: 2 [41600/73257] Loss: 0.258973 Acc: 0.9375\n",
            "Train Epoch: 2 [44800/73257] Loss: 0.363609 Acc: 0.9062\n",
            "Train Epoch: 2 [48000/73257] Loss: 0.523399 Acc: 0.8438\n",
            "Train Epoch: 2 [51200/73257] Loss: 0.536048 Acc: 0.8125\n",
            "Train Epoch: 2 [54400/73257] Loss: 0.561513 Acc: 0.8125\n",
            "Train Epoch: 2 [57600/73257] Loss: 0.317973 Acc: 0.8750\n",
            "Train Epoch: 2 [60800/73257] Loss: 0.373354 Acc: 0.8438\n",
            "Train Epoch: 2 [64000/73257] Loss: 0.396378 Acc: 0.8125\n",
            "Train Epoch: 2 [67200/73257] Loss: 0.395345 Acc: 0.8750\n",
            "Train Epoch: 2 [70400/73257] Loss: 0.743752 Acc: 0.7500\n",
            "Elapsed 65.10s, 21.70 s/epoch, 0.01 s/batch, ets 368.89s\n",
            "\n",
            "Test set: Average loss: 0.5606, Accuracy: 21704/26032 (83%)\n",
            "\n",
            "Train Epoch: 3 [3200/73257] Loss: 0.309196 Acc: 0.9062\n",
            "Train Epoch: 3 [6400/73257] Loss: 0.586178 Acc: 0.8125\n",
            "Train Epoch: 3 [9600/73257] Loss: 0.325234 Acc: 0.8750\n",
            "Train Epoch: 3 [12800/73257] Loss: 0.201161 Acc: 0.9062\n",
            "Train Epoch: 3 [16000/73257] Loss: 0.575300 Acc: 0.8125\n",
            "Train Epoch: 3 [19200/73257] Loss: 0.202103 Acc: 0.9688\n",
            "Train Epoch: 3 [22400/73257] Loss: 0.990189 Acc: 0.7812\n",
            "Train Epoch: 3 [25600/73257] Loss: 0.670100 Acc: 0.7188\n",
            "Train Epoch: 3 [28800/73257] Loss: 0.248668 Acc: 0.9375\n",
            "Train Epoch: 3 [32000/73257] Loss: 0.502554 Acc: 0.8750\n",
            "Train Epoch: 3 [35200/73257] Loss: 0.465822 Acc: 0.8750\n",
            "Train Epoch: 3 [38400/73257] Loss: 0.692069 Acc: 0.7812\n",
            "Train Epoch: 3 [41600/73257] Loss: 0.440298 Acc: 0.9062\n",
            "Train Epoch: 3 [44800/73257] Loss: 0.405516 Acc: 0.9062\n",
            "Train Epoch: 3 [48000/73257] Loss: 0.566170 Acc: 0.8125\n",
            "Train Epoch: 3 [51200/73257] Loss: 0.392932 Acc: 0.9062\n",
            "Train Epoch: 3 [54400/73257] Loss: 0.505299 Acc: 0.8125\n",
            "Train Epoch: 3 [57600/73257] Loss: 0.112444 Acc: 0.9688\n",
            "Train Epoch: 3 [60800/73257] Loss: 1.056321 Acc: 0.7812\n",
            "Train Epoch: 3 [64000/73257] Loss: 0.228215 Acc: 0.9375\n",
            "Train Epoch: 3 [67200/73257] Loss: 0.289671 Acc: 0.9062\n",
            "Train Epoch: 3 [70400/73257] Loss: 0.273089 Acc: 0.9062\n",
            "Elapsed 89.10s, 22.28 s/epoch, 0.01 s/batch, ets 356.41s\n",
            "\n",
            "Test set: Average loss: 0.4840, Accuracy: 22386/26032 (86%)\n",
            "\n",
            "Train Epoch: 4 [3200/73257] Loss: 0.611563 Acc: 0.8438\n",
            "Train Epoch: 4 [6400/73257] Loss: 0.412208 Acc: 0.8750\n",
            "Train Epoch: 4 [9600/73257] Loss: 0.523020 Acc: 0.8438\n",
            "Train Epoch: 4 [12800/73257] Loss: 0.332745 Acc: 0.9062\n",
            "Train Epoch: 4 [16000/73257] Loss: 0.313912 Acc: 0.9375\n",
            "Train Epoch: 4 [19200/73257] Loss: 0.584090 Acc: 0.8750\n",
            "Train Epoch: 4 [22400/73257] Loss: 0.327031 Acc: 0.9062\n",
            "Train Epoch: 4 [25600/73257] Loss: 0.334251 Acc: 0.9375\n",
            "Train Epoch: 4 [28800/73257] Loss: 0.292587 Acc: 0.9062\n",
            "Train Epoch: 4 [32000/73257] Loss: 0.108761 Acc: 0.9688\n",
            "Train Epoch: 4 [35200/73257] Loss: 0.313383 Acc: 0.9062\n",
            "Train Epoch: 4 [38400/73257] Loss: 0.148834 Acc: 1.0000\n",
            "Train Epoch: 4 [41600/73257] Loss: 0.197914 Acc: 0.9375\n",
            "Train Epoch: 4 [44800/73257] Loss: 0.219953 Acc: 0.9375\n",
            "Train Epoch: 4 [48000/73257] Loss: 0.487970 Acc: 0.9375\n",
            "Train Epoch: 4 [51200/73257] Loss: 0.353190 Acc: 0.9062\n",
            "Train Epoch: 4 [54400/73257] Loss: 0.289418 Acc: 0.9062\n",
            "Train Epoch: 4 [57600/73257] Loss: 0.176216 Acc: 0.9688\n",
            "Train Epoch: 4 [60800/73257] Loss: 0.497566 Acc: 0.8125\n",
            "Train Epoch: 4 [64000/73257] Loss: 0.388297 Acc: 0.8438\n",
            "Train Epoch: 4 [67200/73257] Loss: 0.293350 Acc: 0.9688\n",
            "Train Epoch: 4 [70400/73257] Loss: 0.424565 Acc: 0.8438\n",
            "Elapsed 112.45s, 22.49 s/epoch, 0.01 s/batch, ets 337.35s\n",
            "\n",
            "Test set: Average loss: 0.4322, Accuracy: 22720/26032 (87%)\n",
            "\n",
            "Train Epoch: 5 [3200/73257] Loss: 0.292709 Acc: 0.9375\n",
            "Train Epoch: 5 [6400/73257] Loss: 0.354796 Acc: 0.9375\n",
            "Train Epoch: 5 [9600/73257] Loss: 0.257967 Acc: 0.9375\n",
            "Train Epoch: 5 [12800/73257] Loss: 0.309692 Acc: 0.9375\n",
            "Train Epoch: 5 [16000/73257] Loss: 0.753400 Acc: 0.9062\n",
            "Train Epoch: 5 [19200/73257] Loss: 0.205771 Acc: 0.9062\n",
            "Train Epoch: 5 [22400/73257] Loss: 0.625553 Acc: 0.8125\n",
            "Train Epoch: 5 [25600/73257] Loss: 0.368767 Acc: 0.9375\n",
            "Train Epoch: 5 [28800/73257] Loss: 0.126115 Acc: 1.0000\n",
            "Train Epoch: 5 [32000/73257] Loss: 0.433823 Acc: 0.8750\n",
            "Train Epoch: 5 [35200/73257] Loss: 0.589995 Acc: 0.7812\n",
            "Train Epoch: 5 [38400/73257] Loss: 0.470031 Acc: 0.9375\n",
            "Train Epoch: 5 [41600/73257] Loss: 0.400447 Acc: 0.9062\n",
            "Train Epoch: 5 [44800/73257] Loss: 0.144294 Acc: 0.9688\n",
            "Train Epoch: 5 [48000/73257] Loss: 0.283777 Acc: 0.8750\n",
            "Train Epoch: 5 [51200/73257] Loss: 0.653248 Acc: 0.8438\n",
            "Train Epoch: 5 [54400/73257] Loss: 0.377016 Acc: 0.9375\n",
            "Train Epoch: 5 [57600/73257] Loss: 0.317269 Acc: 0.9062\n",
            "Train Epoch: 5 [60800/73257] Loss: 0.414403 Acc: 0.8750\n",
            "Train Epoch: 5 [64000/73257] Loss: 0.409074 Acc: 0.8438\n",
            "Train Epoch: 5 [67200/73257] Loss: 0.509288 Acc: 0.8438\n",
            "Train Epoch: 5 [70400/73257] Loss: 0.505461 Acc: 0.8438\n",
            "Elapsed 136.04s, 22.67 s/epoch, 0.01 s/batch, ets 317.42s\n",
            "\n",
            "Test set: Average loss: 0.4166, Accuracy: 22800/26032 (88%)\n",
            "\n",
            "Train Epoch: 6 [3200/73257] Loss: 0.185748 Acc: 0.9688\n",
            "Train Epoch: 6 [6400/73257] Loss: 0.343340 Acc: 0.9062\n",
            "Train Epoch: 6 [9600/73257] Loss: 0.445691 Acc: 0.9062\n",
            "Train Epoch: 6 [12800/73257] Loss: 0.593904 Acc: 0.7812\n",
            "Train Epoch: 6 [16000/73257] Loss: 0.229794 Acc: 0.9375\n",
            "Train Epoch: 6 [19200/73257] Loss: 0.417171 Acc: 0.9062\n",
            "Train Epoch: 6 [22400/73257] Loss: 0.262454 Acc: 0.9062\n",
            "Train Epoch: 6 [25600/73257] Loss: 0.353094 Acc: 0.9375\n",
            "Train Epoch: 6 [28800/73257] Loss: 0.237369 Acc: 0.9062\n",
            "Train Epoch: 6 [32000/73257] Loss: 0.404733 Acc: 0.9062\n",
            "Train Epoch: 6 [35200/73257] Loss: 0.120519 Acc: 0.9688\n",
            "Train Epoch: 6 [38400/73257] Loss: 0.216198 Acc: 0.8750\n",
            "Train Epoch: 6 [41600/73257] Loss: 0.417094 Acc: 0.8438\n",
            "Train Epoch: 6 [44800/73257] Loss: 0.284924 Acc: 0.8750\n",
            "Train Epoch: 6 [48000/73257] Loss: 0.315746 Acc: 0.9062\n",
            "Train Epoch: 6 [51200/73257] Loss: 0.682544 Acc: 0.7812\n",
            "Train Epoch: 6 [54400/73257] Loss: 0.670249 Acc: 0.8125\n",
            "Train Epoch: 6 [57600/73257] Loss: 0.706050 Acc: 0.7500\n",
            "Train Epoch: 6 [60800/73257] Loss: 0.371038 Acc: 0.8438\n",
            "Train Epoch: 6 [64000/73257] Loss: 0.314716 Acc: 0.8750\n",
            "Train Epoch: 6 [67200/73257] Loss: 0.202735 Acc: 0.9375\n",
            "Train Epoch: 6 [70400/73257] Loss: 0.235676 Acc: 0.9062\n",
            "Elapsed 159.83s, 22.83 s/epoch, 0.01 s/batch, ets 296.82s\n",
            "\n",
            "Test set: Average loss: 0.4223, Accuracy: 22774/26032 (87%)\n",
            "\n",
            "Train Epoch: 7 [3200/73257] Loss: 1.312646 Acc: 0.7500\n",
            "Train Epoch: 7 [6400/73257] Loss: 0.399613 Acc: 0.8750\n",
            "Train Epoch: 7 [9600/73257] Loss: 0.377730 Acc: 0.9062\n",
            "Train Epoch: 7 [12800/73257] Loss: 0.273153 Acc: 0.9062\n",
            "Train Epoch: 7 [16000/73257] Loss: 0.206191 Acc: 0.9375\n",
            "Train Epoch: 7 [19200/73257] Loss: 0.264874 Acc: 0.9375\n",
            "Train Epoch: 7 [22400/73257] Loss: 0.458416 Acc: 0.7812\n",
            "Train Epoch: 7 [25600/73257] Loss: 0.147015 Acc: 0.9375\n",
            "Train Epoch: 7 [28800/73257] Loss: 0.707759 Acc: 0.8438\n",
            "Train Epoch: 7 [32000/73257] Loss: 0.248846 Acc: 0.9062\n",
            "Train Epoch: 7 [35200/73257] Loss: 0.425010 Acc: 0.8750\n",
            "Train Epoch: 7 [38400/73257] Loss: 0.172345 Acc: 0.9375\n",
            "Train Epoch: 7 [41600/73257] Loss: 0.664510 Acc: 0.9062\n",
            "Train Epoch: 7 [44800/73257] Loss: 0.415134 Acc: 0.9062\n",
            "Train Epoch: 7 [48000/73257] Loss: 0.338826 Acc: 0.8438\n",
            "Train Epoch: 7 [51200/73257] Loss: 0.307887 Acc: 0.8750\n",
            "Train Epoch: 7 [54400/73257] Loss: 0.588633 Acc: 0.8750\n",
            "Train Epoch: 7 [57600/73257] Loss: 0.200896 Acc: 0.9375\n",
            "Train Epoch: 7 [60800/73257] Loss: 0.787920 Acc: 0.8750\n",
            "Train Epoch: 7 [64000/73257] Loss: 0.234164 Acc: 0.9375\n",
            "Train Epoch: 7 [67200/73257] Loss: 0.676813 Acc: 0.8750\n",
            "Train Epoch: 7 [70400/73257] Loss: 0.159147 Acc: 0.9688\n",
            "Elapsed 183.38s, 22.92 s/epoch, 0.01 s/batch, ets 275.06s\n",
            "\n",
            "Test set: Average loss: 0.3776, Accuracy: 23168/26032 (89%)\n",
            "\n",
            "Train Epoch: 8 [3200/73257] Loss: 0.181874 Acc: 0.9688\n",
            "Train Epoch: 8 [6400/73257] Loss: 0.537964 Acc: 0.8438\n",
            "Train Epoch: 8 [9600/73257] Loss: 0.208683 Acc: 0.9062\n",
            "Train Epoch: 8 [12800/73257] Loss: 0.126778 Acc: 0.9688\n",
            "Train Epoch: 8 [16000/73257] Loss: 0.049048 Acc: 1.0000\n",
            "Train Epoch: 8 [19200/73257] Loss: 0.207584 Acc: 0.9062\n",
            "Train Epoch: 8 [22400/73257] Loss: 0.196099 Acc: 0.8750\n",
            "Train Epoch: 8 [25600/73257] Loss: 0.325189 Acc: 0.9688\n",
            "Train Epoch: 8 [28800/73257] Loss: 0.833892 Acc: 0.7500\n",
            "Train Epoch: 8 [32000/73257] Loss: 0.223470 Acc: 0.9375\n",
            "Train Epoch: 8 [35200/73257] Loss: 0.133522 Acc: 1.0000\n",
            "Train Epoch: 8 [38400/73257] Loss: 0.642071 Acc: 0.8125\n",
            "Train Epoch: 8 [41600/73257] Loss: 0.030976 Acc: 1.0000\n",
            "Train Epoch: 8 [44800/73257] Loss: 0.228955 Acc: 0.9375\n",
            "Train Epoch: 8 [48000/73257] Loss: 0.591967 Acc: 0.7812\n",
            "Train Epoch: 8 [51200/73257] Loss: 0.304476 Acc: 0.9375\n",
            "Train Epoch: 8 [54400/73257] Loss: 0.140507 Acc: 0.9688\n",
            "Train Epoch: 8 [57600/73257] Loss: 0.332575 Acc: 0.9375\n",
            "Train Epoch: 8 [60800/73257] Loss: 0.264006 Acc: 0.9375\n",
            "Train Epoch: 8 [64000/73257] Loss: 0.352086 Acc: 0.9062\n",
            "Train Epoch: 8 [67200/73257] Loss: 0.241386 Acc: 0.9375\n",
            "Train Epoch: 8 [70400/73257] Loss: 0.090753 Acc: 1.0000\n",
            "Elapsed 206.83s, 22.98 s/epoch, 0.01 s/batch, ets 252.79s\n",
            "\n",
            "Test set: Average loss: 0.4131, Accuracy: 22924/26032 (88%)\n",
            "\n",
            "Train Epoch: 9 [3200/73257] Loss: 0.522302 Acc: 0.8438\n",
            "Train Epoch: 9 [6400/73257] Loss: 0.211472 Acc: 0.9375\n",
            "Train Epoch: 9 [9600/73257] Loss: 0.250418 Acc: 0.9375\n",
            "Train Epoch: 9 [12800/73257] Loss: 0.328377 Acc: 0.9062\n",
            "Train Epoch: 9 [16000/73257] Loss: 0.205279 Acc: 0.9375\n",
            "Train Epoch: 9 [19200/73257] Loss: 0.256857 Acc: 0.9375\n",
            "Train Epoch: 9 [22400/73257] Loss: 0.578086 Acc: 0.9062\n",
            "Train Epoch: 9 [25600/73257] Loss: 0.308847 Acc: 0.9062\n",
            "Train Epoch: 9 [28800/73257] Loss: 0.209275 Acc: 0.9375\n",
            "Train Epoch: 9 [32000/73257] Loss: 0.326195 Acc: 0.9062\n",
            "Train Epoch: 9 [35200/73257] Loss: 0.170050 Acc: 0.9062\n",
            "Train Epoch: 9 [38400/73257] Loss: 0.216193 Acc: 0.9375\n",
            "Train Epoch: 9 [41600/73257] Loss: 0.626890 Acc: 0.8750\n",
            "Train Epoch: 9 [44800/73257] Loss: 0.235982 Acc: 0.8750\n",
            "Train Epoch: 9 [48000/73257] Loss: 0.134963 Acc: 0.9375\n",
            "Train Epoch: 9 [51200/73257] Loss: 0.345561 Acc: 0.9375\n",
            "Train Epoch: 9 [54400/73257] Loss: 0.073535 Acc: 1.0000\n",
            "Train Epoch: 9 [57600/73257] Loss: 0.162742 Acc: 0.9375\n",
            "Train Epoch: 9 [60800/73257] Loss: 0.243799 Acc: 0.9375\n",
            "Train Epoch: 9 [64000/73257] Loss: 0.245726 Acc: 0.9375\n",
            "Train Epoch: 9 [67200/73257] Loss: 0.214321 Acc: 0.9375\n",
            "Train Epoch: 9 [70400/73257] Loss: 0.115786 Acc: 0.9688\n",
            "Elapsed 230.32s, 23.03 s/epoch, 0.01 s/batch, ets 230.32s\n",
            "\n",
            "Test set: Average loss: 0.3918, Accuracy: 23028/26032 (88%)\n",
            "\n",
            "Train Epoch: 10 [3200/73257] Loss: 0.164337 Acc: 0.9688\n",
            "Train Epoch: 10 [6400/73257] Loss: 0.202874 Acc: 0.9375\n",
            "Train Epoch: 10 [9600/73257] Loss: 0.385918 Acc: 0.9062\n",
            "Train Epoch: 10 [12800/73257] Loss: 0.262660 Acc: 0.9688\n",
            "Train Epoch: 10 [16000/73257] Loss: 0.154194 Acc: 0.9688\n",
            "Train Epoch: 10 [19200/73257] Loss: 0.633606 Acc: 0.8750\n",
            "Train Epoch: 10 [22400/73257] Loss: 0.502543 Acc: 0.8438\n",
            "Train Epoch: 10 [25600/73257] Loss: 0.272811 Acc: 0.9375\n",
            "Train Epoch: 10 [28800/73257] Loss: 0.444946 Acc: 0.8750\n",
            "Train Epoch: 10 [32000/73257] Loss: 0.130135 Acc: 0.9375\n",
            "Train Epoch: 10 [35200/73257] Loss: 0.130652 Acc: 0.9688\n",
            "Train Epoch: 10 [38400/73257] Loss: 0.207977 Acc: 0.9375\n",
            "Train Epoch: 10 [41600/73257] Loss: 0.511411 Acc: 0.9062\n",
            "Train Epoch: 10 [44800/73257] Loss: 0.161027 Acc: 0.9375\n",
            "Train Epoch: 10 [48000/73257] Loss: 0.540663 Acc: 0.8750\n",
            "Train Epoch: 10 [51200/73257] Loss: 0.301588 Acc: 0.8750\n",
            "Train Epoch: 10 [54400/73257] Loss: 0.410121 Acc: 0.8438\n",
            "Train Epoch: 10 [57600/73257] Loss: 0.365705 Acc: 0.9375\n",
            "Train Epoch: 10 [60800/73257] Loss: 0.311848 Acc: 0.9062\n",
            "Train Epoch: 10 [64000/73257] Loss: 0.947631 Acc: 0.9062\n",
            "Train Epoch: 10 [67200/73257] Loss: 0.211359 Acc: 0.9688\n",
            "Train Epoch: 10 [70400/73257] Loss: 0.100087 Acc: 1.0000\n",
            "Elapsed 254.49s, 23.14 s/epoch, 0.01 s/batch, ets 208.22s\n",
            "\n",
            "Test set: Average loss: 0.3607, Accuracy: 23336/26032 (90%)\n",
            "\n",
            "Train Epoch: 11 [3200/73257] Loss: 0.444760 Acc: 0.8750\n",
            "Train Epoch: 11 [6400/73257] Loss: 0.314512 Acc: 0.8750\n",
            "Train Epoch: 11 [9600/73257] Loss: 0.120640 Acc: 0.9688\n",
            "Train Epoch: 11 [12800/73257] Loss: 0.072556 Acc: 1.0000\n",
            "Train Epoch: 11 [16000/73257] Loss: 0.197773 Acc: 0.9375\n",
            "Train Epoch: 11 [19200/73257] Loss: 0.314187 Acc: 0.9062\n",
            "Train Epoch: 11 [22400/73257] Loss: 0.097586 Acc: 1.0000\n",
            "Train Epoch: 11 [25600/73257] Loss: 0.484822 Acc: 0.8750\n",
            "Train Epoch: 11 [28800/73257] Loss: 0.358312 Acc: 0.8750\n",
            "Train Epoch: 11 [32000/73257] Loss: 0.153495 Acc: 0.9688\n",
            "Train Epoch: 11 [35200/73257] Loss: 0.370511 Acc: 0.9062\n",
            "Train Epoch: 11 [38400/73257] Loss: 0.531679 Acc: 0.8438\n",
            "Train Epoch: 11 [41600/73257] Loss: 0.145989 Acc: 0.9375\n",
            "Train Epoch: 11 [44800/73257] Loss: 0.481537 Acc: 0.7812\n",
            "Train Epoch: 11 [48000/73257] Loss: 0.099608 Acc: 0.9688\n",
            "Train Epoch: 11 [51200/73257] Loss: 0.077700 Acc: 0.9688\n",
            "Train Epoch: 11 [54400/73257] Loss: 0.344916 Acc: 0.8750\n",
            "Train Epoch: 11 [57600/73257] Loss: 0.210144 Acc: 0.9062\n",
            "Train Epoch: 11 [60800/73257] Loss: 0.173234 Acc: 0.9375\n",
            "Train Epoch: 11 [64000/73257] Loss: 0.292365 Acc: 0.9062\n",
            "Train Epoch: 11 [67200/73257] Loss: 0.230950 Acc: 0.9375\n",
            "Train Epoch: 11 [70400/73257] Loss: 0.147152 Acc: 0.9688\n",
            "Elapsed 278.13s, 23.18 s/epoch, 0.01 s/batch, ets 185.42s\n",
            "\n",
            "Test set: Average loss: 0.3541, Accuracy: 23413/26032 (90%)\n",
            "\n",
            "Train Epoch: 12 [3200/73257] Loss: 0.151792 Acc: 0.9375\n",
            "Train Epoch: 12 [6400/73257] Loss: 0.284088 Acc: 0.9062\n",
            "Train Epoch: 12 [9600/73257] Loss: 0.288319 Acc: 0.8750\n",
            "Train Epoch: 12 [12800/73257] Loss: 0.177365 Acc: 0.9375\n",
            "Train Epoch: 12 [16000/73257] Loss: 0.449477 Acc: 0.9062\n",
            "Train Epoch: 12 [19200/73257] Loss: 0.242969 Acc: 0.9688\n",
            "Train Epoch: 12 [22400/73257] Loss: 0.411448 Acc: 0.8750\n",
            "Train Epoch: 12 [25600/73257] Loss: 0.176189 Acc: 0.9688\n",
            "Train Epoch: 12 [28800/73257] Loss: 0.192563 Acc: 0.9375\n",
            "Train Epoch: 12 [32000/73257] Loss: 0.180978 Acc: 0.9375\n",
            "Train Epoch: 12 [35200/73257] Loss: 0.495267 Acc: 0.8750\n",
            "Train Epoch: 12 [38400/73257] Loss: 0.178625 Acc: 0.9062\n",
            "Train Epoch: 12 [41600/73257] Loss: 0.046227 Acc: 1.0000\n",
            "Train Epoch: 12 [44800/73257] Loss: 0.212440 Acc: 0.9688\n",
            "Train Epoch: 12 [48000/73257] Loss: 0.240118 Acc: 0.9062\n",
            "Train Epoch: 12 [51200/73257] Loss: 0.488643 Acc: 0.9375\n",
            "Train Epoch: 12 [54400/73257] Loss: 0.187755 Acc: 0.9062\n",
            "Train Epoch: 12 [57600/73257] Loss: 0.300285 Acc: 0.9062\n",
            "Train Epoch: 12 [60800/73257] Loss: 0.222947 Acc: 0.9062\n",
            "Train Epoch: 12 [64000/73257] Loss: 0.164545 Acc: 0.9688\n",
            "Train Epoch: 12 [67200/73257] Loss: 0.070630 Acc: 0.9688\n",
            "Train Epoch: 12 [70400/73257] Loss: 0.146537 Acc: 0.9375\n",
            "Elapsed 301.75s, 23.21 s/epoch, 0.01 s/batch, ets 162.48s\n",
            "\n",
            "Test set: Average loss: 0.3654, Accuracy: 23335/26032 (90%)\n",
            "\n",
            "Train Epoch: 13 [3200/73257] Loss: 0.500431 Acc: 0.8438\n",
            "Train Epoch: 13 [6400/73257] Loss: 0.379919 Acc: 0.9062\n",
            "Train Epoch: 13 [9600/73257] Loss: 0.132768 Acc: 0.9688\n",
            "Train Epoch: 13 [12800/73257] Loss: 0.238599 Acc: 0.9375\n",
            "Train Epoch: 13 [16000/73257] Loss: 0.172532 Acc: 0.9375\n",
            "Train Epoch: 13 [19200/73257] Loss: 0.161686 Acc: 0.9688\n",
            "Train Epoch: 13 [22400/73257] Loss: 0.280793 Acc: 0.9062\n",
            "Train Epoch: 13 [25600/73257] Loss: 0.036677 Acc: 1.0000\n",
            "Train Epoch: 13 [28800/73257] Loss: 0.062568 Acc: 1.0000\n",
            "Train Epoch: 13 [32000/73257] Loss: 0.119488 Acc: 1.0000\n",
            "Train Epoch: 13 [35200/73257] Loss: 0.032672 Acc: 1.0000\n",
            "Train Epoch: 13 [38400/73257] Loss: 0.086985 Acc: 1.0000\n",
            "Train Epoch: 13 [41600/73257] Loss: 0.161710 Acc: 0.9688\n",
            "Train Epoch: 13 [44800/73257] Loss: 0.242394 Acc: 0.9062\n",
            "Train Epoch: 13 [48000/73257] Loss: 0.599087 Acc: 0.7812\n",
            "Train Epoch: 13 [51200/73257] Loss: 0.284642 Acc: 0.9062\n",
            "Train Epoch: 13 [54400/73257] Loss: 0.266708 Acc: 0.8750\n",
            "Train Epoch: 13 [57600/73257] Loss: 0.389858 Acc: 0.8750\n",
            "Train Epoch: 13 [60800/73257] Loss: 0.493580 Acc: 0.8438\n",
            "Train Epoch: 13 [64000/73257] Loss: 0.212777 Acc: 0.9375\n",
            "Train Epoch: 13 [67200/73257] Loss: 0.204416 Acc: 0.9375\n",
            "Train Epoch: 13 [70400/73257] Loss: 0.135871 Acc: 0.9375\n",
            "Elapsed 325.54s, 23.25 s/epoch, 0.01 s/batch, ets 139.52s\n",
            "\n",
            "Test set: Average loss: 0.3485, Accuracy: 23419/26032 (90%)\n",
            "\n",
            "Train Epoch: 14 [3200/73257] Loss: 0.161992 Acc: 0.9688\n",
            "Train Epoch: 14 [6400/73257] Loss: 0.199933 Acc: 0.9062\n",
            "Train Epoch: 14 [9600/73257] Loss: 0.104940 Acc: 0.9688\n",
            "Train Epoch: 14 [12800/73257] Loss: 0.093040 Acc: 0.9688\n",
            "Train Epoch: 14 [16000/73257] Loss: 0.130318 Acc: 0.9375\n",
            "Train Epoch: 14 [19200/73257] Loss: 0.271941 Acc: 0.8750\n",
            "Train Epoch: 14 [22400/73257] Loss: 0.286220 Acc: 0.9062\n",
            "Train Epoch: 14 [25600/73257] Loss: 0.224174 Acc: 0.9375\n",
            "Train Epoch: 14 [28800/73257] Loss: 0.271488 Acc: 0.9062\n",
            "Train Epoch: 14 [32000/73257] Loss: 0.272616 Acc: 0.9062\n",
            "Train Epoch: 14 [35200/73257] Loss: 0.225559 Acc: 0.9375\n",
            "Train Epoch: 14 [38400/73257] Loss: 0.222902 Acc: 0.8750\n",
            "Train Epoch: 14 [41600/73257] Loss: 0.138978 Acc: 0.9688\n",
            "Train Epoch: 14 [44800/73257] Loss: 0.151264 Acc: 1.0000\n",
            "Train Epoch: 14 [48000/73257] Loss: 0.151282 Acc: 0.9375\n",
            "Train Epoch: 14 [51200/73257] Loss: 0.111836 Acc: 0.9688\n",
            "Train Epoch: 14 [54400/73257] Loss: 0.059571 Acc: 1.0000\n",
            "Train Epoch: 14 [57600/73257] Loss: 0.175936 Acc: 0.9375\n",
            "Train Epoch: 14 [60800/73257] Loss: 0.167326 Acc: 0.9375\n",
            "Train Epoch: 14 [64000/73257] Loss: 0.570914 Acc: 0.8125\n",
            "Train Epoch: 14 [67200/73257] Loss: 0.236193 Acc: 0.9062\n",
            "Train Epoch: 14 [70400/73257] Loss: 0.537221 Acc: 0.8750\n",
            "Elapsed 349.14s, 23.28 s/epoch, 0.01 s/batch, ets 116.38s\n",
            "\n",
            "Test set: Average loss: 0.3553, Accuracy: 23451/26032 (90%)\n",
            "\n",
            "Train Epoch: 15 [3200/73257] Loss: 0.145195 Acc: 0.9688\n",
            "Train Epoch: 15 [6400/73257] Loss: 0.227779 Acc: 0.9375\n",
            "Train Epoch: 15 [9600/73257] Loss: 0.217745 Acc: 0.9375\n",
            "Train Epoch: 15 [12800/73257] Loss: 0.144688 Acc: 0.9375\n",
            "Train Epoch: 15 [16000/73257] Loss: 0.294333 Acc: 0.9062\n",
            "Train Epoch: 15 [19200/73257] Loss: 0.110368 Acc: 0.9688\n",
            "Train Epoch: 15 [22400/73257] Loss: 0.225330 Acc: 0.9375\n",
            "Train Epoch: 15 [25600/73257] Loss: 0.074799 Acc: 1.0000\n",
            "Train Epoch: 15 [28800/73257] Loss: 0.239483 Acc: 0.9375\n",
            "Train Epoch: 15 [32000/73257] Loss: 0.298639 Acc: 0.9062\n",
            "Train Epoch: 15 [35200/73257] Loss: 0.058465 Acc: 1.0000\n",
            "Train Epoch: 15 [38400/73257] Loss: 0.277815 Acc: 0.8750\n",
            "Train Epoch: 15 [41600/73257] Loss: 0.041849 Acc: 1.0000\n",
            "Train Epoch: 15 [44800/73257] Loss: 0.416729 Acc: 0.8750\n",
            "Train Epoch: 15 [48000/73257] Loss: 0.131444 Acc: 0.9375\n",
            "Train Epoch: 15 [51200/73257] Loss: 0.084114 Acc: 1.0000\n",
            "Train Epoch: 15 [54400/73257] Loss: 0.026204 Acc: 1.0000\n",
            "Train Epoch: 15 [57600/73257] Loss: 0.331530 Acc: 0.9062\n",
            "Train Epoch: 15 [60800/73257] Loss: 0.277684 Acc: 0.9062\n",
            "Train Epoch: 15 [64000/73257] Loss: 0.227759 Acc: 0.9375\n",
            "Train Epoch: 15 [67200/73257] Loss: 0.245046 Acc: 0.9375\n",
            "Train Epoch: 15 [70400/73257] Loss: 0.217294 Acc: 0.9375\n",
            "Elapsed 373.22s, 23.33 s/epoch, 0.01 s/batch, ets 93.31s\n",
            "\n",
            "Test set: Average loss: 0.3778, Accuracy: 23242/26032 (89%)\n",
            "\n",
            "Train Epoch: 16 [3200/73257] Loss: 0.608929 Acc: 0.8438\n",
            "Train Epoch: 16 [6400/73257] Loss: 0.253849 Acc: 0.9375\n",
            "Train Epoch: 16 [9600/73257] Loss: 0.214844 Acc: 0.9375\n",
            "Train Epoch: 16 [12800/73257] Loss: 0.615021 Acc: 0.9062\n",
            "Train Epoch: 16 [16000/73257] Loss: 0.159927 Acc: 0.9062\n",
            "Train Epoch: 16 [19200/73257] Loss: 0.273164 Acc: 0.9062\n",
            "Train Epoch: 16 [22400/73257] Loss: 0.125230 Acc: 0.9688\n",
            "Train Epoch: 16 [25600/73257] Loss: 0.344249 Acc: 0.8438\n",
            "Train Epoch: 16 [28800/73257] Loss: 0.311164 Acc: 0.9062\n",
            "Train Epoch: 16 [32000/73257] Loss: 0.435567 Acc: 0.8125\n",
            "Train Epoch: 16 [35200/73257] Loss: 0.270636 Acc: 0.9062\n",
            "Train Epoch: 16 [38400/73257] Loss: 0.166579 Acc: 0.9688\n",
            "Train Epoch: 16 [41600/73257] Loss: 0.115227 Acc: 0.9688\n",
            "Train Epoch: 16 [44800/73257] Loss: 0.197671 Acc: 0.9688\n",
            "Train Epoch: 16 [48000/73257] Loss: 0.275410 Acc: 0.9062\n",
            "Train Epoch: 16 [51200/73257] Loss: 0.213230 Acc: 0.8750\n",
            "Train Epoch: 16 [54400/73257] Loss: 0.479604 Acc: 0.8125\n",
            "Train Epoch: 16 [57600/73257] Loss: 0.067551 Acc: 1.0000\n",
            "Train Epoch: 16 [60800/73257] Loss: 0.095964 Acc: 0.9688\n",
            "Train Epoch: 16 [64000/73257] Loss: 0.273340 Acc: 0.9062\n",
            "Train Epoch: 16 [67200/73257] Loss: 0.134655 Acc: 0.9375\n",
            "Train Epoch: 16 [70400/73257] Loss: 0.267025 Acc: 0.9062\n",
            "Elapsed 397.39s, 23.38 s/epoch, 0.01 s/batch, ets 70.13s\n",
            "\n",
            "Test set: Average loss: 0.3534, Accuracy: 23500/26032 (90%)\n",
            "\n",
            "Train Epoch: 17 [3200/73257] Loss: 0.041782 Acc: 1.0000\n",
            "Train Epoch: 17 [6400/73257] Loss: 0.264473 Acc: 0.8750\n",
            "Train Epoch: 17 [9600/73257] Loss: 0.207754 Acc: 0.9062\n",
            "Train Epoch: 17 [12800/73257] Loss: 0.048158 Acc: 1.0000\n",
            "Train Epoch: 17 [16000/73257] Loss: 0.224369 Acc: 0.9375\n",
            "Train Epoch: 17 [19200/73257] Loss: 0.057718 Acc: 0.9688\n",
            "Train Epoch: 17 [22400/73257] Loss: 0.172318 Acc: 0.9375\n",
            "Train Epoch: 17 [25600/73257] Loss: 0.056822 Acc: 0.9688\n",
            "Train Epoch: 17 [28800/73257] Loss: 0.359799 Acc: 0.9688\n",
            "Train Epoch: 17 [32000/73257] Loss: 0.105466 Acc: 0.9688\n",
            "Train Epoch: 17 [35200/73257] Loss: 0.684456 Acc: 0.8125\n",
            "Train Epoch: 17 [38400/73257] Loss: 0.219709 Acc: 0.9688\n",
            "Train Epoch: 17 [41600/73257] Loss: 0.598783 Acc: 0.9062\n",
            "Train Epoch: 17 [44800/73257] Loss: 0.326769 Acc: 0.8438\n",
            "Train Epoch: 17 [48000/73257] Loss: 0.588259 Acc: 0.9375\n",
            "Train Epoch: 17 [51200/73257] Loss: 0.232760 Acc: 0.8750\n",
            "Train Epoch: 17 [54400/73257] Loss: 0.294049 Acc: 0.8438\n",
            "Train Epoch: 17 [57600/73257] Loss: 0.343812 Acc: 0.8750\n",
            "Train Epoch: 17 [60800/73257] Loss: 0.207667 Acc: 0.9688\n",
            "Train Epoch: 17 [64000/73257] Loss: 0.196544 Acc: 0.9688\n",
            "Train Epoch: 17 [67200/73257] Loss: 0.293181 Acc: 0.8438\n",
            "Train Epoch: 17 [70400/73257] Loss: 0.350149 Acc: 0.9688\n",
            "Elapsed 421.23s, 23.40 s/epoch, 0.01 s/batch, ets 46.80s\n",
            "\n",
            "Test set: Average loss: 0.3455, Accuracy: 23589/26032 (91%)\n",
            "\n",
            "Train Epoch: 18 [3200/73257] Loss: 0.099256 Acc: 0.9375\n",
            "Train Epoch: 18 [6400/73257] Loss: 0.109131 Acc: 0.9375\n",
            "Train Epoch: 18 [9600/73257] Loss: 0.071463 Acc: 1.0000\n",
            "Train Epoch: 18 [12800/73257] Loss: 0.578184 Acc: 0.8125\n",
            "Train Epoch: 18 [16000/73257] Loss: 0.166639 Acc: 0.9375\n",
            "Train Epoch: 18 [19200/73257] Loss: 0.080544 Acc: 0.9688\n",
            "Train Epoch: 18 [22400/73257] Loss: 0.133406 Acc: 0.9062\n",
            "Train Epoch: 18 [25600/73257] Loss: 0.307091 Acc: 0.9375\n",
            "Train Epoch: 18 [28800/73257] Loss: 0.315007 Acc: 0.8750\n",
            "Train Epoch: 18 [32000/73257] Loss: 0.270055 Acc: 0.9062\n",
            "Train Epoch: 18 [35200/73257] Loss: 0.162353 Acc: 0.9062\n",
            "Train Epoch: 18 [38400/73257] Loss: 0.235426 Acc: 0.9375\n",
            "Train Epoch: 18 [41600/73257] Loss: 0.107632 Acc: 0.9688\n",
            "Train Epoch: 18 [44800/73257] Loss: 0.352779 Acc: 0.9062\n",
            "Train Epoch: 18 [48000/73257] Loss: 0.165759 Acc: 0.9375\n",
            "Train Epoch: 18 [51200/73257] Loss: 0.295194 Acc: 0.9375\n",
            "Train Epoch: 18 [54400/73257] Loss: 0.172433 Acc: 0.9688\n",
            "Train Epoch: 18 [57600/73257] Loss: 0.150661 Acc: 0.9688\n",
            "Train Epoch: 18 [60800/73257] Loss: 0.276267 Acc: 0.9375\n",
            "Train Epoch: 18 [64000/73257] Loss: 0.104440 Acc: 0.9688\n",
            "Train Epoch: 18 [67200/73257] Loss: 0.247985 Acc: 0.8750\n",
            "Train Epoch: 18 [70400/73257] Loss: 0.397670 Acc: 0.8750\n",
            "Elapsed 445.20s, 23.43 s/epoch, 0.01 s/batch, ets 23.43s\n",
            "\n",
            "Test set: Average loss: 0.3439, Accuracy: 23542/26032 (90%)\n",
            "\n",
            "Train Epoch: 19 [3200/73257] Loss: 0.151274 Acc: 0.9375\n",
            "Train Epoch: 19 [6400/73257] Loss: 0.056473 Acc: 1.0000\n",
            "Train Epoch: 19 [9600/73257] Loss: 0.599449 Acc: 0.9375\n",
            "Train Epoch: 19 [12800/73257] Loss: 0.446105 Acc: 0.8438\n",
            "Train Epoch: 19 [16000/73257] Loss: 0.462122 Acc: 0.8438\n",
            "Train Epoch: 19 [19200/73257] Loss: 0.106194 Acc: 1.0000\n",
            "Train Epoch: 19 [22400/73257] Loss: 0.263907 Acc: 0.8750\n",
            "Train Epoch: 19 [25600/73257] Loss: 0.155587 Acc: 0.9375\n",
            "Train Epoch: 19 [28800/73257] Loss: 0.316489 Acc: 0.9062\n",
            "Train Epoch: 19 [32000/73257] Loss: 0.183216 Acc: 0.9375\n",
            "Train Epoch: 19 [35200/73257] Loss: 0.187091 Acc: 0.9688\n",
            "Train Epoch: 19 [38400/73257] Loss: 0.255821 Acc: 0.9062\n",
            "Train Epoch: 19 [41600/73257] Loss: 0.193009 Acc: 0.9375\n",
            "Train Epoch: 19 [44800/73257] Loss: 0.403486 Acc: 0.9062\n",
            "Train Epoch: 19 [48000/73257] Loss: 0.162310 Acc: 0.9688\n",
            "Train Epoch: 19 [51200/73257] Loss: 0.130390 Acc: 0.9375\n",
            "Train Epoch: 19 [54400/73257] Loss: 0.064555 Acc: 1.0000\n",
            "Train Epoch: 19 [57600/73257] Loss: 0.467481 Acc: 0.8438\n",
            "Train Epoch: 19 [60800/73257] Loss: 0.222042 Acc: 0.9062\n",
            "Train Epoch: 19 [64000/73257] Loss: 0.152044 Acc: 0.9375\n",
            "Train Epoch: 19 [67200/73257] Loss: 0.124945 Acc: 0.9688\n",
            "Train Epoch: 19 [70400/73257] Loss: 0.074856 Acc: 1.0000\n",
            "Elapsed 469.21s, 23.46 s/epoch, 0.01 s/batch, ets 0.00s\n",
            "\n",
            "Test set: Average loss: 0.3876, Accuracy: 23294/26032 (89%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUMHs1Revk4O",
        "colab_type": "code",
        "outputId": "5dd4571b-f8ba-459d-a219-fb08de239342",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"Total time: {:.2f}, Best Loss: {:.3f}\".format(time.time() - t_begin, best_loss))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total time: 474.99, Best Loss: 0.344\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeeFfy2I99GO",
        "colab_type": "code",
        "outputId": "bc95d85a-f1ec-49e4-cb51-ce696f001b4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        }
      },
      "source": [
        "# Plot loss\n",
        "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
        "x = range(len(epoch_train_loss))\n",
        "\n",
        "plt.plot(x, epoch_train_loss, 'r', label=\"train loss\")\n",
        "plt.plot(x, epoch_test_loss, 'b',label=\"validation loss\")\n",
        "\n",
        "plt.xlabel('epoch no.')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmEAAAGDCAYAAABjkcdfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nOzdeXhU5d3/8fc3IRDZd5RNlgHZBURF\nEQFRi1ixuIGVurRq1fpYH1vr0p+41VZb2vporVbb4lqt1VqxolgVRC2ogIjgxi6LsglhV0ju3x/3\nDDOEyWSSzJmTST6v6zrXzJxz5pw7J8Pkw70dc84hIiIiItmVF3YBRERERGojhTARERGRECiEiYiI\niIRAIUxEREQkBAphIiIiIiFQCBMREREJgUKYiGBm+Wa23cw6ZnLfMJlZxMwCmYOn9LHN7BUzOy+I\ncpjZTWb2QGXfLyLVl0KYSA6KhqDYUmJmuxJeJw0DqTjnip1zDZ1zn2dy3+rKzF41s4lJ1p9pZmvM\nLL8ix3POneyceyID5TrRzFaUOvbtzrnLqnrsJOe62MxmZPq4IpI+hTCRHBQNQQ2dcw2Bz4HTEtYd\nEAbMrE72S1mtPQJ8L8n67wGPO+eKs1weEamFFMJEaiAz+4WZ/d3MnjSzbcAEMzvGzGab2RYz+8LM\n7jGzguj+dczMmVmn6OvHo9tfMrNtZjbLzDpXdN/o9lPM7DMzKzKze83sbTO7sIxyp1PGH5rZEjPb\nbGb3JLw338x+b2abzGwZMCrFJfoncLCZHZvw/hbAaODR6OsxZjbfzLaa2edmdlOK6/1W7GcqrxzR\nGqiPo9dqqZldHF3fBHgB6JhQq9k6+rt8OOH9Y81sUfQavW5mhyVsW21m15jZh9Hr/aSZ1UtxHcr6\nedqb2b/N7CszW2xm30/YNtjM5kWvyzoz+010fX0z+1v0595iZu+aWcuKnlukNlEIE6m5xgJ/A5oA\nfwf2Aj8GWgJD8OHghyne/13gJqA5vrbt9orua2atgaeBa6PnXQ4cleI46ZRxNHAEMAAfLk+Mrr8c\nOBk4HDgSOKeskzjndgDPAOcnrB4PLHDOLYq+3g6cBzQFTgN+bGbfTlH2mPLKsQ44FWgMXALca2b9\nnHNF0fN8nlCruT7xjWbWE3gM+B+gFfAqMCUWVKPOAU4CuuCvU7Iav/L8Hf+7aguMA35tZsOi2+4F\nfuOcawxE8NcR4CKgPtAeaAFcAeyuxLlFag2FMJGa6y3n3AvOuRLn3C7n3HvOuXecc3udc8uAB4Fh\nKd7/jHNujnNuD/AE0L8S+34bmO+cez667ffAxrIOkmYZf+WcK3LOrQBmJJzrHOD3zrnVzrlNwJ0p\nygu+SfKchJqi86PrYmV53Tm3KHr9PgCeSlKWZFKWI/o7Wea814HXgKFpHBd8UJwSLdue6LGbAEcn\n7HO3c+7L6Ln/Terf2wGitZhHAdc753Y75+YBk4mHuT1ANzNr4Zzb5px7J2F9SyAS7Tc4xzm3vSLn\nFqltFMJEaq5ViS/MrIeZvWhmX5rZVuA2/B/NsnyZ8Hwn0LAS+7ZNLIdzzgGryzpImmVM61zAyhTl\nBXgD2AqcZmbd8TVrTyaU5Rgzm2FmG8ysCLg4SVmSSVkOM/u2mb0Tberbgq81S7fZrm3i8ZxzJfjr\n2S5hn4r83so6x8ZobWHMyoRzXAT0Aj6NNjmOjq5/GF8z97T5wQ13mvoiiqSkECZSc5WeFuFPwEJ8\nTUVjYCJgAZfhC3zzFABmZuwfGEqrShm/ADokvE45hUY0ED6KrwH7HjDVOZdYS/cU8CzQwTnXBPhz\nmmUpsxxmdhC++e5XQBvnXFPglYTjljeVxVrg0ITj5eGv75o0ypWutUBLM2uQsK5j7BzOuU+dc+OB\n1sBvgWfNrNA5941z7hbnXE/gOHxzeIVH6orUJgphIrVHI6AI2BHtW5SqP1im/BsYaGanRWtFfozv\nyxREGZ8GrjazdtFO9tel8Z5H8f3Ovk9CU2RCWb5yzu02s8H4psCqlqMeUBfYABRH+5iNTNi+Dh+A\nGqU49hgzGx7tB3YtsA14p4z9y5NnZoWJi3NuOTAH+KWZ1TOz/vjar8cBzOx7ZtYyWgtXhA+OJWZ2\ngpn1iQbDrfjmyZJKlkukVlAIE6k9fgJcgP+j/Sd85+tAOefW4Tt2/w7YBHQF3ge+DqCM9+P7V30I\nvEe8w3iq8i0B3sWHoxdLbb4c+JX50aU34gNQlcrhnNsC/C/wHPAVcBY+qMa2L8TXvq2IjjBsXaq8\ni/DX5358kBsFjIn2D6uMocCuUgv431k3fNPmM8CNzrkZ0W2jgY+j12USMM459w2+GfOf+AC2CN80\n+bdKlkukVjBfIy8iEjzzk6CuBc5yzr0ZdnlERMKkmjARCZSZjTKzptFRiDfhm6neDblYIiKhUwgT\nkaAdByzDN599CxjrnCurOVJEpNZQc6SIiIhICFQTJiIiIhIChTARERGREOTcbMYtW7Z0nTp1CrsY\nIiIiIuWaO3fuRudc0vkRcy6EderUiTlz5oRdDBEREZFymVmZt1BTc6SIiIhICBTCREREREKgECYi\nIiISgpzrEyYiIlJb7Nmzh9WrV7N79+6wiyLlKCwspH379hQUFKT9nsBCmJn9Ffg2sN451yfJ9vOA\n6wDD36z3cufcB0GVR0REJNesXr2aRo0a0alTJ8ws7OJIGZxzbNq0idWrV9O5c+e03xdkc+TDwKgU\n25cDw5xzfYHbgQcDLIuIiEjO2b17Ny1atFAAq+bMjBYtWlS4xjKwmjDn3Ewz65Ri+38TXs4G2gdV\nFhERkVylAJYbKvN7qi4d838AvFTWRjO71MzmmNmcDRs2ZLFYIiIitdeWLVv44x//WKn3jh49mi1b\ntqS9/y233MKkSZMqda5cFXoIM7MR+BB2XVn7OOcedM4Ncs4NatUq6aSzIiIikmGpQtjevXtTvnfq\n1Kk0bdo0iGLVGKGGMDPrB/wZON05tynMsoiIiMj+rr/+epYuXUr//v259tprmTFjBkOHDmXMmDH0\n6tULgO985zscccQR9O7dmwcfjHfv7tSpExs3bmTFihX07NmTSy65hN69e3PyySeza9eulOedP38+\ngwcPpl+/fowdO5bNmzcDcM8999CrVy/69evH+PHjAXjjjTfo378//fv3Z8CAAWzbti2gq5F5oU1R\nYWYdgX8C33POfRZWOURERHLC1VfD/PmZPWb//nD33WVuvvPOO1m4cCHzo+edMWMG8+bNY+HChftG\nAf71r3+lefPm7Nq1iyOPPJIzzzyTFi1a7HecxYsX8+STT/LQQw9xzjnn8OyzzzJhwoQyz3v++edz\n7733MmzYMCZOnMitt97K3XffzZ133sny5cupV6/evqbOSZMmcd999zFkyBC2b99OYWFhVa9K1gRW\nE2ZmTwKzgMPMbLWZ/cDMLjOzy6K7TARaAH80s/lmVj1uCLl1K7z0EqxfH3ZJREREqp2jjjpqv2kY\n7rnnHg4//HAGDx7MqlWrWLx48QHv6dy5M/379wfgiCOOYMWKFWUev6ioiC1btjBs2DAALrjgAmbO\nnAlAv379OO+883j88cepU8fXIw0ZMoRrrrmGe+65hy1btuxbnwuCHB15bjnbLwYuDur8lbZ0KYwe\nDc88A2eeGXZpREREvBQ1VtnUoEGDfc9nzJjBq6++yqxZs6hfvz7Dhw9POk1DvXr19j3Pz88vtzmy\nLC+++CIzZ87khRde4I477uDDDz/k+uuv59RTT2Xq1KkMGTKEadOm0aNHj0odP9tC75hf7XTt6h+X\nLAm3HCIiIiFr1KhRyj5WRUVFNGvWjPr16/PJJ58we/bsKp+zSZMmNGvWjDfffBOAxx57jGHDhlFS\nUsKqVasYMWIEd911F0VFRWzfvp2lS5fSt29frrvuOo488kg++eSTKpchW3Knzi5bGjeG1q0VwkRE\npNZr0aIFQ4YMoU+fPpxyyimceuqp+20fNWoUDzzwAD179uSwww5j8ODBGTnvI488wmWXXcbOnTvp\n0qULkydPpri4mAkTJlBUVIRzjquuuoqmTZty0003MX36dPLy8ujduzennHJKRsqQDeacC7sMFTJo\n0CA3Z07A3ceGDIG6dWH69GDPIyIiksLHH39Mz549wy6GpCnZ78vM5jrnBiXbX82RyUQiqgkTERGR\nQCmEJROJwOrVUMmOgyIiIiLlUQhLJhLxj8uWhVsOERERqbEUwpKJhTA1SYqIiEhAFMKSUQgTERGR\ngCmEJdOsGTRvrhAmIiIigVEIK4tGSIqIiFRYw4YNAVi7di1nnXVW0n2GDx9OedNN3X333ezcuXPf\n69GjR++7X2RV3HLLLUyaNKnKx8kEhbCyKISJiIhUWtu2bXnmmWcq/f7SIWzq1Kk0bdo0E0WrNhTC\nyhKJwOefw9dfh10SERGRUFx//fXcd999+17HapG2b9/OyJEjGThwIH379uX5558/4L0rVqygT58+\nAOzatYvx48fTs2dPxo4du9+9Iy+//HIGDRpE7969ufnmmwF/U/C1a9cyYsQIRowYAUCnTp3YuHEj\nAL/73e/o06cPffr04e7oPTVXrFhBz549ueSSS+jduzcnn3xyufeonD9/PoMHD6Zfv36MHTuWzZs3\n7zt/r1696NevH+PHjwfgjTfeoH///vTv358BAwakvJ1TunTborJEIlBSAitWwGGHhV0aERGp5a6+\nGubPz+wx+/dPfV/wcePGcfXVV/OjH/0IgKeffppp06ZRWFjIc889R+PGjdm4cSODBw9mzJgxmFnS\n49x///3Ur1+fjz/+mAULFjBw4MB92+644w6aN29OcXExI0eOZMGCBVx11VX87ne/Y/r06bRs2XK/\nY82dO5fJkyfzzjvv4Jzj6KOPZtiwYTRr1ozFixfz5JNP8tBDD3HOOefw7LPPMmHChDJ/vvPPP597\n772XYcOGMXHiRG699Vbuvvtu7rzzTpYvX069evX2NYFOmjSJ++67jyFDhrB9+3YKCwvTvcxlUk1Y\nWTRCUkREarkBAwawfv161q5dywcffECzZs3o0KEDzjluvPFG+vXrx4knnsiaNWtYt25dmceZOXPm\nvjDUr18/+vXrt2/b008/zcCBAxkwYACLFi3io48+Slmmt956i7Fjx9KgQQMaNmzIGWecse9m3507\nd6Z///4AHHHEEaxYsaLM4xQVFbFlyxaGDRsGwAUXXMDMmTP3lfG8887j8ccfp04dX181ZMgQrrnm\nGu655x62bNmyb31VqCasLAphIiJSjaSqsQrS2WefzTPPPMOXX37JuHHjAHjiiSfYsGEDc+fOpaCg\ngE6dOrF79+4KH3v58uVMmjSJ9957j2bNmnHhhRdW6jgx9erV2/c8Pz+/3ObIsrz44ovMnDmTF154\ngTvuuIMPP/yQ66+/nlNPPZWpU6cyZMgQpk2bRo8ePSpdVlBNWNlatoTGjRXCRESkVhs3bhxPPfUU\nzzzzDGeffTbga5Fat25NQUEB06dPZ+XKlSmPcfzxx/O3v/0NgIULF7JgwQIAtm7dSoMGDWjSpAnr\n1q3jpZde2veeRo0aJe13NXToUP71r3+xc+dOduzYwXPPPcfQoUMr/HM1adKEZs2a7atFe+yxxxg2\nbBglJSWsWrWKESNGcNddd1FUVMT27dtZunQpffv25brrruPII4/kk08+qfA5S1NNWFnMNEJSRERq\nvd69e7Nt2zbatWvHIYccAsB5553HaaedRt++fRk0aFC5NUKXX345F110ET179qRnz54cccQRABx+\n+OEMGDCAHj160KFDB4YMGbLvPZdeeimjRo2ibdu2TJ8+fd/6gQMHcuGFF3LUUUcBcPHFFzNgwICU\nTY9leeSRR7jsssvYuXMnXbp0YfLkyRQXFzNhwgSKiopwznHVVVfRtGlTbrrpJqZPn05eXh69e/fm\nlFNOqfD5SjPnXJUPkk2DBg1y5c0tkjHjxsG8ebB4cXbOJyIikuDjjz+mZ8+eYRdD0pTs92Vmc51z\ng5Ltr+bIVCIRPzpyz56wSyIiIiI1jEJYKpEI7N3r5wsTERERySCFsFQ0QlJEREQCohCWikKYiIiE\nLNf6btdWlfk9KYSlcvDBUL++QpiIiISisLCQTZs2KYhVc845Nm3aVOFZ9DVFRSqapkJERELUvn17\nVq9ezYYNG8IuipSjsLCQ9u3bV+g9CmHliUSgnFsoiIiIBKGgoIDOnTuHXQwJiJojyxOJwLJlUFwc\ndklERESkBlEIK08kAt98A6tXh10SERERqUEUwsqjEZIiIiISAIWw8iiEiYiISAAUwsrTrh3Uq6cQ\nJiIiIhmlEFaevDzo2lUhTERERDJKISwdmitMREREMkwhLB2RCCxdCiUlYZdEREREagiFsHREIrBr\nF3zxRdglERERkRpCISwdGiEpIiIiGaYQlg6FMBEREckwhbB0dOgABQUKYSIiIpIxCmHpqFMHOndW\nCBMREZGMUQhLVyQCixeHXQoRERGpIRTC0hWbK8y5sEsiIiIiNYBCWLoiEdixA9atC7skIiIiUgMo\nhKVLIyRFREQkgxTC0qUQJiIiIhmkEJauQw+F/HyFMBEREckIhbB01a3rg5hCmIiIiGSAQlhFxEZI\nioiIiFSRQlhFaJoKERERyRCFsIqIRKCoCDZtCrskIiIikuMUwipCIyRFREQkQxTCKkIhTERERDJE\nIawiOncGM4UwERERqTKFsIooLIQOHRTCREREpMoCC2Fm9lczW29mC8vYbmZ2j5ktMbMFZjYwqLJk\nlKapEBERkQwIsibsYWBUiu2nAN2iy6XA/QGWJXMUwkRERCQDAgthzrmZwFcpdjkdeNR5s4GmZnZI\nUOXJmEjET1GxeXPYJREREZEcFmafsHbAqoTXq6PrDmBml5rZHDObs2HDhqwUrkyxEZJLl4ZbDhER\nEclpOdEx3zn3oHNukHNuUKtWrcItjKapEBERkQwIM4StATokvG4fXVe9deniHxXCREREpArCDGFT\ngPOjoyQHA0XOuS9CLE96GjSAtm0VwkRERKRK6gR1YDN7EhgOtDSz1cDNQAGAc+4BYCowGlgC7AQu\nCqosGacRkiIiIlJFgYUw59y55Wx3wI+COn+gIhF48cWwSyEiIiI5LCc65lc7kQisWwfbtoVdEhER\nEclRCmGVoWkqREREpIoUwipD01SIiIhIFSmEVUbXrv5RIUxEREQqSSGsMho3htatFcJERESk0hTC\nKkvTVIiIiEgVKIRVlkKYiIiIVIFCWGVFIrBmDezcGXZJREREJAcphFVWbITksmXhlkNERERykkJY\nZWmaChEREakChbDKUggTERGRKlAIq6xmzaB5c4UwERERqRSFsKrQCEkRERGpJIWwqlAIExERkUpS\nCKuKSAQ+/xy+/jrskoiIiEiOUQirikgEnIPly8MuiYiIiOQYhbCq0AhJERERqSSFsKpQCBMREZFK\nUgiripYtoXFjhTARERGpMIWwqjDTCEkRERGpFIWwqlIIExERkUpQCKuqSARWrIA9e8IuiYiIiOQQ\nhbCqikSguBhWrgy7JCIiIpJDFMKqSiMkRUREpBIUwqpKIUxEREQqQSGsqg4+GOrXVwgTERGRClEI\nqypNUyEiIiKVoBCWCd26KYSJiIhIhSiEZUIkAsuW+VGSIiIiImlQCMuESMTPE7ZqVdglERERkRyh\nEJYJGiEpIiIiFaQQlgkKYSIiIlJBCmGZ0LYtFBYqhImIiEjaFMIyIS8PunZVCBMREZG0KYRliuYK\nExERkQpQCMuUSASWLoWSkrBLIiIiIjlAISxTIhHYvRvWrg27JCIiIpIDFMIyRSMkRUREpAIUwjJF\nIUxEREQqQCEsUzp0gIIChTARERFJi0JYpuTnQ5cuCmEiIiKSFoWwTNI0FSIiIpImhbBMioUw58Iu\niYiIiFRzCmGZFInAjh2wbl3YJREREZFqTiEskzRCUkRERNKkEJZJCmEiIiKSJoWwTDr0UD9KUiFM\nREREyqEQlkkFBdCpk0KYiIiIlEshLNM0TYWIiIikQSEs0zRNhYiIiKRBISzTIhEoKoJNm8IuiYiI\niFRjCmGZphGSIiIikoZAQ5iZjTKzT81siZldn2R7RzObbmbvm9kCMxsdZHmyQiFMRERE0hBYCDOz\nfOA+4BSgF3CumfUqtdv/A552zg0AxgN/DKo8WdO5M5gphImIiEhKQdaEHQUscc4tc859AzwFnF5q\nHwc0jj5vAqwNsDzZUa8edOyoECYiIiIp1Qnw2O2AVQmvVwNHl9rnFuAVM/sfoAFwYoDlyR5NUyEi\nIiLlCLtj/rnAw8659sBo4DEzO6BMZnapmc0xszkbNmzIeiErTCFMREREyhFkCFsDdEh43T66LtEP\ngKcBnHOzgEKgZekDOecedM4Ncs4NatWqVUDFzaBIxE9RsXlz2CURERGRairIEPYe0M3MOptZXXzH\n+yml9vkcGAlgZj3xISwHqrrKERshuXRpuOUQERGRaiuwEOac2wtcCUwDPsaPglxkZreZ2Zjobj8B\nLjGzD4AngQudqwFTzWuaChERESlHkB3zcc5NBaaWWjcx4flHwJAgyxCKLl38o0KYiIiIlCHsjvk1\nU/360K6dQpiIiIiUSSEsKBohKSIiIikohAVFIUxERERSUAgLSiQC69bBtm1hl0RERESqIYWwoGia\nChEREUlBISwomqZCREREUlAIC0rXrv5RIUxERESSUAgLSqNG0KaNQpiIiIgkpRAWJI2QFBERkTIo\nhAVJIUxERETKoBAWpEgE1qyBnTvDLomIiIhUMwphQYqNkFy2LNxyiIiISLWjEBYkTVMhIiIiZVAI\nC5KmqRAREZEyKIQFqVkzaNFCIUxEREQOoBBWytdfw/33w2efgXMZOKBGSIqIiEgSdcIuQHUzZw5c\ncYV/3qEDjBwZXw45pBIHjETgrbcyWkYRERHJfaoJK+XYY30t2P33w1FHwZQp8L3vQdu20Ls3XHUV\nPP88FBWlecBIBD7/3FexiYiIiESpJqwUM+jWzS+XXQYlJTB/Prz6Krz2Gvz5z3DvvZCXB0ceGa8l\nO/ZYKCxMcsBIxLdrLl8OPXpk/ecRERGR6imtmjAz+7GZNTbvL2Y2z8xODrpw1UFeHgwcCD/7GUyb\nBps3w4wZcOONfttdd/kQ1qwZnHQS3HknvPceFBdHD6BpKkRERCSJdGvCvu+c+z8z+xbQDPge8Bjw\nSmAlq6bq1YNhw/xy++2wdSu88YavJXvtNbjhBr9f06YwYgSMPLoPJ9Kd7ouXYOEWXURERKqRdENY\nLD+MBh5zzi0yM2UKoHFjOO00vwB8+SW8/no8lD33XEPgU9rdtJmR8+PNl+3ahVpsERERCZm5NOZh\nMLPJQDugM3A4kA/McM4dEWzxDjRo0CA3Z86cbJ+2Upzzdyx69YRf8trXx/H63uPZtMlv69HDh7Gz\nz/a1aiIiIlLzmNlc59ygpNvSDGF5QH9gmXNui5k1B9o75xZktqjly6UQts/48TBnDiWfLWHBgngn\n/5kzYdcueOcd38lfREREapZUISzdKSqOAT6NBrAJwP8D0p2kQSIRWLGCvOI99O8PP/0pvPQSrF0L\nbdr4ecn2deQXERGRWiHdEHY/sNPMDgd+AiwFHg2sVDVNJOJT1sqV+61u0gQmTfITxP7lLyGVTURE\nREKRbgjb63y75enAH5xz9wGNgitWDZNimorvfheOP96Pqoz1FxMREZGaL90Qts3MbsBPTfFitI9Y\nQXDFqmFShDAzuO8+PwP/jTdmuVwiIiISmnRD2Djga/x8YV8C7YHfBFaqmqZNG2jQoMwJW/v0gR//\nGB56CN59N8tlExERkVCkFcKiwesJoImZfRvY7ZxTn7B0mfnasBSz5t98Mxx8MPzoR+qkLyIiUhuk\ne9uic4B3gbOBc4B3zOysIAtW45QTwho3jnfS//Ofs1guERERCUW6zZE/B450zl3gnDsfOAq4Kbhi\n1UCRiJ+5NUU117nn+olbb7gBNm7MYtlEREQk69INYXnOufUJrzdV4L0CPoTt2QOrVpW5S6yT/tat\n6qQvIiJS06UbpF42s2lmdqGZXQi8CEwNrlg1UIoRkol694arr/ZNku+8k4VyiYiISCjS7Zh/LfAg\n0C+6POicuy7IgtU4aYYw8J30DzlEnfRFRERqsrSbFJ1zzzrnrokuzwVZqBqpbVsoLEwrhDVq5Dvp\nz53rp60QERGRmidlCDOzbWa2Ncmyzcy2ZquQNUJeHnTtmlYIA3/P7+HDfd+wDRuCLZqIiIhkX8oQ\n5pxr5JxrnGRp5JxrnK1C1hjlTFORyAz+8AfYts2PlhQREZGaRSMcsykSgaVLoaQkrd1jnfT/8heY\nPTvgsomIiEhWKYRlUyQCu3fD2rVpv2XiRN+dTJ30RUREahaFsGyqwAjJmEaN4Le/hXnz4MEHAyqX\niIiIZJ1CWDZVIoQBjBsHI0aok76IiEhNohCWTR06QEFBhUNYrJP+9u1w/fUBlU1ERESySiEsm/Lz\noUuXCocwgF694H//F/76V3XSFxERqQkUwrKtAtNUlHbTTdCuHVxxhTrpi4iI5DqFsGyLhTDnKvzW\nWCf999+HP/0pgLKJiIhI1iiEZVskAjt2wLp1lXr7OefACSfAz38O69dnuGwiIiKSNQph2VbJEZIx\n6qQvIiJSMyiEZVsVQxhAz55wzTUweTLMmpWhcomIiEhWKYRl26GH+lGSVQhhoE76IiIiuU4hLNsK\nCqBTpyqHsIYN4fe/h/nz4YEHMlM0ERERyR6FsDBUYZqKRGedBSNHqpO+iIhILgo0hJnZKDP71MyW\nmFnSbuRmdo6ZfWRmi8zsb0GWp9qowjQViWKd9HfuhOuuy1DZREREJCsCC2Fmlg/cB5wC9ALONbNe\npfbpBtwADHHO9QauDqo81UokAkVFsGlTlQ/Vo4fvpP/ww/Df/1a9aCIiIpIdQdaEHQUscc4tc859\nAzwFnF5qn0uA+5xzmwGcc7WjUS0DIyQT/b//B+3bw49+BHv3ZuSQIiIiErAgQ1g7YFXC69XRdYm6\nA93N7G0zm21mowIsT/WR4RCmTvoiIiK5J+yO+XWAbsBw4FzgITNrWnonM7vUzOaY2ZwNGzZkuYgB\n6NzZd+jKUAgDOPNMOPFEXzZWnzMAACAASURBVCtWycn4RUREJIuCDGFrgA4Jr9tH1yVaDUxxzu1x\nzi0HPsOHsv045x50zg1yzg1q1apVYAXOmnr1oGPHjIYwM7j3XnXSFxERyRVBhrD3gG5m1tnM6gLj\ngSml9vkXvhYMM2uJb55cFmCZqo8MTVORqEcP+MlP4JFH4O23M3poERERybDAQphzbi9wJTAN+Bh4\n2jm3yMxuM7Mx0d2mAZvM7CNgOnCtc67qQwZzQQAhDHxzZIcO6qQvIiJS3QXaJ8w5N9U5190519U5\nd0d03UTn3JToc+ecu8Y518s519c591SQ5alWIhE/RcXmzRk9bIMGvpP+Bx/A/fdn9NAiIiKSQWF3\nzK+9YiMkly7N+KHPOANOOkmd9EVERKozhbCwZHiaikSxTvq7dsHPfpbxw4uIiEgGKISFpUsX/xhA\nCAM47DD46U/h0UfhzTcDOYWIiIhUgUJYWOrXh3btAgth4G/srU76IiIi1ZNCWJgCGiEZ06AB3H03\nfPgh/PGPgZ1GREREKkEhLEyRCCxeDM4FdoqxY+Hkk+Gmm+DLLwM7jYiIiFSQQliYBg+G9evhxz+G\nkpJATpHYSf+ii/yM+iIiIhI+hbAw/eAHcM01PiVdfDEUFwdymu7d/SmmTYORI6Em3H5TREQk19UJ\nuwC1mhlMmgSNGsGtt/pqqsceg4KCjJ/qhz+EVq3gvPPg2GPhpZfis2SIiIhI9qkmLGxmcMst8Otf\nw9//DmeeCbt3B3KqM86A117zk/QfcwzMnh3IaURERCQNCmHVxbXXwn33wQsvwGmnwY4dgZzm2GPh\nv/+Fxo3hhBPgX/8K5DQiIiJSDoWw6uSKK+Dhh+H11+Fb34KiokBO0707zJoFffv62rE//CGQ04iI\niEgKCmHVzQUX+GbJd97xveg3bgzkNK1bw/TpMGYM/M//+Iq4gAZoioiISBIKYdXRWWf5dsKFC2H4\n8MAm+KpfH5591s+oP2kSnHtuYN3RREREpBSFsOrq1FNh6lRYsQKGDoXPPw/kNPn5fvqK3/wGnn7a\nT+z61VeBnEpEREQSKIRVZyecAP/5j5/Ya+jQwG5xZOZv9v3UU74VdMgQn/1EREQkOAph1d0xx/jO\nWzt3+iC2aFFgpxo3zme+L7/0k/nPnRvYqURERGo9hbBcMGAAvPGGr7IaNgzmzQvsVMcf76ewKCz0\nz6dODexUIiIitZpCWK7o1QvefBMaNoQRI3xSCkjPnn4Kix49/OjJBx8M7FQiIiK1lkJYLuna1Qex\nNm3gpJP89PcBOeQQX/l28sn+lkc//zk4F9jpREREah2FsFzToQPMnAlduvgRlP/+d2CnatgQpkzx\n9xb/5S/h/PPhm28CO52IiEitohCWiw4+GGbM8FPejx3r55YISJ06vjnyF7+Axx+HU04JbCJ/ERGR\nWkUhLFe1aOGbIwcP9rOsPvxwYKcy882Rjz7qK+GOOw5WrQrsdCIiIrWCQlgua9wYXn7Z397ooov8\nDcAD9L3v+dN9/rnPfh98EOjpREREajSFsFzXoIHvuDVmDFx5Jfz614GebuRIeOstXzs2dKifV0xE\nREQqTiGsJigshGee8c2S110HEycGOpSxb1+YPRs6d4bRowNtCRUREamx6oRdAMmQggJ47DF/V+7b\nb4ft2+G3v/VVVgFo3973DzvrLN8S+vnncNNNgZ1ORESkxlEIq0ny8/1QxgYN4Pe/90Hs/vv9+gA0\naQIvvgiXXAI33wwrV8IDD/g8KCIiIqkphNU0eXlw991+kq9f/tLfc/Lhh/1cEwGoW9cfvlMnuO02\nWLMG/vEPaNQokNOJiIjUGAphNZEZ3HGHD2I33uiD2JNPQr16gZ3u1luhY0c/u/7xx8M//+n7jImI\niEhy6phfk91wA/zf/8Fzz8Hpp/swFqAf/MBP4L90qb/V5S23wK5dgZ5SREQkZymE1XRXXQV/+Qu8\n8gocc4zvTR+gUaPgo4/gO9/xtWO9esG//qX7ToqIiJSmEFYbfP/7fi6xLVtg2DAYN84PZwxI+/a+\n9XP6dD9GYOxYP5XFZ58FdkoREZGcoxBWW3z72/Dxx76NcMoU6NHD96QPsL1w+HB4/30/TuC//4U+\nfXwL6fbtgZ1SREQkZyiE1Sb16/u5JD75xIeym2+Gnj39RK8BtRcWFMCPf+xrwc47D+680+e/v/9d\nTZQiIlK7KYTVRoceCk8/7dsLmzSBs8+GE06ADz8M7JRt2sDkyb5GrE0bGD/en3LhwsBOKSIiUq0p\nhNVmw4fD3Lnwxz/CggXQvz/86EewaVNgpzzmGHj3XT+pa+yU//u/UFQU2ClFRESqJYWw2q5OHbj8\ncli8GK64Av70J+je3QezvXsDOWV+vp9P7LPP4OKL/Swa3bvDI49ASUkgpxQREal2FMLEa94c7r0X\n5s+P14gNHAgzZgR2yhYtfI3Ye+9Bly5w4YVw3HEwb15gpxQREak2FMJkf336wKuv+s76W7fCiBG+\nz9jKlYGd8ogj4O23fZ+xpUth0CBfORdgq6iIiEjoFMLkQGZw5pl+SovbbvN36e7Rw4+mDGjW/bw8\nXxP26ad+ftmHHvJNlH/6ExQXB3JKERGRUCmESdkOOghuuskno+98xweygOeXaNrUzyv2/vvQty9c\ndhkcdRTMmhXI6UREREKjECbl69DBT4E/c6bvyDV+vB9ZOX9+YKfs29fPoPHkk/Dll3DssXDRRbBu\nXWCnFBERySqFMEnf0KEwZ45vI1y0yHfmuvxy2LgxkNOZ+bz36adw3XXwxBNw2GFwzz2BDdwUERHJ\nGoUwqZj8fLj0Uj+lxZVX+s5b3br5kZUBJaOGDf1M+x9+CEcf7WfgHzAA3ngjkNOJiIhkhUKYVE6z\nZn6Crw8+8MMZr7rKT23x2muBnfKww+Dll+G552DbNt8iesYZcN99vqV08+bATi0iIpJx5nLsBn6D\nBg1yc+bMCbsYksg5eP55uOYaWL4cxo6FG2/0zZVmgZxy50646y74wx/gq6/i6zt0gH794kvfvn6U\nZUFBIMUQERFJyczmOucGJd2mECYZs3s3/Pa38Mtf+pTUp4/vTT9hArRuHcgpnYO1a/0tkBYs8E2W\nCxb42TViraN160KvXvsHs379/D0sA8qIIiIigEKYZNuWLX4ai7/+1d8osk4dOPVUH8hGj85KtdQ3\n38AnnxwYztauje/TqtWBtWa9evmZOYLinM+nW7b45tPYkvh62zYYORJOOUUhUUQk1ymESXg++shP\nhf/YY35+idatfc3YRRf5mrIs27gxHshijwsXwq5dfntenm++LF1rduih8UBUUuJvJlA6PKUKVonP\n9+xJXca6dX2IPOYYuP12OOEEhTERkVylECbh27PH96qfPBleeMG3FR55pA9j48f7jv4hKS72t0tK\nDGYLFsCyZfF9Gjf2U6Rt2QJFRalvNJ6f7yedbdrU/1ixJfF1WduaNPHHfvhhH8BWr4Zhw/zzoUMD\nvxQiIpJhCmFSvWzY4Cf9mjzZp5169Xxn/osu8u1w+flhlxDwzYKLFvkifvCBD2CJwamsMNWoUWZq\nrnbv9jOA3HGHr0Q8+WQfxo46qurHzqbFi+HBB32oPfZYHyr79/et1CIiNZ1CmFRPzvn7E02eDH/7\nmx/m2L49XHCBv5FkJBJ2CauFnTvhj3/0c6Vt2gSnnebvINW/f9glK9uePTBlCtx/v5+1pE4d/6td\nscJvb9QIhgzxgez44/0sJ3XrhlpkEZFApAphgc4TZmajzOxTM1tiZten2O9MM3NmlrSQUkOZwcCB\nfqLXtWvh6ad9J6xf/cpPAHv88T6gbd8edklDVb8+/PSnfvaPX/wC3nzTT1Z79tm+y1118vnn/naj\nHTvCWWf5WrBf/MKvX74c1qzxt6KaMMGvu+EGH8aaNvWVoLfdBjNmxPvoiYjUZIHVhJlZPvAZcBKw\nGngPONc591Gp/RoBLwJ1gSudcymruVQTVgusWeM78k+eDJ99Bg0a+MRx0UW+Y1Qt76W+ZQv87nf+\nRufbt8N3vws33+xzaxiKi2HaNHjgAXjxRV/BOXq0v/n6Kaekbl3esMGHyjfe8BPufvCBf3/duv7u\nCMcf72vLjjnG3zlBRCTXhNIcaWbHALc4574VfX0DgHPuV6X2uxv4D3At8FOFMNnHOZg1y4exp57y\niaNrV99UecEFfmbWWmzTJvjNb3xF4tdfw/nnw8SJ0KlTds6/bp2fheTBB30zY5s2cPHFcMklfjRp\nZWzeDG+/HQ9lc+f6kFenjp/7NxbKYrVnIiLVXVgh7CxglHPu4ujr7wFHO+euTNhnIPBz59yZZjaD\nMkKYmV0KXArQsWPHI1auXBlImaUa27EDnn3WDxucPt3Xhp14og9k3/62H75YS61b5/uL3X+/H1n5\ngx/Az3/u+2BlmnO+ufCBB+Cf//SDXE84wdd6nX565vt1bdvmc3gslL37rp++w8z3iYv1KRs6FFq2\nzOy5RUQyoVqGMDPLA14HLnTOrUgVwhKpJkxYvtyHsUcegZUr/eSvI0bAmDF+qaU1ZGvW+JGUf/6z\nn+/sssvg+uvh4IOrfuyvvoJHH/Xh69NP/UjQCy+EH/7Q39MzW3btgnfeiYeyWbPi/cd6946HspNO\ngubNs1cuEZGyVMvmSDNrAiwFYr2uDwa+AsakCmIKYbJPSYn/K/z883757DO/fuBAXy0zZgwcfnit\n60O2YoWfyuKRR3zN1JVXws9+VvGaIud8zdP99/sbIOzeDYMHw+WX+y56Qd5ZIF3ffANz5sRD2Vtv\n+VbrggLfH23CBF9RWh3KKiLVy969/jukfv1gzxNWCKuD75g/EliD75j/XefcojL2n4FqwqQqPvnE\nz4vw/PM+nDnnOyfFasiGDatVd/JevNiPNnziCT+24eqr4Sc/Kb8v1bZtfsaQBx6A+fN9h/gJE3yt\nV3WeFgP8l+rcufCPf/hRmGvX+ukwzjoLzjsPhg+vNtPQiUhI9u7133G33+7/Q/nLXwZ7vlCmqHDO\n7QWuBKYBHwNPO+cWmdltZjYmqPNKLdajh6/yeftt+OIL3y53+OF+xtOTTvI3i/zud321TlFR2KUN\nXLdufpDpwoW+VugXv4DOnf3jtm0H7r9gAVxxBbRr55syS0p8Ldjatf6xugcw8B34jz4aJk3yU2C8\n+qoPYM8847sQduzop/t4/32f0UWk9ti7138n9urlx3Y1bAjHHRdumTRZq9R8O3fCf/7ja8j+/W8/\nL0JBga8WiTVb1oJ+ZPPn+6kspkzxt2C67jrfif/FF33ImjXL37xg3DgfwgYPrjktubt2+V/944/D\nSy/5yWR79fK1Y9/9bvZGlIpI9u3d6wfY336777Vy+OFwyy3+6z8b33GaMV8kprgYZs8+sB/ZgAH+\nX+Tpp9f4fmTvveensnj55fi67t198LrggprfoX3TJt9c+cQTvg8Z+P8NT5jgmyZq+s8vmbdli++T\n+Prrfikp8ZXvo0b5gSLqkxiO4mLfLSEWvvr1i4evvECnqt+fQphIWT79NB7IYv3IOnb0tWOnn16j\n+5G9/Tb861++qXLEiBqdO8u0YoXvG/L44/Dxx/5XPXq0ryFTh34py44d/t9PLHTNneuD10EH+UBv\n5kPZ7t1QWOi/RkaNgm99y/eaqI3/1rKpuDhe8/Xppz583XwzfOc72Q1fMQphIulYt863zT3/vG++\n3LULmjTxKeX00/03aLNmYZdSAuCcb659/HH/P+cvvvBTz515pjr0ix9B9847PnC99pqvTN+zx4f2\nwYP9XHknnOD7I9ar59+za5cPYi+/7JdPPvHrO3b0XyWjRvlbdTVpEt7PVdMUF/suv7fd5sNX376+\n5ius8BWjECZSUbF+ZFOmwAsv+H5kZv5f9XHH+dlBjzsumBlRJVTFxX4+4Cee8PMDb9sGbdvCuef6\nJsua3FrtnG+uLSys3beJKi6GefPiNV1vvulDVV6enwFn5EgfuoYM8SOP07Fypb+917RpfsDI1q0+\n2B9zTLyWbODAcMNCriou9rcevu02H3b79PHha+zY6nE9FcJEqiLWjyz2bTxrVvym4p06+TAWC2Y9\nelSPf/WSEbt2+Qwe69C/d6/v0D9hgu/QX9nbM4Vp2zY/33HpZcUK/7h9uw+Z3bv7UDBgQPyxpvaX\nKymBRYvioeuNN+IDqPv0idd0HX98ZirD9+zxXynTpvlasrlz/fqWLeHkk30oO/lkfyswKVtxse/f\nedttvjtBnz6+2fGMM6rX17BCmEgm7d3r7zT91ls+lL31lm/KBD/scMiQeCgbODDz9/KRUMQ69D/+\nuO8PBL4Wo1s3/8ezRYv4Uvp1rIkqG77+2te6JAtay5f7nyNRgwZ+6pLEZetWXxP0/vt+qo+YQw/d\nP5QNHAiHHJJ7NYPOwZIl8dA1fbqv7AaIROKha/jw7ASh9et9xfvLL8Mrr/jX4K9xrJbs2GNrbPfU\nCispiYevjz7yd8u4+WbffaA6ha8YhTCRIMW+0d96Kx7MFi/22woLfUeRWPPlMcfU6vtc1hTLl/sO\n/VOmwJdf+mCzY0fZ+zdoUH5QK/26YcPk4aa42N+iqqyQtXbt/nOg1a3rw1NiyOrUKf68ZcvUIWrj\nRt9fLhbK5s2LDyoGaN36wBqzLl2qVzBzDlav9mErFrxWrfLb2raNNy+OGBF+7WZJib/eL7/sa8r+\n+1///75GjXwZY6Gsc+dwyxmGkhI/59+tt/rw1auXD19nnVU9w1eMQphItq1bFw9lb73l/3oVF/tv\nisMPj4ey447zVQmS83bv9mEstmzcuP/rZOs2by77eHXr7h/K6tTxTYaff+6bs2LMfNfE0rVZsaVt\n28z/gdq2zVcGJwazjz7yYQF8Z/P+/fcPZ4cd5n+GqnLOB96NG8teYtc6cYmVrWVLH7ZitV3dulWv\nwFja1q0+NMY6+K9c6dd37+7DWK9evrauTRt/n9g2bdLvp5YrSkp8/8xbb/XNxj17xsNXLgyYUQgT\nCdu2bX54Vaz5cvZs3/kfoGvX/UNZ9+7V+6+CZMzevT6IpRPe9uw5sEarc2c/2q46tHjv3u3vzhAL\nZe+/74Pa7t1+e2Gh//9HLJQNHOj78JSUpBeiEpevv05ehry8eK1i4tKihb8TxPHH+3NW51qTVJzz\ntZCxWrLp0+PXN1GDBvFAFnss63nQ902sipIS+Oc/ffhauNCHr4kT/Xx+uRC+YhTCRKqbPXv8X6nE\nfmUbN/ptrVv7ce9HH+0fBw1SE6bkpL17/VQBiTVm77/va3fA/18j1Z+g5s2TB6rS62JL06a5G7Aq\nY+9e33/syy995fu6dWU/L90XMKZhw/KDWps2/q5v9er52syg/49YUgLPPefD14cf+vFOEyfCOefk\nVviKUQgTqe6c83+tYs2Xs2f71+C/8Xr18qEstvTunZm2HZEsKynxfdfef9/XbtSrlzxQNWumj3gm\n7dnjA1uqoBZ7/tVXqY+Vn+8HCRQU+N9RRZ6ns9/06f5etocd5sPXuHG5Gb5iFMJEctHmzfDuu74Z\nM7bE/jvboIGvIUsMZu3ahVteEakRvvnmwMC2caNfv2ePr4Hbs6fqz8va1qED3HADjB+f2+ErRiFM\npCZwDpYt87VksVD2/vvxXtrt2sWbMY8+Go44oub10BURyTGpQpgqe0VyhZnvxN+1q7+XDvheufPn\n719b9uyzflt+vu+FnBjMNJmsiEi1oZowkZpmw4b9Q9m778an/27cGI48Mt7p/6ijNC23iEiA1Bwp\nUpuVlPhx7YnNmAsW+HnLwIewvn19rVnfvn7p3bt6j10XEckRCmEisr+dO/18Ae+95wPZwoV+FsRd\nu/z2WNNnLJTFlkikZvSUFRHJEvUJE5H91a8fnxw2prjYd/z/8MP9l+ef97Vp4Gfc7NVr/1qzvn1z\n8waCIiIhU02YiKS2a5e/J00slC1c6B+/+CK+T/PmB9aa9enjb3gnIlKLqSZMRCrvoIP8dBdHHLH/\n+o0b44Estjz8MGzfHt+nU6d4rVmfPn72xe7dFc5ERFAIE5HKatkShg/3S0xJib/DcGKN2Ycf+pvd\nxe6gDL75snv3eCiLPXbu7KfMFhGpBRTCRCRz8vLid5YeMya+/uuvYckSP0rz00/98tlnfk6zxJva\n1anjBwQkC2ht2qjfmYjUKAphIhK8evX8tBe9ex+4bdMmH8hKB7RXXvHhLaZx43goSwxo3brpzgAi\nkpPUMV9EqqfiYli1Kh7KEh8//3z/fdu337/WrHt3H846dVLzpoiESh3zRST35Of7ENWpE3zrW/tv\n27nTN2+WDmhPPglbthx4jG7d/Bxn3brFnyugiUjIFMJEJPfUrw/9+vklkXP+tk2LF/uQtnhx/Pnb\nb8O2bfF98/N937XS4SxWg1ZHX48iEix9y4hIzWEGrVv7ZciQ/bc5B+vXHxjOFi+Gt97af2qNOnXi\nNWila9EOPVQBTUQyQt8kIlI7mPkRlm3alB3QSoezxYvhzTcPDGidO+8fziIRvxx6qJo4RSRtCmEi\nIokBLfFWTuAD2rp1yZs433gDduyI7xvrg1Y6nEUiPrjVrZvVH0tEqjeFMBGRVMzg4IP9MnTo/tti\nAW3JkgOXWbNg69b4vnl50LHj/sEstnTt6u/LKSK1ikKYiEhlJQa0ZDVomzbFQ1ms9mzJEnj6afjq\nq/2P07592QFN86CJ1EgKYSIiQTDzt3Zq2RIGDz5w+1dfwdKlB9agPf+875+W6JBDoEsX/9i2rX9M\nXNq29TdR1x0FRHKKQpiISBiaN/fLkUceuG3r1gPD2fLl/j6cr7yyfzNnTN26vkaudDgrHdhatfJ9\n10QkdAphIiLVTePGMHCgX5LZsQO+/BLWroUvvogvsdeLF8PMmfs3ecbk5/sBCMlq0xJft26tgQQi\nAVMIExHJNQ0a+L5iXbum3m/3bh/WkgW1L77wt4V65x0/wW0yLVrE+7ylWpo39wMPRKRCFMJERGqq\nwsL4rZ9S2bPHj/KMhbR163x4S1z++1+/fffuA99fp46vXSsdzpKta9hQfddEohTCRERqu4ICPzqz\nffvU+znnb/1UOqAlLmvXwrx5fnBBcfGBx6hff/9QFjtvhw7xpW1b3ZVAagV9ykVEJD1mvr9a48bQ\nvXvqfYuL/RQdyWrVYsvHH8N//rP/PT3BN20ecsj+waz00qaNmkAl5ymEiYhI5uXnx+/j2bdv6n2L\ninz/tGTL/PnwwgsHNoMWFEC7dqmDWosWavqUak0hTEREwtWkiV/69Em+PTbx7apVsHr1gUFt1iz4\nxz9837ZEhYX7N3WW1VetWTOFNQmFQpiIiFRviRPfDhiQfJ+SEt8PrawatenTfdPoN98c+N6Cgngw\nKx3QEl+3aQONGimwScYohImISO7Ly4uHpWQT4IKvUduyZf9+aqX7rJU3sOCgg8oOaInPW7fW7aak\nXAphIiJSO5j5psdmzaBHj9T7lpT4JtCywlrsxu1vvQUbNyY/RoMGPozFQlni89LrNNdaraQQJiIi\nUlpenr/FU6tW5Q8s2LPHT3gbC2jr1/uQlvi4ciW8+67fL1kNW36+P1dZYS0xtLVu7fu7Sc5TCBMR\nEamKggI/t1nbtuXvW1Libye1fn3ysBZ7vmyZf9yxI/lxGjc+sDatrODWpIn6sVVTCmEiIiLZkpcX\nH2TQq1f5++/Y4WvPygpr69fDZ5/Fm0WdO/AYdeuWH9Rijy1baqLcLNKVFhERqa4aNPBLebeeAti7\n1wexZDVsiY8LFvjnyUaKxkaiJgtorVrFA2TsufqyVYlCmIiISE1Qp058hGZ5nPOT5JYX2ObM8c+3\nbk1+nLw8H8QSg1mysJb4XKNG91EIExERqW3MoGlTv5R3CyrwdyzYuDG+bNiw/2Ps+eLF/mbvGzcm\nH4AAfpqPVCGtRYv4BL6Jy0EH1bi+bQphIiIiklrs7gPl3eQ9JjYnW3mhbeNGP9XHxo1l17bF1KkT\nD2RNmyYPasmWxH0LC6tVkFMIExERkcxKnJOtW7f03vPNNz6MffWVbyqNLVu27P86cVm6NP5869bk\nAxMSFRTsH9DGj4drr636z1tJCmEiIiISvrp105/qI5mSEti+PXVoK70cdFBmf4YKCjSEmdko4P+A\nfODPzrk7S22/BrgY2AtsAL7vnFsZZJlERESkBsrL8/OnNW4cdknSFti4UjPLB+4DTgF6AeeaWelJ\nUd4HBjnn+gHPAL8OqjwiIiIi1UmQk3scBSxxzi1zzn0DPAWcnriDc266c25n9OVsIM0efyIiIiK5\nLcgQ1g5YlfB6dXRdWX4AvJRsg5ldamZzzGzOhg0bMlhEERERkXBUi2luzWwCMAj4TbLtzrkHnXOD\nnHODWrVqld3CiYiIiAQgyI75a4AOCa/bR9ftx8xOBH4ODHPOfR1geURERESqjSBrwt4DuplZZzOr\nC4wHpiTuYGYDgD8BY5xz6wMsi4iIiEi1ElgIc87tBa4EpgEfA0875xaZ2W1mNia622+AhsA/zGy+\nmU0p43AiIiIiNUqg84Q556YCU0utm5jw/MQgzy8iIiJSXVWLjvkiIiIitY1CmIiIiEgIFMJERERE\nQqAQJiIiIhIChTARERGREJhzLuwyVIiZbQBWZuFULYGNWThPdafrEKdrEadrEadr4ek6xOlaxOla\nwKHOuaS3+8m5EJYtZjbHOTco7HKETdchTtciTtciTtfC03WI07WI07VITc2RIiIiIiFQCBMREREJ\ngUJY2R4MuwDVhK5DnK5FnK5FnK6Fp+sQp2sRp2uRgvqEiYiIiIRANWEiIiIiIajVIczMRpnZp2a2\nxMyuT7K9npn9Pbr9HTPrlP1SBs/MOpjZdDP7yMwWmdmPk+wz3MyKzGx+dJmY7Fg1gZmtMLMPoz/n\nnCTbzczuiX4uFpjZwDDKGTQzOyzh9z3fzLaa2dWl9qmxnwsz+6uZrTezhQnrmpvZf8xscfSxWRnv\nvSC6z2IzuyB7pc68OQ5KFgAABuxJREFUMq7Db8zsk+jn/zkza1rGe1P+W8o1ZVyLW8xsTcK/gdFl\nvDfl35tcU8a1+HvCdVhhZvPLeG+N+lxUiXOuVi5APrAU6ALUBT4AepXa5wrggejz8cDfwy53QNfi\nEGBg9Hkj4LMk12I48O+wy5ql67ECaJli+2jgJcCAwcA7YZc5C9ckH/gSP99NrfhcAMcDA4GFCet+\nDVwffX49cFeS9zUHlkUfm0WfNwv758nwdTgZqBN9fley6xDdlvLfUq4tZVyLW4CflvO+cv/e5NqS\n7FqU2v5bYGJt+FxUZanNNWFHAUucc8ucc98ATwGnl9rndOCR6PNngJFmZlksY1Y4575wzs2LPt8G\nfAy0C7dU1drpwKPOmw00NbNDwi5UwEYCS51z2ZgouVpwzs0Eviq1OvE74RHgO0ne+i3gP865r5xz\nm4H/AKMCK2jAkl0H59wrzrm90ZezgfZZL1gIyvhMpCOdvzc5JdW1iP6dPAd4MquFykG1OYS1A1Yl\nvF7NgcFj3z7RL5wioEVWSheSaJPrAOCdJJuPMbMPzOwlM+ud1YJllwNeMbO5ZnZpku3pfHZqmvGU\n/YVaWz4XAG2cc19En38JtEmyT237fHwfXzOcTHn/lmqKK6NNs38to4m6tn0mhgLrnHOLy9heWz4X\n5arNIUxKMbOGwLPA1c65raU2z8M3RR0O3Av8K9vly6LjnHMDgVOAH5nZ8WEXKExmVhcYA/wjyeba\n9LnYj/PtKrV6eLmZ/RzYCzxRxi614d/S/UBXoD/wBb4ZrrY7l9S1YLXhc5GW2hzC1gAdEl63j65L\nuo+Z1QGaAJuyUrosM7MCfAB7wjn3z9LbnXNbnXPbo8+nAgVm1jLLxcwK59ya6ON64Dl8U0KidD47\nNckpwDzn3LrSG2rT5yJqXazpOfq4Psk+teLzYWYXAt8GzosG0gOk8W8p5znn1jnnip1zJcBDJP8Z\na8VnAvb9rTwD+HtZ+9SGz0W6anMIew/oZmado//THw9MKbXPFCA2suks4PWyvmxyWbT9/i/Ax865\n35Wxz8Gx/nBmdhT+s1PjAqmZNTCzRrHn+A7IC0vtNgU4PzpKcjBQlNBEVROV+b/a2vK5SJD4nXAB\n8HySfaYBJ5tZs2jT1MnRdTWGmY0CfgaMcc7tLGOfdP4t5bxS/UHHkvxnTOfvTU1xIvCJc251so21\n5XORtrBHBoS54Ee5fYYftfLz6Lrb8F8sAIX4JpglwLtAl7DLHNB1OA7frLIAmB9dRgOXAZdF97kS\nWIQf1TMbODbscgd0LbpEf8YPoj9v7HOReC0MuC/6ufkQGBR2uQO8Hg3woapJwrpa8bnAB88vgD34\nPjw/wPcJfQ1YDLwKNI/uOwj4c8J7vx/93lgCXBT2zxLAdViC7+MU+76IjSJvC0yNPk/6bymXlzKu\nxWPR74EF+GB1SOlrEX19wN+bXF6SXYvo+odj3w8J+9boz0VVFs2YLyIiIhKC2twcKSIiIhIahTAR\nERGRECiEiYiIiIRAIUxEREQkBAphIiIiIiFQCBMRAcxsuJn9O+xyiEjtoRAmIiIiEgKFMBHJGWY2\nwczeNbP5ZvYnM8uPrt9uZr83s0Vm9pqZtYqu729ms6M3V34udnNlM4uY2avRG4/PM7Ou0VM0NLNn\nzOwTM3sidjeAUmWYYWZ3RcvxmZkNja4vNLPJZvahmb1vZiOydFlEJEcphIlITjCznsA4YIhzrj9Q\nDJwX3dyA/9/e3btGEUVhGH9OCCgiKBbiB6hYpFAURUihwUL/AYtUQtBeEDsRBMHWWvuI1nYWikVQ\nLPyAoCJaCulFVDCFvhZzl6SJrHFhzPL8qplzL3fPNMvhznAPvEpyGFgAbrT4XeBqkqN0p5oP4veB\n2+kaj5+kO/kb4DhwBThEd7L3qTXSmUwy3eYO1rxE19f7CF2rp/mq2vxvTy1pnFmESdoozgIngJdV\ntdjuD7axX6w0DL4HzFTVNmB7koUWnwdOt751e5M8AEjyIyv9D18kWUrXjHkROLBGLoMm969XzZlp\nv02SD8AnYGr9jytp3E32nYAkDamA+STXhpi73n5sy6uuf7L2f+TyEHMk6Y/cCZO0UTwBZqtqJ0BV\n7aiq/W1sApht1+eBZ0m+AJ8H32wBc8BCkq/AUlWda+tsqqotI8jvKe31aFVNAfuAjyNYV9KYsgiT\ntCEkeQ9cBx5V1RvgMbC7DX8HpqvqHXAGuNniF4Bbbf6xVfE54HKLPwd2jSDFO8BEVb2lezV6Mcly\nVe2pqocjWF/SmKlkvbv2kvR/qKpvSbb2nYck/Q13wiRJknrgTpgkSVIP3AmTJEnqgUWYJElSDyzC\nJEmSemARJkmS1AOLMEmSpB5YhEmSJPXgNxGQ+bTAHneMAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtAuFBvK99I9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#save model with best loss\n",
        "torch.save(best_model.state_dict(), '../content/gdrive/My Drive/models/SVHM_model.pth')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TXaeqk499Kj",
        "colab_type": "code",
        "outputId": "a32a4141-e5c9-4e25-bf7c-85ff93e0abee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "#download and test model\n",
        "model = LeNetBN()\n",
        "model.load_state_dict(torch.load('../content/gdrive/My Drive/models/SVHM_model.pth'))\n",
        "#print(model1)\n",
        "images, labels = next(iter(test_loader_SVHM))\n",
        "plt.imshow(images[0][0],'gray')\n",
        "image = images[0,:]#.to(device)\n",
        "image = image[None]\n",
        "plt.show()\n",
        "score = model(image)\n",
        "prob = nn.functional.softmax(score[0], dim=0)\n",
        "y_pred =  prob.argmax()\n",
        "print(\"Predicted class {} with probability {}\".format(y_pred, prob[y_pred]))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWgAAAFlCAYAAADGe3ILAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nyg8GwB4oAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Predicted class 5 with probability 0.998762845993042\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHZWLiNkUHjk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "test = pd.read_csv(\"../content/gdrive/My Drive/MNIST/test.csv\")\n",
        "test_image = test.loc[:,test.columns != \"label\"]\n",
        "test_dataset = torch.from_numpy(np.reshape(test_image.to_numpy().astype(np.uint8), (test_image.shape[0], 1, 28,28)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyFHcxJyqDLR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results = []\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    for img in test_dataset:\n",
        "        img = transforms.ToPILImage()(img)\n",
        "        img = transforms.Resize((32, 32))(img)\n",
        "        img = transforms.ToTensor()(img)\n",
        "        img = transforms.Normalize((0.4514, ), (0.1993, ))(img)\n",
        "        test_im = img#.to(device)\n",
        "        test_im = test_im[None]\n",
        "        output = model(test_im)\n",
        "        prob = nn.functional.softmax(output[0], dim=0)\n",
        "        y_pred =  prob.argmax()\n",
        "        results.append( y_pred.cpu().data.numpy().tolist())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIaobUUtqPEC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d46a1b45-63c4-4e86-e53e-2939bb2940f0"
      },
      "source": [
        "len(results)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cHoRVscqRDl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = np.array(results).flatten()\n",
        "submissions=pd.DataFrame({\"ImageId\": list(range(1,len(predictions)+1)),\n",
        "                         \"Label\": predictions})\n",
        "submissions.to_csv(\"../content/gdrive/My Drive/MNIST/my_submissions02.csv\", index=False, header=True)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}