{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TrainDigitRecognizer_MNIST_SVHM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_fPhAC9u2FE",
        "colab_type": "code",
        "outputId": "237b3816-c168-473e-9589-d9c718cb1173",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "#use Google Coolab with free GPU to train model\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWQfJ0MPvGXJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import time\n",
        "import copy\n",
        "\n",
        "from typing import Iterable\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MIBzDDPvI80",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#LeNet with Batch Normalization\n",
        "class LeNetBN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # convolution layers\n",
        "        self._body = nn.Sequential(\n",
        "            # First convolution Layer\n",
        "            # input size = (32, 32), output size = (28, 28)\n",
        "            nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5),\n",
        "            nn.BatchNorm2d(6),\n",
        "            # ReLU activation\n",
        "            nn.ReLU(inplace=True),\n",
        "            # Max pool 2-d\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            \n",
        "            # Second convolution layer\n",
        "            # input size = (14, 14), output size = (10, 10)\n",
        "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            # output size = (5, 5)\n",
        "        )\n",
        "        \n",
        "        # Fully connected layers\n",
        "        self._head = nn.Sequential(\n",
        "            # First fully connected layer\n",
        "            # in_features = total number of weight in last conv layer = 16 * 5 * 5\n",
        "            nn.Linear(in_features=16 * 5 * 5, out_features=120), \n",
        "            \n",
        "            # ReLU activation\n",
        "            nn.ReLU(inplace=True),\n",
        "            \n",
        "            # second fully connected layer\n",
        "            # in_features = output of last linear layer = 120 \n",
        "            nn.Linear(in_features=120, out_features=84), \n",
        "            \n",
        "            # ReLU activation\n",
        "            nn.ReLU(inplace=True),\n",
        "            \n",
        "            # Third fully connected layer. It is also output layer\n",
        "            # in_features = output of last linear layer = 84\n",
        "            # and out_features = number of classes = 10 (0-9)\n",
        "            nn.Linear(in_features=84, out_features=10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # apply feature extractor\n",
        "        x = self._body(x)\n",
        "        # flatten the output of conv layers\n",
        "        # dimension should be batch_size * number_of weight_in_last conv_layer\n",
        "        x = x.view(x.size()[0], -1)\n",
        "        # apply classification head\n",
        "        x = self._head(x)\n",
        "        return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0skH0fkYQSsj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "c5bf8b67-3edd-43dd-fbcd-9b8d87ab6f79"
      },
      "source": [
        "model = LeNetBN()\n",
        "print(model)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LeNetBN(\n",
            "  (_body): Sequential(\n",
            "    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (_head): Sequential(\n",
            "    (0): Linear(in_features=400, out_features=120, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Linear(in_features=120, out_features=84, bias=True)\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): Linear(in_features=84, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRAT2WWsvWQz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_test_transforms_SVHM = transforms.Compose([\n",
        "    #convert to grayscale                                             \n",
        "    transforms.Grayscale(1),\n",
        "    #re-scale image tensor values between 0-1. image_tensor /= 255\n",
        "    transforms.ToTensor(),\n",
        "    # subtract mean (0.3026) and divide by variance (0.2884).\n",
        "    # This mean and variance is calculated on training data \n",
        "    transforms.Normalize((0.3026, ), (0.2884, ))\n",
        "])\n",
        "\n",
        "train_test_transforms_MNIST = transforms.Compose([\n",
        "    #resize to (32,32)                                              \n",
        "    transforms.Resize((32, 32)),\n",
        "    # re-scale image tensor values between 0-1. image_tensor /= 255\n",
        "    transforms.ToTensor(),\n",
        "    # subtract mean (0.3026) and divide by variance (0.2884).\n",
        "    # This mean and variance is calculated on training data\n",
        "    transforms.Normalize((0.3026, ), (0.2884, ))\n",
        "])\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIVZRdzk3vZq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#download data from Google Drive\n",
        "trainSVHM = datasets.SVHN('../content/gdrive/My Drive/SVHN', split='train', download=False, transform=train_test_transforms_SVHM)\n",
        "testSVHM = datasets.SVHN('../content/gdrive/My Drive/SVHN', split='test', download=False, transform=train_test_transforms_SVHM)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Jzggwr8TOwo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#download data from Google Drive\n",
        "trainMNIST = datasets.MNIST('../content/gdrive/My Drive', train=True, download=False, transform=train_test_transforms_MNIST)\n",
        "testMNIST = datasets.MNIST('../content/gdrive/My Drive', train=False, download=False, transform=train_test_transforms_MNIST)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kk2gufzwQfgs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.manual_seed(42)\n",
        "#if torch.cuda.is_available():\n",
        "#    torch.backend.cudnn_benchmark_enabled = True\n",
        "#    torch.backend.cudnn.deterministic = True\n",
        "\n",
        "#training parameters\n",
        "batch_size = 32\n",
        "epochs_count = 20\n",
        "learning_rate = 0.01\n",
        "log_interval = 100\n",
        "test_interval = 1\n",
        "num_workers  = 10\n",
        "device = 'cuda'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhG-KqsEviSQ",
        "colab_type": "code",
        "outputId": "66218ea6-a557-4a7f-8683-f8fa8e07544f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "    print('GPU')\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "    batch_size = 16\n",
        "    num_workers = 0\n",
        "    epochs_count = 10\n",
        "    print('CPU')\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "#SGDoptimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "#array for plotting\n",
        "best_loss = torch.tensor(np.inf)\n",
        "\n",
        "# epoch train/test loss\n",
        "epoch_train_loss = np.array([])\n",
        "epoch_test_loss = np.array([])\n",
        "\n",
        "# epch train/test accuracy\n",
        "epoch_train_acc = np.array([])\n",
        "epoch_test_acc = np.array([])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOdWeCXxvkaC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train dataloader\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "     trainMNIST + trainSVHM,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=num_workers\n",
        ")\n",
        "# test dataloader\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    testMNIST + testSVHM,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=num_workers\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VE6H7m-svksn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, optimizer, train_loader, epoch_idx):\n",
        "    # change model in training mood\n",
        "    model.train()\n",
        "\n",
        "    # to get batch loss\n",
        "    batch_loss = np.array([])\n",
        "\n",
        "    # to get batch accuracy\n",
        "    batch_acc = np.array([])\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "\n",
        "        # clone target\n",
        "        indx_target = target.clone()\n",
        "        # send data to device (its is medatory if GPU has to be used)\n",
        "        data = data.to(device)\n",
        "        # send target to device\n",
        "        target = target.to(device)\n",
        "\n",
        "        # reset parameters gradient to zero\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward pass to the model\n",
        "        output = model(data)\n",
        "\n",
        "        # cross entropy loss\n",
        "        loss = F.cross_entropy(output, target)\n",
        "\n",
        "        # find gradients w.r.t training parameters\n",
        "        loss.backward()\n",
        "        # Update parameters using gardients\n",
        "        optimizer.step()\n",
        "\n",
        "        batch_loss = np.append(batch_loss, [loss.item()])\n",
        "\n",
        "        # Score to probability using softmax\n",
        "        prob = F.softmax(output, dim=1)\n",
        "\n",
        "        # get the index of the max probability\n",
        "        pred = prob.data.max(dim=1)[1]\n",
        "\n",
        "        # correct prediction\n",
        "        correct = pred.cpu().eq(indx_target).sum()\n",
        "\n",
        "        # accuracy\n",
        "        acc = float(correct) / float(len(data))\n",
        "\n",
        "        batch_acc = np.append(batch_acc, [acc])\n",
        "\n",
        "        if batch_idx % log_interval == 0 and batch_idx > 0:\n",
        "            print(\n",
        "                'Train Epoch: {} [{}/{}] Loss: {:.6f} Acc: {:.4f}'.format(\n",
        "                    epoch_idx, batch_idx * len(data), len(train_loader.dataset), loss.item(), acc\n",
        "                )\n",
        "            )\n",
        "\n",
        "    epoch_loss = batch_loss.mean()\n",
        "    epoch_acc = batch_acc.mean()\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "\n",
        "def validate(model, test_loader):\n",
        "    #\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    count_corect_predictions = 0\n",
        "    for data, target in test_loader:\n",
        "        indx_target = target.clone()\n",
        "        data = data.to(device)\n",
        "\n",
        "        target = target.to(device)\n",
        "\n",
        "        output = model(data)\n",
        "        # add loss for each mini batch\n",
        "        test_loss += F.cross_entropy(output, target).item()\n",
        "\n",
        "        # Score to probability using softmax\n",
        "        prob = F.softmax(output, dim=1)\n",
        "\n",
        "        # get the index of the max probability\n",
        "        pred = prob.data.max(dim=1)[1]\n",
        "\n",
        "        # add correct prediction count\n",
        "        count_corect_predictions += pred.cpu().eq(indx_target).sum()\n",
        "\n",
        "    # average over number of mini-batches\n",
        "    test_loss = test_loss / len(test_loader)\n",
        "\n",
        "    # average over number of dataset\n",
        "    accuracy = 100. * count_corect_predictions / len(test_loader.dataset)\n",
        "\n",
        "    print(\n",
        "        '\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "            test_loss, count_corect_predictions, len(test_loader.dataset), accuracy\n",
        "        )\n",
        "    )\n",
        "    return test_loss, accuracy / 100.0\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjythJS-vkv_",
        "colab_type": "code",
        "outputId": "176b5e85-0edb-4767-9e84-fd807a1279fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#train\n",
        "t_begin = time.time()\n",
        "#for save best model\n",
        "best_model = copy.deepcopy(model)\n",
        "for epoch in range(epochs_count):\n",
        "\n",
        "    train_loss, train_acc = train(model, optimizer, train_loader, epoch)\n",
        "\n",
        "    epoch_train_loss = np.append(epoch_train_loss, [train_loss])\n",
        "\n",
        "    epoch_train_acc = np.append(epoch_train_acc, [train_acc])\n",
        "\n",
        "    elapsed_time = time.time() - t_begin\n",
        "    speed_epoch = elapsed_time / (epoch + 1)\n",
        "    speed_batch = speed_epoch / len(train_loader)\n",
        "    eta = speed_epoch * epochs_count - elapsed_time\n",
        "\n",
        "    print(\n",
        "        \"Elapsed {:.2f}s, {:.2f} s/epoch, {:.2f} s/batch, ets {:.2f}s\".format(\n",
        "            elapsed_time, speed_epoch, speed_batch, eta\n",
        "        )\n",
        "    )\n",
        "\n",
        "    if epoch % test_interval == 0:\n",
        "        current_loss, current_accuracy = validate(model, test_loader)\n",
        "\n",
        "        epoch_test_loss = np.append(epoch_test_loss, [current_loss])\n",
        "\n",
        "        epoch_test_acc = np.append(epoch_test_acc, [current_accuracy])\n",
        "\n",
        "        if current_loss < best_loss:\n",
        "            best_loss = current_loss\n",
        "            best_model = copy.deepcopy(model)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [3200/133257] Loss: 2.211398 Acc: 0.2500\n",
            "Train Epoch: 0 [6400/133257] Loss: 2.056465 Acc: 0.3750\n",
            "Train Epoch: 0 [9600/133257] Loss: 1.893609 Acc: 0.4375\n",
            "Train Epoch: 0 [12800/133257] Loss: 1.580754 Acc: 0.3750\n",
            "Train Epoch: 0 [16000/133257] Loss: 1.413705 Acc: 0.5312\n",
            "Train Epoch: 0 [19200/133257] Loss: 1.308830 Acc: 0.5312\n",
            "Train Epoch: 0 [22400/133257] Loss: 1.281905 Acc: 0.5938\n",
            "Train Epoch: 0 [25600/133257] Loss: 1.277649 Acc: 0.5938\n",
            "Train Epoch: 0 [28800/133257] Loss: 1.178882 Acc: 0.6562\n",
            "Train Epoch: 0 [32000/133257] Loss: 0.942455 Acc: 0.6875\n",
            "Train Epoch: 0 [35200/133257] Loss: 1.049977 Acc: 0.7500\n",
            "Train Epoch: 0 [38400/133257] Loss: 0.803896 Acc: 0.7812\n",
            "Train Epoch: 0 [41600/133257] Loss: 1.024963 Acc: 0.6250\n",
            "Train Epoch: 0 [44800/133257] Loss: 0.545605 Acc: 0.8438\n",
            "Train Epoch: 0 [48000/133257] Loss: 0.762905 Acc: 0.7188\n",
            "Train Epoch: 0 [51200/133257] Loss: 0.771202 Acc: 0.8438\n",
            "Train Epoch: 0 [54400/133257] Loss: 0.764981 Acc: 0.8438\n",
            "Train Epoch: 0 [57600/133257] Loss: 0.387808 Acc: 0.8750\n",
            "Train Epoch: 0 [60800/133257] Loss: 0.435656 Acc: 0.8438\n",
            "Train Epoch: 0 [64000/133257] Loss: 0.558810 Acc: 0.7812\n",
            "Train Epoch: 0 [67200/133257] Loss: 0.623497 Acc: 0.8125\n",
            "Train Epoch: 0 [70400/133257] Loss: 0.456623 Acc: 0.8438\n",
            "Train Epoch: 0 [73600/133257] Loss: 0.713245 Acc: 0.8125\n",
            "Train Epoch: 0 [76800/133257] Loss: 0.668402 Acc: 0.8438\n",
            "Train Epoch: 0 [80000/133257] Loss: 0.471764 Acc: 0.8438\n",
            "Train Epoch: 0 [83200/133257] Loss: 0.864881 Acc: 0.7500\n",
            "Train Epoch: 0 [86400/133257] Loss: 0.579597 Acc: 0.7812\n",
            "Train Epoch: 0 [89600/133257] Loss: 0.250694 Acc: 0.9688\n",
            "Train Epoch: 0 [92800/133257] Loss: 0.548112 Acc: 0.8750\n",
            "Train Epoch: 0 [96000/133257] Loss: 0.771862 Acc: 0.7500\n",
            "Train Epoch: 0 [99200/133257] Loss: 0.444411 Acc: 0.8125\n",
            "Train Epoch: 0 [102400/133257] Loss: 0.633604 Acc: 0.7812\n",
            "Train Epoch: 0 [105600/133257] Loss: 0.557807 Acc: 0.8750\n",
            "Train Epoch: 0 [108800/133257] Loss: 0.622917 Acc: 0.8438\n",
            "Train Epoch: 0 [112000/133257] Loss: 0.446461 Acc: 0.8438\n",
            "Train Epoch: 0 [115200/133257] Loss: 0.233350 Acc: 0.9062\n",
            "Train Epoch: 0 [118400/133257] Loss: 0.455197 Acc: 0.8125\n",
            "Train Epoch: 0 [121600/133257] Loss: 0.469854 Acc: 0.8750\n",
            "Train Epoch: 0 [124800/133257] Loss: 0.519429 Acc: 0.8125\n",
            "Train Epoch: 0 [128000/133257] Loss: 0.440015 Acc: 0.8750\n",
            "Train Epoch: 0 [131200/133257] Loss: 0.603584 Acc: 0.7500\n",
            "Elapsed 34.66s, 34.66 s/epoch, 0.01 s/batch, ets 658.59s\n",
            "\n",
            "Test set: Average loss: 0.5756, Accuracy: 30046/36032 (83%)\n",
            "\n",
            "Train Epoch: 1 [3200/133257] Loss: 0.542069 Acc: 0.8438\n",
            "Train Epoch: 1 [6400/133257] Loss: 0.229343 Acc: 0.9375\n",
            "Train Epoch: 1 [9600/133257] Loss: 0.280164 Acc: 0.9375\n",
            "Train Epoch: 1 [12800/133257] Loss: 0.553918 Acc: 0.8125\n",
            "Train Epoch: 1 [16000/133257] Loss: 0.670739 Acc: 0.7812\n",
            "Train Epoch: 1 [19200/133257] Loss: 0.274739 Acc: 0.9375\n",
            "Train Epoch: 1 [22400/133257] Loss: 0.314700 Acc: 0.8750\n",
            "Train Epoch: 1 [25600/133257] Loss: 0.254439 Acc: 0.9375\n",
            "Train Epoch: 1 [28800/133257] Loss: 0.185159 Acc: 0.9688\n",
            "Train Epoch: 1 [32000/133257] Loss: 0.478684 Acc: 0.8750\n",
            "Train Epoch: 1 [35200/133257] Loss: 0.578722 Acc: 0.7812\n",
            "Train Epoch: 1 [38400/133257] Loss: 0.249139 Acc: 0.9688\n",
            "Train Epoch: 1 [41600/133257] Loss: 0.278461 Acc: 0.9062\n",
            "Train Epoch: 1 [44800/133257] Loss: 0.504787 Acc: 0.9062\n",
            "Train Epoch: 1 [48000/133257] Loss: 0.706000 Acc: 0.8438\n",
            "Train Epoch: 1 [51200/133257] Loss: 0.253173 Acc: 0.9375\n",
            "Train Epoch: 1 [54400/133257] Loss: 0.165880 Acc: 0.9375\n",
            "Train Epoch: 1 [57600/133257] Loss: 0.413503 Acc: 0.9062\n",
            "Train Epoch: 1 [60800/133257] Loss: 0.554766 Acc: 0.8750\n",
            "Train Epoch: 1 [64000/133257] Loss: 0.223818 Acc: 0.9688\n",
            "Train Epoch: 1 [67200/133257] Loss: 0.315094 Acc: 0.9375\n",
            "Train Epoch: 1 [70400/133257] Loss: 0.306355 Acc: 0.9062\n",
            "Train Epoch: 1 [73600/133257] Loss: 0.305061 Acc: 0.9062\n",
            "Train Epoch: 1 [76800/133257] Loss: 0.316545 Acc: 0.9375\n",
            "Train Epoch: 1 [80000/133257] Loss: 0.549712 Acc: 0.7812\n",
            "Train Epoch: 1 [83200/133257] Loss: 0.110517 Acc: 0.9688\n",
            "Train Epoch: 1 [86400/133257] Loss: 0.283054 Acc: 0.9062\n",
            "Train Epoch: 1 [89600/133257] Loss: 0.286978 Acc: 0.9062\n",
            "Train Epoch: 1 [92800/133257] Loss: 0.381090 Acc: 0.8750\n",
            "Train Epoch: 1 [96000/133257] Loss: 0.156780 Acc: 1.0000\n",
            "Train Epoch: 1 [99200/133257] Loss: 0.308009 Acc: 0.8750\n",
            "Train Epoch: 1 [102400/133257] Loss: 0.431821 Acc: 0.9375\n",
            "Train Epoch: 1 [105600/133257] Loss: 0.346888 Acc: 0.9375\n",
            "Train Epoch: 1 [108800/133257] Loss: 0.066971 Acc: 1.0000\n",
            "Train Epoch: 1 [112000/133257] Loss: 0.453243 Acc: 0.9062\n",
            "Train Epoch: 1 [115200/133257] Loss: 0.530610 Acc: 0.9062\n",
            "Train Epoch: 1 [118400/133257] Loss: 0.454246 Acc: 0.8438\n",
            "Train Epoch: 1 [121600/133257] Loss: 0.255468 Acc: 0.9375\n",
            "Train Epoch: 1 [124800/133257] Loss: 0.419629 Acc: 0.8438\n",
            "Train Epoch: 1 [128000/133257] Loss: 0.527512 Acc: 0.8750\n",
            "Train Epoch: 1 [131200/133257] Loss: 0.597036 Acc: 0.7812\n",
            "Elapsed 77.08s, 38.54 s/epoch, 0.01 s/batch, ets 693.70s\n",
            "\n",
            "Test set: Average loss: 0.4792, Accuracy: 31026/36032 (86%)\n",
            "\n",
            "Train Epoch: 2 [3200/133257] Loss: 0.298963 Acc: 0.9375\n",
            "Train Epoch: 2 [6400/133257] Loss: 0.369636 Acc: 0.8438\n",
            "Train Epoch: 2 [9600/133257] Loss: 0.613618 Acc: 0.8125\n",
            "Train Epoch: 2 [12800/133257] Loss: 0.537039 Acc: 0.8438\n",
            "Train Epoch: 2 [16000/133257] Loss: 0.347693 Acc: 0.9062\n",
            "Train Epoch: 2 [19200/133257] Loss: 0.693720 Acc: 0.8750\n",
            "Train Epoch: 2 [22400/133257] Loss: 0.385049 Acc: 0.9375\n",
            "Train Epoch: 2 [25600/133257] Loss: 0.194590 Acc: 1.0000\n",
            "Train Epoch: 2 [28800/133257] Loss: 0.349246 Acc: 0.8750\n",
            "Train Epoch: 2 [32000/133257] Loss: 0.417072 Acc: 0.8750\n",
            "Train Epoch: 2 [35200/133257] Loss: 0.432663 Acc: 0.9062\n",
            "Train Epoch: 2 [38400/133257] Loss: 0.198591 Acc: 0.9375\n",
            "Train Epoch: 2 [41600/133257] Loss: 0.112511 Acc: 0.9375\n",
            "Train Epoch: 2 [44800/133257] Loss: 0.286957 Acc: 0.9375\n",
            "Train Epoch: 2 [48000/133257] Loss: 0.323284 Acc: 0.9062\n",
            "Train Epoch: 2 [51200/133257] Loss: 0.147146 Acc: 0.9688\n",
            "Train Epoch: 2 [54400/133257] Loss: 0.303571 Acc: 0.9062\n",
            "Train Epoch: 2 [57600/133257] Loss: 0.239661 Acc: 0.9062\n",
            "Train Epoch: 2 [60800/133257] Loss: 0.334757 Acc: 0.8750\n",
            "Train Epoch: 2 [64000/133257] Loss: 1.074633 Acc: 0.8438\n",
            "Train Epoch: 2 [67200/133257] Loss: 0.869538 Acc: 0.8750\n",
            "Train Epoch: 2 [70400/133257] Loss: 0.376734 Acc: 0.9062\n",
            "Train Epoch: 2 [73600/133257] Loss: 0.279207 Acc: 0.9062\n",
            "Train Epoch: 2 [76800/133257] Loss: 0.364526 Acc: 0.8750\n",
            "Train Epoch: 2 [80000/133257] Loss: 0.290810 Acc: 0.9062\n",
            "Train Epoch: 2 [83200/133257] Loss: 0.328183 Acc: 0.9375\n",
            "Train Epoch: 2 [86400/133257] Loss: 0.349762 Acc: 0.8750\n",
            "Train Epoch: 2 [89600/133257] Loss: 0.418040 Acc: 0.9062\n",
            "Train Epoch: 2 [92800/133257] Loss: 0.462115 Acc: 0.8750\n",
            "Train Epoch: 2 [96000/133257] Loss: 0.271330 Acc: 0.9688\n",
            "Train Epoch: 2 [99200/133257] Loss: 0.436076 Acc: 0.8438\n",
            "Train Epoch: 2 [102400/133257] Loss: 0.177374 Acc: 0.9375\n",
            "Train Epoch: 2 [105600/133257] Loss: 0.584805 Acc: 0.8750\n",
            "Train Epoch: 2 [108800/133257] Loss: 0.614899 Acc: 0.8438\n",
            "Train Epoch: 2 [112000/133257] Loss: 0.093036 Acc: 1.0000\n",
            "Train Epoch: 2 [115200/133257] Loss: 0.231952 Acc: 0.8750\n",
            "Train Epoch: 2 [118400/133257] Loss: 0.500227 Acc: 0.8438\n",
            "Train Epoch: 2 [121600/133257] Loss: 0.491219 Acc: 0.9375\n",
            "Train Epoch: 2 [124800/133257] Loss: 0.101054 Acc: 1.0000\n",
            "Train Epoch: 2 [128000/133257] Loss: 0.424018 Acc: 0.8438\n",
            "Train Epoch: 2 [131200/133257] Loss: 0.151914 Acc: 1.0000\n",
            "Elapsed 119.32s, 39.77 s/epoch, 0.01 s/batch, ets 676.13s\n",
            "\n",
            "Test set: Average loss: 0.4188, Accuracy: 31622/36032 (88%)\n",
            "\n",
            "Train Epoch: 3 [3200/133257] Loss: 0.319098 Acc: 0.9062\n",
            "Train Epoch: 3 [6400/133257] Loss: 0.208464 Acc: 0.9062\n",
            "Train Epoch: 3 [9600/133257] Loss: 0.303666 Acc: 0.9375\n",
            "Train Epoch: 3 [12800/133257] Loss: 0.250238 Acc: 0.9375\n",
            "Train Epoch: 3 [16000/133257] Loss: 0.230608 Acc: 0.9375\n",
            "Train Epoch: 3 [19200/133257] Loss: 0.105565 Acc: 0.9688\n",
            "Train Epoch: 3 [22400/133257] Loss: 0.091767 Acc: 0.9688\n",
            "Train Epoch: 3 [25600/133257] Loss: 0.111306 Acc: 1.0000\n",
            "Train Epoch: 3 [28800/133257] Loss: 0.642780 Acc: 0.7500\n",
            "Train Epoch: 3 [32000/133257] Loss: 0.246727 Acc: 0.8750\n",
            "Train Epoch: 3 [35200/133257] Loss: 0.378808 Acc: 0.8438\n",
            "Train Epoch: 3 [38400/133257] Loss: 0.368740 Acc: 0.8438\n",
            "Train Epoch: 3 [41600/133257] Loss: 0.374279 Acc: 0.9062\n",
            "Train Epoch: 3 [44800/133257] Loss: 0.238280 Acc: 0.9375\n",
            "Train Epoch: 3 [48000/133257] Loss: 0.289087 Acc: 0.9062\n",
            "Train Epoch: 3 [51200/133257] Loss: 0.237893 Acc: 0.8750\n",
            "Train Epoch: 3 [54400/133257] Loss: 0.144121 Acc: 0.9375\n",
            "Train Epoch: 3 [57600/133257] Loss: 0.362651 Acc: 0.9062\n",
            "Train Epoch: 3 [60800/133257] Loss: 0.935622 Acc: 0.6875\n",
            "Train Epoch: 3 [64000/133257] Loss: 0.323981 Acc: 0.9375\n",
            "Train Epoch: 3 [67200/133257] Loss: 0.693237 Acc: 0.8438\n",
            "Train Epoch: 3 [70400/133257] Loss: 0.411370 Acc: 0.9062\n",
            "Train Epoch: 3 [73600/133257] Loss: 0.243860 Acc: 0.9062\n",
            "Train Epoch: 3 [76800/133257] Loss: 0.220432 Acc: 0.9375\n",
            "Train Epoch: 3 [80000/133257] Loss: 0.290637 Acc: 0.8750\n",
            "Train Epoch: 3 [83200/133257] Loss: 0.182348 Acc: 0.9375\n",
            "Train Epoch: 3 [86400/133257] Loss: 0.254733 Acc: 0.9062\n",
            "Train Epoch: 3 [89600/133257] Loss: 0.427769 Acc: 0.8438\n",
            "Train Epoch: 3 [92800/133257] Loss: 0.328094 Acc: 0.8750\n",
            "Train Epoch: 3 [96000/133257] Loss: 0.294121 Acc: 0.9062\n",
            "Train Epoch: 3 [99200/133257] Loss: 0.266925 Acc: 0.9688\n",
            "Train Epoch: 3 [102400/133257] Loss: 0.152189 Acc: 0.9062\n",
            "Train Epoch: 3 [105600/133257] Loss: 0.785977 Acc: 0.8438\n",
            "Train Epoch: 3 [108800/133257] Loss: 0.231603 Acc: 0.9688\n",
            "Train Epoch: 3 [112000/133257] Loss: 0.497234 Acc: 0.9062\n",
            "Train Epoch: 3 [115200/133257] Loss: 0.161677 Acc: 0.9688\n",
            "Train Epoch: 3 [118400/133257] Loss: 0.687318 Acc: 0.8750\n",
            "Train Epoch: 3 [121600/133257] Loss: 0.231042 Acc: 0.9375\n",
            "Train Epoch: 3 [124800/133257] Loss: 0.228126 Acc: 0.9688\n",
            "Train Epoch: 3 [128000/133257] Loss: 0.573909 Acc: 0.8750\n",
            "Train Epoch: 3 [131200/133257] Loss: 0.113201 Acc: 0.9688\n",
            "Elapsed 161.73s, 40.43 s/epoch, 0.01 s/batch, ets 646.92s\n",
            "\n",
            "Test set: Average loss: 0.3958, Accuracy: 31896/36032 (89%)\n",
            "\n",
            "Train Epoch: 4 [3200/133257] Loss: 0.527384 Acc: 0.8438\n",
            "Train Epoch: 4 [6400/133257] Loss: 0.265011 Acc: 0.9375\n",
            "Train Epoch: 4 [9600/133257] Loss: 0.184094 Acc: 0.9375\n",
            "Train Epoch: 4 [12800/133257] Loss: 0.256822 Acc: 0.9375\n",
            "Train Epoch: 4 [16000/133257] Loss: 0.099286 Acc: 0.9688\n",
            "Train Epoch: 4 [19200/133257] Loss: 0.371371 Acc: 0.9062\n",
            "Train Epoch: 4 [22400/133257] Loss: 0.179542 Acc: 0.9062\n",
            "Train Epoch: 4 [25600/133257] Loss: 0.125905 Acc: 0.9688\n",
            "Train Epoch: 4 [28800/133257] Loss: 0.301523 Acc: 0.9375\n",
            "Train Epoch: 4 [32000/133257] Loss: 0.211677 Acc: 0.9375\n",
            "Train Epoch: 4 [35200/133257] Loss: 0.186579 Acc: 0.9062\n",
            "Train Epoch: 4 [38400/133257] Loss: 0.221325 Acc: 0.9688\n",
            "Train Epoch: 4 [41600/133257] Loss: 0.233638 Acc: 0.9062\n",
            "Train Epoch: 4 [44800/133257] Loss: 0.176399 Acc: 0.9375\n",
            "Train Epoch: 4 [48000/133257] Loss: 0.489097 Acc: 0.8750\n",
            "Train Epoch: 4 [51200/133257] Loss: 0.272187 Acc: 0.8750\n",
            "Train Epoch: 4 [54400/133257] Loss: 0.167469 Acc: 0.9062\n",
            "Train Epoch: 4 [57600/133257] Loss: 0.164666 Acc: 0.9375\n",
            "Train Epoch: 4 [60800/133257] Loss: 0.238007 Acc: 0.9375\n",
            "Train Epoch: 4 [64000/133257] Loss: 0.336968 Acc: 0.9375\n",
            "Train Epoch: 4 [67200/133257] Loss: 0.255778 Acc: 0.9062\n",
            "Train Epoch: 4 [70400/133257] Loss: 0.258838 Acc: 0.8750\n",
            "Train Epoch: 4 [73600/133257] Loss: 0.349466 Acc: 0.9375\n",
            "Train Epoch: 4 [76800/133257] Loss: 0.246546 Acc: 0.8750\n",
            "Train Epoch: 4 [80000/133257] Loss: 0.142274 Acc: 0.9688\n",
            "Train Epoch: 4 [83200/133257] Loss: 0.094563 Acc: 0.9688\n",
            "Train Epoch: 4 [86400/133257] Loss: 0.216896 Acc: 0.9062\n",
            "Train Epoch: 4 [89600/133257] Loss: 0.792792 Acc: 0.9062\n",
            "Train Epoch: 4 [92800/133257] Loss: 0.453420 Acc: 0.8750\n",
            "Train Epoch: 4 [96000/133257] Loss: 0.476669 Acc: 0.9062\n",
            "Train Epoch: 4 [99200/133257] Loss: 0.136136 Acc: 0.9688\n",
            "Train Epoch: 4 [102400/133257] Loss: 0.149197 Acc: 0.9688\n",
            "Train Epoch: 4 [105600/133257] Loss: 0.448252 Acc: 0.7812\n",
            "Train Epoch: 4 [108800/133257] Loss: 0.335276 Acc: 0.8750\n",
            "Train Epoch: 4 [112000/133257] Loss: 0.349985 Acc: 0.8750\n",
            "Train Epoch: 4 [115200/133257] Loss: 0.437664 Acc: 0.8750\n",
            "Train Epoch: 4 [118400/133257] Loss: 0.313050 Acc: 0.9062\n",
            "Train Epoch: 4 [121600/133257] Loss: 0.184760 Acc: 0.9375\n",
            "Train Epoch: 4 [124800/133257] Loss: 0.341042 Acc: 0.8750\n",
            "Train Epoch: 4 [128000/133257] Loss: 0.376359 Acc: 0.9062\n",
            "Train Epoch: 4 [131200/133257] Loss: 0.218652 Acc: 0.9062\n",
            "Elapsed 204.13s, 40.83 s/epoch, 0.01 s/batch, ets 612.40s\n",
            "\n",
            "Test set: Average loss: 0.3808, Accuracy: 31979/36032 (89%)\n",
            "\n",
            "Train Epoch: 5 [3200/133257] Loss: 0.211246 Acc: 0.9062\n",
            "Train Epoch: 5 [6400/133257] Loss: 0.218623 Acc: 0.9688\n",
            "Train Epoch: 5 [9600/133257] Loss: 0.261604 Acc: 0.9375\n",
            "Train Epoch: 5 [12800/133257] Loss: 0.168431 Acc: 0.9688\n",
            "Train Epoch: 5 [16000/133257] Loss: 0.333686 Acc: 0.9062\n",
            "Train Epoch: 5 [19200/133257] Loss: 0.196739 Acc: 0.9062\n",
            "Train Epoch: 5 [22400/133257] Loss: 0.273486 Acc: 0.9062\n",
            "Train Epoch: 5 [25600/133257] Loss: 0.324055 Acc: 0.9062\n",
            "Train Epoch: 5 [28800/133257] Loss: 0.168157 Acc: 0.9688\n",
            "Train Epoch: 5 [32000/133257] Loss: 0.382543 Acc: 0.8750\n",
            "Train Epoch: 5 [35200/133257] Loss: 0.272949 Acc: 0.9062\n",
            "Train Epoch: 5 [38400/133257] Loss: 0.430949 Acc: 0.8438\n",
            "Train Epoch: 5 [41600/133257] Loss: 0.583388 Acc: 0.8750\n",
            "Train Epoch: 5 [44800/133257] Loss: 0.236515 Acc: 0.9688\n",
            "Train Epoch: 5 [48000/133257] Loss: 0.232098 Acc: 0.9375\n",
            "Train Epoch: 5 [51200/133257] Loss: 0.299310 Acc: 0.9375\n",
            "Train Epoch: 5 [54400/133257] Loss: 0.269815 Acc: 0.9062\n",
            "Train Epoch: 5 [57600/133257] Loss: 0.095570 Acc: 1.0000\n",
            "Train Epoch: 5 [60800/133257] Loss: 0.147636 Acc: 0.9375\n",
            "Train Epoch: 5 [64000/133257] Loss: 0.232086 Acc: 0.9375\n",
            "Train Epoch: 5 [67200/133257] Loss: 0.037543 Acc: 1.0000\n",
            "Train Epoch: 5 [70400/133257] Loss: 0.142140 Acc: 0.9375\n",
            "Train Epoch: 5 [73600/133257] Loss: 0.415867 Acc: 0.8750\n",
            "Train Epoch: 5 [76800/133257] Loss: 0.504072 Acc: 0.9375\n",
            "Train Epoch: 5 [80000/133257] Loss: 0.160223 Acc: 0.9375\n",
            "Train Epoch: 5 [83200/133257] Loss: 0.130177 Acc: 0.9688\n",
            "Train Epoch: 5 [86400/133257] Loss: 0.157883 Acc: 0.9062\n",
            "Train Epoch: 5 [89600/133257] Loss: 0.059452 Acc: 1.0000\n",
            "Train Epoch: 5 [92800/133257] Loss: 0.337658 Acc: 0.9375\n",
            "Train Epoch: 5 [96000/133257] Loss: 0.189693 Acc: 0.9375\n",
            "Train Epoch: 5 [99200/133257] Loss: 0.083865 Acc: 0.9688\n",
            "Train Epoch: 5 [102400/133257] Loss: 0.192154 Acc: 0.9688\n",
            "Train Epoch: 5 [105600/133257] Loss: 0.417946 Acc: 0.9375\n",
            "Train Epoch: 5 [108800/133257] Loss: 0.279648 Acc: 0.9062\n",
            "Train Epoch: 5 [112000/133257] Loss: 0.220512 Acc: 0.9375\n",
            "Train Epoch: 5 [115200/133257] Loss: 0.389687 Acc: 0.9062\n",
            "Train Epoch: 5 [118400/133257] Loss: 0.226099 Acc: 0.9062\n",
            "Train Epoch: 5 [121600/133257] Loss: 0.270812 Acc: 0.9375\n",
            "Train Epoch: 5 [124800/133257] Loss: 0.151991 Acc: 0.9688\n",
            "Train Epoch: 5 [128000/133257] Loss: 0.125364 Acc: 0.9375\n",
            "Train Epoch: 5 [131200/133257] Loss: 0.143452 Acc: 1.0000\n",
            "Elapsed 246.55s, 41.09 s/epoch, 0.01 s/batch, ets 575.29s\n",
            "\n",
            "Test set: Average loss: 0.3599, Accuracy: 32225/36032 (89%)\n",
            "\n",
            "Train Epoch: 6 [3200/133257] Loss: 0.195619 Acc: 0.8750\n",
            "Train Epoch: 6 [6400/133257] Loss: 0.098690 Acc: 0.9688\n",
            "Train Epoch: 6 [9600/133257] Loss: 0.501627 Acc: 0.9062\n",
            "Train Epoch: 6 [12800/133257] Loss: 0.118051 Acc: 0.9375\n",
            "Train Epoch: 6 [16000/133257] Loss: 0.194456 Acc: 0.9062\n",
            "Train Epoch: 6 [19200/133257] Loss: 0.321284 Acc: 0.9375\n",
            "Train Epoch: 6 [22400/133257] Loss: 0.279875 Acc: 0.9062\n",
            "Train Epoch: 6 [25600/133257] Loss: 0.086198 Acc: 0.9688\n",
            "Train Epoch: 6 [28800/133257] Loss: 0.071251 Acc: 0.9688\n",
            "Train Epoch: 6 [32000/133257] Loss: 0.403549 Acc: 0.8438\n",
            "Train Epoch: 6 [35200/133257] Loss: 0.086679 Acc: 1.0000\n",
            "Train Epoch: 6 [38400/133257] Loss: 0.079641 Acc: 0.9688\n",
            "Train Epoch: 6 [41600/133257] Loss: 0.117655 Acc: 0.9688\n",
            "Train Epoch: 6 [44800/133257] Loss: 0.243020 Acc: 0.9062\n",
            "Train Epoch: 6 [48000/133257] Loss: 0.411505 Acc: 0.8750\n",
            "Train Epoch: 6 [51200/133257] Loss: 0.261787 Acc: 0.9375\n",
            "Train Epoch: 6 [54400/133257] Loss: 0.076818 Acc: 1.0000\n",
            "Train Epoch: 6 [57600/133257] Loss: 0.430937 Acc: 0.8125\n",
            "Train Epoch: 6 [60800/133257] Loss: 0.105182 Acc: 0.9688\n",
            "Train Epoch: 6 [64000/133257] Loss: 0.282456 Acc: 0.8750\n",
            "Train Epoch: 6 [67200/133257] Loss: 0.154797 Acc: 0.9688\n",
            "Train Epoch: 6 [70400/133257] Loss: 0.324267 Acc: 0.8438\n",
            "Train Epoch: 6 [73600/133257] Loss: 0.344286 Acc: 0.8750\n",
            "Train Epoch: 6 [76800/133257] Loss: 0.391357 Acc: 0.9062\n",
            "Train Epoch: 6 [80000/133257] Loss: 0.138280 Acc: 0.9688\n",
            "Train Epoch: 6 [83200/133257] Loss: 0.110219 Acc: 0.9688\n",
            "Train Epoch: 6 [86400/133257] Loss: 0.140195 Acc: 0.9375\n",
            "Train Epoch: 6 [89600/133257] Loss: 0.342668 Acc: 0.8750\n",
            "Train Epoch: 6 [92800/133257] Loss: 0.246017 Acc: 0.9062\n",
            "Train Epoch: 6 [96000/133257] Loss: 0.255240 Acc: 0.9062\n",
            "Train Epoch: 6 [99200/133257] Loss: 0.162649 Acc: 0.9062\n",
            "Train Epoch: 6 [102400/133257] Loss: 0.279975 Acc: 0.8750\n",
            "Train Epoch: 6 [105600/133257] Loss: 0.090943 Acc: 1.0000\n",
            "Train Epoch: 6 [108800/133257] Loss: 0.306364 Acc: 0.9375\n",
            "Train Epoch: 6 [112000/133257] Loss: 0.301512 Acc: 0.9062\n",
            "Train Epoch: 6 [115200/133257] Loss: 0.140548 Acc: 1.0000\n",
            "Train Epoch: 6 [118400/133257] Loss: 0.312736 Acc: 0.9375\n",
            "Train Epoch: 6 [121600/133257] Loss: 0.237293 Acc: 0.9062\n",
            "Train Epoch: 6 [124800/133257] Loss: 0.346906 Acc: 0.9062\n",
            "Train Epoch: 6 [128000/133257] Loss: 0.095391 Acc: 0.9688\n",
            "Train Epoch: 6 [131200/133257] Loss: 0.253154 Acc: 0.9375\n",
            "Elapsed 289.03s, 41.29 s/epoch, 0.01 s/batch, ets 536.77s\n",
            "\n",
            "Test set: Average loss: 0.3282, Accuracy: 32627/36032 (91%)\n",
            "\n",
            "Train Epoch: 7 [3200/133257] Loss: 0.219640 Acc: 0.9062\n",
            "Train Epoch: 7 [6400/133257] Loss: 0.233074 Acc: 0.9062\n",
            "Train Epoch: 7 [9600/133257] Loss: 0.293534 Acc: 0.9375\n",
            "Train Epoch: 7 [12800/133257] Loss: 0.171468 Acc: 0.9375\n",
            "Train Epoch: 7 [16000/133257] Loss: 0.246146 Acc: 0.9375\n",
            "Train Epoch: 7 [19200/133257] Loss: 0.036973 Acc: 1.0000\n",
            "Train Epoch: 7 [22400/133257] Loss: 0.110474 Acc: 0.9688\n",
            "Train Epoch: 7 [25600/133257] Loss: 0.490077 Acc: 0.8750\n",
            "Train Epoch: 7 [28800/133257] Loss: 0.150296 Acc: 0.9688\n",
            "Train Epoch: 7 [32000/133257] Loss: 0.219203 Acc: 0.9375\n",
            "Train Epoch: 7 [35200/133257] Loss: 0.259154 Acc: 0.9375\n",
            "Train Epoch: 7 [38400/133257] Loss: 0.271752 Acc: 0.9062\n",
            "Train Epoch: 7 [41600/133257] Loss: 0.107379 Acc: 0.9375\n",
            "Train Epoch: 7 [44800/133257] Loss: 0.114843 Acc: 1.0000\n",
            "Train Epoch: 7 [48000/133257] Loss: 0.288068 Acc: 0.9062\n",
            "Train Epoch: 7 [51200/133257] Loss: 0.327168 Acc: 0.9062\n",
            "Train Epoch: 7 [54400/133257] Loss: 0.281487 Acc: 0.8750\n",
            "Train Epoch: 7 [57600/133257] Loss: 0.221771 Acc: 0.8750\n",
            "Train Epoch: 7 [60800/133257] Loss: 0.098608 Acc: 0.9688\n",
            "Train Epoch: 7 [64000/133257] Loss: 0.117354 Acc: 1.0000\n",
            "Train Epoch: 7 [67200/133257] Loss: 0.178089 Acc: 0.9375\n",
            "Train Epoch: 7 [70400/133257] Loss: 0.123266 Acc: 0.9688\n",
            "Train Epoch: 7 [73600/133257] Loss: 0.121165 Acc: 1.0000\n",
            "Train Epoch: 7 [76800/133257] Loss: 0.220475 Acc: 0.9062\n",
            "Train Epoch: 7 [80000/133257] Loss: 0.065118 Acc: 1.0000\n",
            "Train Epoch: 7 [83200/133257] Loss: 0.157211 Acc: 0.9375\n",
            "Train Epoch: 7 [86400/133257] Loss: 0.096603 Acc: 1.0000\n",
            "Train Epoch: 7 [89600/133257] Loss: 0.099572 Acc: 1.0000\n",
            "Train Epoch: 7 [92800/133257] Loss: 0.131616 Acc: 0.9688\n",
            "Train Epoch: 7 [96000/133257] Loss: 0.284208 Acc: 0.9062\n",
            "Train Epoch: 7 [99200/133257] Loss: 0.181658 Acc: 0.9375\n",
            "Train Epoch: 7 [102400/133257] Loss: 0.022613 Acc: 1.0000\n",
            "Train Epoch: 7 [105600/133257] Loss: 0.088787 Acc: 0.9688\n",
            "Train Epoch: 7 [108800/133257] Loss: 0.282913 Acc: 0.9062\n",
            "Train Epoch: 7 [112000/133257] Loss: 0.176105 Acc: 0.9375\n",
            "Train Epoch: 7 [115200/133257] Loss: 0.178998 Acc: 0.9375\n",
            "Train Epoch: 7 [118400/133257] Loss: 0.264727 Acc: 0.9375\n",
            "Train Epoch: 7 [121600/133257] Loss: 0.201738 Acc: 0.9375\n",
            "Train Epoch: 7 [124800/133257] Loss: 0.191768 Acc: 0.9375\n",
            "Train Epoch: 7 [128000/133257] Loss: 0.444270 Acc: 0.8125\n",
            "Train Epoch: 7 [131200/133257] Loss: 0.030037 Acc: 1.0000\n",
            "Elapsed 331.64s, 41.46 s/epoch, 0.01 s/batch, ets 497.46s\n",
            "\n",
            "Test set: Average loss: 0.3207, Accuracy: 32626/36032 (91%)\n",
            "\n",
            "Train Epoch: 8 [3200/133257] Loss: 0.307953 Acc: 0.9062\n",
            "Train Epoch: 8 [6400/133257] Loss: 0.271739 Acc: 0.9062\n",
            "Train Epoch: 8 [9600/133257] Loss: 0.357426 Acc: 0.9062\n",
            "Train Epoch: 8 [12800/133257] Loss: 0.139205 Acc: 0.9688\n",
            "Train Epoch: 8 [16000/133257] Loss: 0.145279 Acc: 0.9688\n",
            "Train Epoch: 8 [19200/133257] Loss: 0.387446 Acc: 0.9062\n",
            "Train Epoch: 8 [22400/133257] Loss: 0.182221 Acc: 0.9062\n",
            "Train Epoch: 8 [25600/133257] Loss: 0.207601 Acc: 0.8750\n",
            "Train Epoch: 8 [28800/133257] Loss: 0.167159 Acc: 0.9375\n",
            "Train Epoch: 8 [32000/133257] Loss: 0.026299 Acc: 1.0000\n",
            "Train Epoch: 8 [35200/133257] Loss: 0.241577 Acc: 0.9375\n",
            "Train Epoch: 8 [38400/133257] Loss: 0.356923 Acc: 0.8750\n",
            "Train Epoch: 8 [41600/133257] Loss: 0.296345 Acc: 0.9062\n",
            "Train Epoch: 8 [44800/133257] Loss: 0.262364 Acc: 0.9062\n",
            "Train Epoch: 8 [48000/133257] Loss: 0.145961 Acc: 0.9688\n",
            "Train Epoch: 8 [51200/133257] Loss: 0.172698 Acc: 0.9375\n",
            "Train Epoch: 8 [54400/133257] Loss: 0.139314 Acc: 0.9688\n",
            "Train Epoch: 8 [57600/133257] Loss: 0.299614 Acc: 0.9062\n",
            "Train Epoch: 8 [60800/133257] Loss: 0.147074 Acc: 0.9688\n",
            "Train Epoch: 8 [64000/133257] Loss: 0.369394 Acc: 0.9062\n",
            "Train Epoch: 8 [67200/133257] Loss: 0.168442 Acc: 0.9375\n",
            "Train Epoch: 8 [70400/133257] Loss: 0.174764 Acc: 0.9375\n",
            "Train Epoch: 8 [73600/133257] Loss: 0.138446 Acc: 0.9375\n",
            "Train Epoch: 8 [76800/133257] Loss: 0.237996 Acc: 0.9062\n",
            "Train Epoch: 8 [80000/133257] Loss: 0.144026 Acc: 0.9688\n",
            "Train Epoch: 8 [83200/133257] Loss: 0.065807 Acc: 0.9688\n",
            "Train Epoch: 8 [86400/133257] Loss: 0.130526 Acc: 0.9375\n",
            "Train Epoch: 8 [89600/133257] Loss: 0.145525 Acc: 0.9375\n",
            "Train Epoch: 8 [92800/133257] Loss: 0.174510 Acc: 0.9062\n",
            "Train Epoch: 8 [96000/133257] Loss: 0.166967 Acc: 0.9375\n",
            "Train Epoch: 8 [99200/133257] Loss: 0.082835 Acc: 0.9688\n",
            "Train Epoch: 8 [102400/133257] Loss: 0.164297 Acc: 0.9375\n",
            "Train Epoch: 8 [105600/133257] Loss: 0.111789 Acc: 0.9688\n",
            "Train Epoch: 8 [108800/133257] Loss: 0.229839 Acc: 0.9375\n",
            "Train Epoch: 8 [112000/133257] Loss: 0.155429 Acc: 0.9688\n",
            "Train Epoch: 8 [115200/133257] Loss: 0.117616 Acc: 0.9688\n",
            "Train Epoch: 8 [118400/133257] Loss: 0.179525 Acc: 0.9375\n",
            "Train Epoch: 8 [121600/133257] Loss: 0.186535 Acc: 0.9375\n",
            "Train Epoch: 8 [124800/133257] Loss: 0.052494 Acc: 1.0000\n",
            "Train Epoch: 8 [128000/133257] Loss: 0.263815 Acc: 0.9688\n",
            "Train Epoch: 8 [131200/133257] Loss: 0.180088 Acc: 0.9375\n",
            "Elapsed 374.05s, 41.56 s/epoch, 0.01 s/batch, ets 457.17s\n",
            "\n",
            "Test set: Average loss: 0.3294, Accuracy: 32517/36032 (90%)\n",
            "\n",
            "Train Epoch: 9 [3200/133257] Loss: 0.102498 Acc: 0.9688\n",
            "Train Epoch: 9 [6400/133257] Loss: 0.425218 Acc: 0.8438\n",
            "Train Epoch: 9 [9600/133257] Loss: 0.203902 Acc: 0.9375\n",
            "Train Epoch: 9 [12800/133257] Loss: 0.226886 Acc: 0.9375\n",
            "Train Epoch: 9 [16000/133257] Loss: 0.247245 Acc: 0.9375\n",
            "Train Epoch: 9 [19200/133257] Loss: 0.172275 Acc: 0.9375\n",
            "Train Epoch: 9 [22400/133257] Loss: 0.169515 Acc: 0.9688\n",
            "Train Epoch: 9 [25600/133257] Loss: 0.162660 Acc: 0.9375\n",
            "Train Epoch: 9 [28800/133257] Loss: 0.313624 Acc: 0.9062\n",
            "Train Epoch: 9 [32000/133257] Loss: 0.026750 Acc: 1.0000\n",
            "Train Epoch: 9 [35200/133257] Loss: 0.279329 Acc: 0.9062\n",
            "Train Epoch: 9 [38400/133257] Loss: 0.084026 Acc: 1.0000\n",
            "Train Epoch: 9 [41600/133257] Loss: 0.069965 Acc: 0.9688\n",
            "Train Epoch: 9 [44800/133257] Loss: 0.147100 Acc: 0.9375\n",
            "Train Epoch: 9 [48000/133257] Loss: 0.155453 Acc: 0.9375\n",
            "Train Epoch: 9 [51200/133257] Loss: 0.202220 Acc: 0.9375\n",
            "Train Epoch: 9 [54400/133257] Loss: 0.330675 Acc: 0.9062\n",
            "Train Epoch: 9 [57600/133257] Loss: 0.017732 Acc: 1.0000\n",
            "Train Epoch: 9 [60800/133257] Loss: 0.128605 Acc: 0.9375\n",
            "Train Epoch: 9 [64000/133257] Loss: 0.277640 Acc: 0.9375\n",
            "Train Epoch: 9 [67200/133257] Loss: 0.264238 Acc: 0.8750\n",
            "Train Epoch: 9 [70400/133257] Loss: 0.079029 Acc: 1.0000\n",
            "Train Epoch: 9 [73600/133257] Loss: 0.348508 Acc: 0.9375\n",
            "Train Epoch: 9 [76800/133257] Loss: 0.296415 Acc: 0.9375\n",
            "Train Epoch: 9 [80000/133257] Loss: 0.424776 Acc: 0.9375\n",
            "Train Epoch: 9 [83200/133257] Loss: 0.148849 Acc: 0.9688\n",
            "Train Epoch: 9 [86400/133257] Loss: 0.229541 Acc: 0.9062\n",
            "Train Epoch: 9 [89600/133257] Loss: 0.297157 Acc: 0.9375\n",
            "Train Epoch: 9 [92800/133257] Loss: 0.105347 Acc: 1.0000\n",
            "Train Epoch: 9 [96000/133257] Loss: 0.125716 Acc: 0.9688\n",
            "Train Epoch: 9 [99200/133257] Loss: 0.104935 Acc: 0.9688\n",
            "Train Epoch: 9 [102400/133257] Loss: 0.219937 Acc: 0.9375\n",
            "Train Epoch: 9 [105600/133257] Loss: 0.200349 Acc: 0.9062\n",
            "Train Epoch: 9 [108800/133257] Loss: 0.281866 Acc: 0.9062\n",
            "Train Epoch: 9 [112000/133257] Loss: 0.172057 Acc: 0.9062\n",
            "Train Epoch: 9 [115200/133257] Loss: 0.236923 Acc: 0.9062\n",
            "Train Epoch: 9 [118400/133257] Loss: 0.007532 Acc: 1.0000\n",
            "Train Epoch: 9 [121600/133257] Loss: 0.173744 Acc: 0.9375\n",
            "Train Epoch: 9 [124800/133257] Loss: 0.228902 Acc: 0.9375\n",
            "Train Epoch: 9 [128000/133257] Loss: 0.046560 Acc: 1.0000\n",
            "Train Epoch: 9 [131200/133257] Loss: 0.327877 Acc: 0.8750\n",
            "Elapsed 416.47s, 41.65 s/epoch, 0.01 s/batch, ets 416.47s\n",
            "\n",
            "Test set: Average loss: 0.3108, Accuracy: 32821/36032 (91%)\n",
            "\n",
            "Train Epoch: 10 [3200/133257] Loss: 0.694930 Acc: 0.8750\n",
            "Train Epoch: 10 [6400/133257] Loss: 0.119124 Acc: 0.9688\n",
            "Train Epoch: 10 [9600/133257] Loss: 0.174467 Acc: 0.9375\n",
            "Train Epoch: 10 [12800/133257] Loss: 0.083563 Acc: 0.9688\n",
            "Train Epoch: 10 [16000/133257] Loss: 0.059594 Acc: 1.0000\n",
            "Train Epoch: 10 [19200/133257] Loss: 0.059384 Acc: 1.0000\n",
            "Train Epoch: 10 [22400/133257] Loss: 0.565555 Acc: 0.8125\n",
            "Train Epoch: 10 [25600/133257] Loss: 0.188232 Acc: 0.9375\n",
            "Train Epoch: 10 [28800/133257] Loss: 0.085494 Acc: 0.9688\n",
            "Train Epoch: 10 [32000/133257] Loss: 0.131919 Acc: 0.9688\n",
            "Train Epoch: 10 [35200/133257] Loss: 0.178273 Acc: 0.9375\n",
            "Train Epoch: 10 [38400/133257] Loss: 0.091254 Acc: 0.9688\n",
            "Train Epoch: 10 [41600/133257] Loss: 0.107351 Acc: 0.9375\n",
            "Train Epoch: 10 [44800/133257] Loss: 0.189779 Acc: 0.9688\n",
            "Train Epoch: 10 [48000/133257] Loss: 0.422568 Acc: 0.9375\n",
            "Train Epoch: 10 [51200/133257] Loss: 0.274784 Acc: 0.9375\n",
            "Train Epoch: 10 [54400/133257] Loss: 0.123129 Acc: 0.9688\n",
            "Train Epoch: 10 [57600/133257] Loss: 0.379406 Acc: 0.8750\n",
            "Train Epoch: 10 [60800/133257] Loss: 0.103300 Acc: 0.9375\n",
            "Train Epoch: 10 [64000/133257] Loss: 0.184536 Acc: 0.9375\n",
            "Train Epoch: 10 [67200/133257] Loss: 0.227903 Acc: 0.9375\n",
            "Train Epoch: 10 [70400/133257] Loss: 0.247535 Acc: 0.9375\n",
            "Train Epoch: 10 [73600/133257] Loss: 0.305555 Acc: 0.9062\n",
            "Train Epoch: 10 [76800/133257] Loss: 0.116006 Acc: 0.9688\n",
            "Train Epoch: 10 [80000/133257] Loss: 0.142123 Acc: 0.9688\n",
            "Train Epoch: 10 [83200/133257] Loss: 0.216018 Acc: 0.9375\n",
            "Train Epoch: 10 [86400/133257] Loss: 0.490497 Acc: 0.9062\n",
            "Train Epoch: 10 [89600/133257] Loss: 0.219626 Acc: 0.9688\n",
            "Train Epoch: 10 [92800/133257] Loss: 0.059229 Acc: 0.9688\n",
            "Train Epoch: 10 [96000/133257] Loss: 0.220957 Acc: 0.9062\n",
            "Train Epoch: 10 [99200/133257] Loss: 0.024723 Acc: 1.0000\n",
            "Train Epoch: 10 [102400/133257] Loss: 0.013395 Acc: 1.0000\n",
            "Train Epoch: 10 [105600/133257] Loss: 0.214879 Acc: 0.9062\n",
            "Train Epoch: 10 [108800/133257] Loss: 0.161814 Acc: 0.9375\n",
            "Train Epoch: 10 [112000/133257] Loss: 0.226905 Acc: 0.9375\n",
            "Train Epoch: 10 [115200/133257] Loss: 0.209466 Acc: 0.9688\n",
            "Train Epoch: 10 [118400/133257] Loss: 0.245228 Acc: 0.9062\n",
            "Train Epoch: 10 [121600/133257] Loss: 0.166479 Acc: 0.9375\n",
            "Train Epoch: 10 [124800/133257] Loss: 0.206967 Acc: 0.9375\n",
            "Train Epoch: 10 [128000/133257] Loss: 0.107823 Acc: 0.9375\n",
            "Train Epoch: 10 [131200/133257] Loss: 0.172124 Acc: 0.8750\n",
            "Elapsed 458.82s, 41.71 s/epoch, 0.01 s/batch, ets 375.40s\n",
            "\n",
            "Test set: Average loss: 0.3133, Accuracy: 32695/36032 (91%)\n",
            "\n",
            "Train Epoch: 11 [3200/133257] Loss: 0.054141 Acc: 1.0000\n",
            "Train Epoch: 11 [6400/133257] Loss: 0.269010 Acc: 0.9375\n",
            "Train Epoch: 11 [9600/133257] Loss: 0.541553 Acc: 0.9062\n",
            "Train Epoch: 11 [12800/133257] Loss: 0.145900 Acc: 0.9375\n",
            "Train Epoch: 11 [16000/133257] Loss: 0.131117 Acc: 0.9375\n",
            "Train Epoch: 11 [19200/133257] Loss: 0.150161 Acc: 0.9375\n",
            "Train Epoch: 11 [22400/133257] Loss: 0.025067 Acc: 1.0000\n",
            "Train Epoch: 11 [25600/133257] Loss: 0.104924 Acc: 0.9688\n",
            "Train Epoch: 11 [28800/133257] Loss: 0.207194 Acc: 0.9688\n",
            "Train Epoch: 11 [32000/133257] Loss: 0.232113 Acc: 0.9375\n",
            "Train Epoch: 11 [35200/133257] Loss: 0.097595 Acc: 0.9688\n",
            "Train Epoch: 11 [38400/133257] Loss: 0.158967 Acc: 0.9375\n",
            "Train Epoch: 11 [41600/133257] Loss: 0.144070 Acc: 0.9688\n",
            "Train Epoch: 11 [44800/133257] Loss: 0.261037 Acc: 0.9062\n",
            "Train Epoch: 11 [48000/133257] Loss: 0.691999 Acc: 0.9375\n",
            "Train Epoch: 11 [51200/133257] Loss: 0.167021 Acc: 0.9375\n",
            "Train Epoch: 11 [54400/133257] Loss: 0.335476 Acc: 0.8750\n",
            "Train Epoch: 11 [57600/133257] Loss: 0.102043 Acc: 0.9688\n",
            "Train Epoch: 11 [60800/133257] Loss: 0.069636 Acc: 1.0000\n",
            "Train Epoch: 11 [64000/133257] Loss: 0.160216 Acc: 0.9375\n",
            "Train Epoch: 11 [67200/133257] Loss: 0.152509 Acc: 0.9375\n",
            "Train Epoch: 11 [70400/133257] Loss: 0.017716 Acc: 1.0000\n",
            "Train Epoch: 11 [73600/133257] Loss: 0.084625 Acc: 0.9688\n",
            "Train Epoch: 11 [76800/133257] Loss: 0.282169 Acc: 0.9062\n",
            "Train Epoch: 11 [80000/133257] Loss: 0.069761 Acc: 0.9688\n",
            "Train Epoch: 11 [83200/133257] Loss: 0.108745 Acc: 0.9688\n",
            "Train Epoch: 11 [86400/133257] Loss: 0.297506 Acc: 0.9375\n",
            "Train Epoch: 11 [89600/133257] Loss: 0.360002 Acc: 0.8438\n",
            "Train Epoch: 11 [92800/133257] Loss: 0.193949 Acc: 0.9062\n",
            "Train Epoch: 11 [96000/133257] Loss: 0.358350 Acc: 0.8438\n",
            "Train Epoch: 11 [99200/133257] Loss: 0.145445 Acc: 0.9688\n",
            "Train Epoch: 11 [102400/133257] Loss: 0.074780 Acc: 1.0000\n",
            "Train Epoch: 11 [105600/133257] Loss: 0.338717 Acc: 0.8750\n",
            "Train Epoch: 11 [108800/133257] Loss: 0.066949 Acc: 1.0000\n",
            "Train Epoch: 11 [112000/133257] Loss: 0.083755 Acc: 0.9688\n",
            "Train Epoch: 11 [115200/133257] Loss: 0.147452 Acc: 0.9688\n",
            "Train Epoch: 11 [118400/133257] Loss: 0.065142 Acc: 0.9688\n",
            "Train Epoch: 11 [121600/133257] Loss: 0.087238 Acc: 1.0000\n",
            "Train Epoch: 11 [124800/133257] Loss: 0.071705 Acc: 0.9688\n",
            "Train Epoch: 11 [128000/133257] Loss: 0.038375 Acc: 1.0000\n",
            "Train Epoch: 11 [131200/133257] Loss: 0.444114 Acc: 0.8750\n",
            "Elapsed 501.12s, 41.76 s/epoch, 0.01 s/batch, ets 334.08s\n",
            "\n",
            "Test set: Average loss: 0.2946, Accuracy: 32961/36032 (91%)\n",
            "\n",
            "Train Epoch: 12 [3200/133257] Loss: 0.013074 Acc: 1.0000\n",
            "Train Epoch: 12 [6400/133257] Loss: 0.231580 Acc: 0.9375\n",
            "Train Epoch: 12 [9600/133257] Loss: 0.242305 Acc: 0.9375\n",
            "Train Epoch: 12 [12800/133257] Loss: 0.198214 Acc: 0.9688\n",
            "Train Epoch: 12 [16000/133257] Loss: 0.154675 Acc: 0.9688\n",
            "Train Epoch: 12 [19200/133257] Loss: 0.310962 Acc: 0.8750\n",
            "Train Epoch: 12 [22400/133257] Loss: 0.109815 Acc: 0.9688\n",
            "Train Epoch: 12 [25600/133257] Loss: 0.108340 Acc: 0.9688\n",
            "Train Epoch: 12 [28800/133257] Loss: 0.132075 Acc: 0.9688\n",
            "Train Epoch: 12 [32000/133257] Loss: 0.112586 Acc: 0.9688\n",
            "Train Epoch: 12 [35200/133257] Loss: 0.084538 Acc: 0.9688\n",
            "Train Epoch: 12 [38400/133257] Loss: 0.041217 Acc: 0.9688\n",
            "Train Epoch: 12 [41600/133257] Loss: 0.231983 Acc: 0.9062\n",
            "Train Epoch: 12 [44800/133257] Loss: 0.055285 Acc: 1.0000\n",
            "Train Epoch: 12 [48000/133257] Loss: 0.336428 Acc: 0.8750\n",
            "Train Epoch: 12 [51200/133257] Loss: 0.194186 Acc: 0.9062\n",
            "Train Epoch: 12 [54400/133257] Loss: 0.055881 Acc: 1.0000\n",
            "Train Epoch: 12 [57600/133257] Loss: 0.066889 Acc: 1.0000\n",
            "Train Epoch: 12 [60800/133257] Loss: 0.294992 Acc: 0.9062\n",
            "Train Epoch: 12 [64000/133257] Loss: 0.190683 Acc: 0.9375\n",
            "Train Epoch: 12 [67200/133257] Loss: 0.069618 Acc: 0.9688\n",
            "Train Epoch: 12 [70400/133257] Loss: 0.053107 Acc: 0.9688\n",
            "Train Epoch: 12 [73600/133257] Loss: 0.128242 Acc: 1.0000\n",
            "Train Epoch: 12 [76800/133257] Loss: 0.029565 Acc: 1.0000\n",
            "Train Epoch: 12 [80000/133257] Loss: 0.094021 Acc: 1.0000\n",
            "Train Epoch: 12 [83200/133257] Loss: 0.406286 Acc: 0.9062\n",
            "Train Epoch: 12 [86400/133257] Loss: 0.366103 Acc: 0.9062\n",
            "Train Epoch: 12 [89600/133257] Loss: 0.169005 Acc: 0.9062\n",
            "Train Epoch: 12 [92800/133257] Loss: 0.082881 Acc: 0.9688\n",
            "Train Epoch: 12 [96000/133257] Loss: 0.093426 Acc: 0.9688\n",
            "Train Epoch: 12 [99200/133257] Loss: 0.173466 Acc: 0.9375\n",
            "Train Epoch: 12 [102400/133257] Loss: 0.171180 Acc: 0.9375\n",
            "Train Epoch: 12 [105600/133257] Loss: 0.170720 Acc: 0.9375\n",
            "Train Epoch: 12 [108800/133257] Loss: 0.055812 Acc: 1.0000\n",
            "Train Epoch: 12 [112000/133257] Loss: 0.120919 Acc: 0.9688\n",
            "Train Epoch: 12 [115200/133257] Loss: 0.129127 Acc: 0.9375\n",
            "Train Epoch: 12 [118400/133257] Loss: 0.157638 Acc: 0.9688\n",
            "Train Epoch: 12 [121600/133257] Loss: 0.166267 Acc: 0.9688\n",
            "Train Epoch: 12 [124800/133257] Loss: 0.139784 Acc: 0.9375\n",
            "Train Epoch: 12 [128000/133257] Loss: 0.071629 Acc: 0.9688\n",
            "Train Epoch: 12 [131200/133257] Loss: 0.022155 Acc: 1.0000\n",
            "Elapsed 543.27s, 41.79 s/epoch, 0.01 s/batch, ets 292.53s\n",
            "\n",
            "Test set: Average loss: 0.3146, Accuracy: 32784/36032 (91%)\n",
            "\n",
            "Train Epoch: 13 [3200/133257] Loss: 0.130274 Acc: 0.9688\n",
            "Train Epoch: 13 [6400/133257] Loss: 0.092992 Acc: 0.9688\n",
            "Train Epoch: 13 [9600/133257] Loss: 0.201037 Acc: 0.9375\n",
            "Train Epoch: 13 [12800/133257] Loss: 0.249570 Acc: 0.9062\n",
            "Train Epoch: 13 [16000/133257] Loss: 0.273943 Acc: 0.8750\n",
            "Train Epoch: 13 [19200/133257] Loss: 0.135596 Acc: 0.9375\n",
            "Train Epoch: 13 [22400/133257] Loss: 0.086249 Acc: 0.9688\n",
            "Train Epoch: 13 [25600/133257] Loss: 0.268758 Acc: 0.9375\n",
            "Train Epoch: 13 [28800/133257] Loss: 0.266723 Acc: 0.9375\n",
            "Train Epoch: 13 [32000/133257] Loss: 0.035852 Acc: 1.0000\n",
            "Train Epoch: 13 [35200/133257] Loss: 0.445459 Acc: 0.8750\n",
            "Train Epoch: 13 [38400/133257] Loss: 0.109921 Acc: 0.9375\n",
            "Train Epoch: 13 [41600/133257] Loss: 0.204467 Acc: 0.9375\n",
            "Train Epoch: 13 [44800/133257] Loss: 0.085674 Acc: 0.9688\n",
            "Train Epoch: 13 [48000/133257] Loss: 0.208949 Acc: 0.9062\n",
            "Train Epoch: 13 [51200/133257] Loss: 0.056713 Acc: 1.0000\n",
            "Train Epoch: 13 [54400/133257] Loss: 0.117868 Acc: 0.9375\n",
            "Train Epoch: 13 [57600/133257] Loss: 0.387762 Acc: 0.9375\n",
            "Train Epoch: 13 [60800/133257] Loss: 0.261338 Acc: 0.9062\n",
            "Train Epoch: 13 [64000/133257] Loss: 0.106874 Acc: 0.9688\n",
            "Train Epoch: 13 [67200/133257] Loss: 0.080134 Acc: 0.9375\n",
            "Train Epoch: 13 [70400/133257] Loss: 0.051740 Acc: 1.0000\n",
            "Train Epoch: 13 [73600/133257] Loss: 0.333588 Acc: 0.8750\n",
            "Train Epoch: 13 [76800/133257] Loss: 0.354332 Acc: 0.9062\n",
            "Train Epoch: 13 [80000/133257] Loss: 0.092576 Acc: 1.0000\n",
            "Train Epoch: 13 [83200/133257] Loss: 0.111657 Acc: 0.9375\n",
            "Train Epoch: 13 [86400/133257] Loss: 0.070914 Acc: 0.9688\n",
            "Train Epoch: 13 [89600/133257] Loss: 0.136786 Acc: 0.9062\n",
            "Train Epoch: 13 [92800/133257] Loss: 0.025255 Acc: 1.0000\n",
            "Train Epoch: 13 [96000/133257] Loss: 0.119728 Acc: 0.9688\n",
            "Train Epoch: 13 [99200/133257] Loss: 0.141786 Acc: 0.9375\n",
            "Train Epoch: 13 [102400/133257] Loss: 0.222243 Acc: 0.9375\n",
            "Train Epoch: 13 [105600/133257] Loss: 0.060187 Acc: 1.0000\n",
            "Train Epoch: 13 [108800/133257] Loss: 0.220759 Acc: 0.9062\n",
            "Train Epoch: 13 [112000/133257] Loss: 0.120012 Acc: 1.0000\n",
            "Train Epoch: 13 [115200/133257] Loss: 0.148863 Acc: 0.9062\n",
            "Train Epoch: 13 [118400/133257] Loss: 0.701392 Acc: 0.9375\n",
            "Train Epoch: 13 [121600/133257] Loss: 0.284718 Acc: 0.9375\n",
            "Train Epoch: 13 [124800/133257] Loss: 0.162003 Acc: 0.9375\n",
            "Train Epoch: 13 [128000/133257] Loss: 0.140241 Acc: 0.9688\n",
            "Train Epoch: 13 [131200/133257] Loss: 0.053154 Acc: 1.0000\n",
            "Elapsed 586.00s, 41.86 s/epoch, 0.01 s/batch, ets 251.14s\n",
            "\n",
            "Test set: Average loss: 0.2852, Accuracy: 33102/36032 (92%)\n",
            "\n",
            "Train Epoch: 14 [3200/133257] Loss: 0.119200 Acc: 0.9375\n",
            "Train Epoch: 14 [6400/133257] Loss: 0.318224 Acc: 0.8438\n",
            "Train Epoch: 14 [9600/133257] Loss: 0.017145 Acc: 1.0000\n",
            "Train Epoch: 14 [12800/133257] Loss: 0.079137 Acc: 0.9688\n",
            "Train Epoch: 14 [16000/133257] Loss: 0.055729 Acc: 1.0000\n",
            "Train Epoch: 14 [19200/133257] Loss: 0.267053 Acc: 0.9062\n",
            "Train Epoch: 14 [22400/133257] Loss: 0.102427 Acc: 1.0000\n",
            "Train Epoch: 14 [25600/133257] Loss: 0.074239 Acc: 0.9688\n",
            "Train Epoch: 14 [28800/133257] Loss: 0.038556 Acc: 1.0000\n",
            "Train Epoch: 14 [32000/133257] Loss: 0.193538 Acc: 0.9375\n",
            "Train Epoch: 14 [35200/133257] Loss: 0.346619 Acc: 0.8750\n",
            "Train Epoch: 14 [38400/133257] Loss: 0.053599 Acc: 1.0000\n",
            "Train Epoch: 14 [41600/133257] Loss: 0.072024 Acc: 1.0000\n",
            "Train Epoch: 14 [44800/133257] Loss: 0.057522 Acc: 1.0000\n",
            "Train Epoch: 14 [48000/133257] Loss: 0.251790 Acc: 0.9062\n",
            "Train Epoch: 14 [51200/133257] Loss: 0.174784 Acc: 0.9375\n",
            "Train Epoch: 14 [54400/133257] Loss: 0.110848 Acc: 0.9688\n",
            "Train Epoch: 14 [57600/133257] Loss: 0.220451 Acc: 0.9062\n",
            "Train Epoch: 14 [60800/133257] Loss: 0.178165 Acc: 0.9062\n",
            "Train Epoch: 14 [64000/133257] Loss: 0.279514 Acc: 0.9375\n",
            "Train Epoch: 14 [67200/133257] Loss: 0.336342 Acc: 0.9062\n",
            "Train Epoch: 14 [70400/133257] Loss: 0.096438 Acc: 0.9375\n",
            "Train Epoch: 14 [73600/133257] Loss: 0.152135 Acc: 0.9375\n",
            "Train Epoch: 14 [76800/133257] Loss: 0.075450 Acc: 0.9375\n",
            "Train Epoch: 14 [80000/133257] Loss: 0.051840 Acc: 1.0000\n",
            "Train Epoch: 14 [83200/133257] Loss: 0.182227 Acc: 0.9375\n",
            "Train Epoch: 14 [86400/133257] Loss: 0.173859 Acc: 0.9375\n",
            "Train Epoch: 14 [89600/133257] Loss: 0.473788 Acc: 0.9062\n",
            "Train Epoch: 14 [92800/133257] Loss: 0.220326 Acc: 0.9062\n",
            "Train Epoch: 14 [96000/133257] Loss: 0.281940 Acc: 0.8750\n",
            "Train Epoch: 14 [99200/133257] Loss: 0.214192 Acc: 0.9375\n",
            "Train Epoch: 14 [102400/133257] Loss: 0.019501 Acc: 1.0000\n",
            "Train Epoch: 14 [105600/133257] Loss: 0.132915 Acc: 0.9375\n",
            "Train Epoch: 14 [108800/133257] Loss: 0.243850 Acc: 0.8750\n",
            "Train Epoch: 14 [112000/133257] Loss: 0.197181 Acc: 0.9375\n",
            "Train Epoch: 14 [115200/133257] Loss: 0.112436 Acc: 0.9688\n",
            "Train Epoch: 14 [118400/133257] Loss: 0.093258 Acc: 1.0000\n",
            "Train Epoch: 14 [121600/133257] Loss: 0.552289 Acc: 0.8750\n",
            "Train Epoch: 14 [124800/133257] Loss: 0.428716 Acc: 0.9375\n",
            "Train Epoch: 14 [128000/133257] Loss: 0.048829 Acc: 1.0000\n",
            "Train Epoch: 14 [131200/133257] Loss: 0.116516 Acc: 0.9688\n",
            "Elapsed 628.37s, 41.89 s/epoch, 0.01 s/batch, ets 209.46s\n",
            "\n",
            "Test set: Average loss: 0.2907, Accuracy: 33081/36032 (92%)\n",
            "\n",
            "Train Epoch: 15 [3200/133257] Loss: 0.171580 Acc: 0.9375\n",
            "Train Epoch: 15 [6400/133257] Loss: 0.210134 Acc: 0.9375\n",
            "Train Epoch: 15 [9600/133257] Loss: 0.111342 Acc: 0.9688\n",
            "Train Epoch: 15 [12800/133257] Loss: 0.115101 Acc: 1.0000\n",
            "Train Epoch: 15 [16000/133257] Loss: 0.130555 Acc: 0.9375\n",
            "Train Epoch: 15 [19200/133257] Loss: 0.037822 Acc: 1.0000\n",
            "Train Epoch: 15 [22400/133257] Loss: 0.318739 Acc: 0.9062\n",
            "Train Epoch: 15 [25600/133257] Loss: 0.867285 Acc: 0.8438\n",
            "Train Epoch: 15 [28800/133257] Loss: 0.024604 Acc: 1.0000\n",
            "Train Epoch: 15 [32000/133257] Loss: 0.337811 Acc: 0.9062\n",
            "Train Epoch: 15 [35200/133257] Loss: 0.285923 Acc: 0.9688\n",
            "Train Epoch: 15 [38400/133257] Loss: 0.101981 Acc: 0.9688\n",
            "Train Epoch: 15 [41600/133257] Loss: 0.065032 Acc: 0.9688\n",
            "Train Epoch: 15 [44800/133257] Loss: 0.034647 Acc: 1.0000\n",
            "Train Epoch: 15 [48000/133257] Loss: 0.166046 Acc: 0.9375\n",
            "Train Epoch: 15 [51200/133257] Loss: 0.288943 Acc: 0.9062\n",
            "Train Epoch: 15 [54400/133257] Loss: 0.201445 Acc: 0.9375\n",
            "Train Epoch: 15 [57600/133257] Loss: 0.299780 Acc: 0.9688\n",
            "Train Epoch: 15 [60800/133257] Loss: 0.224352 Acc: 0.9062\n",
            "Train Epoch: 15 [64000/133257] Loss: 0.558359 Acc: 0.8750\n",
            "Train Epoch: 15 [67200/133257] Loss: 0.071636 Acc: 1.0000\n",
            "Train Epoch: 15 [70400/133257] Loss: 0.078944 Acc: 0.9688\n",
            "Train Epoch: 15 [73600/133257] Loss: 0.048073 Acc: 1.0000\n",
            "Train Epoch: 15 [76800/133257] Loss: 0.193937 Acc: 0.9062\n",
            "Train Epoch: 15 [80000/133257] Loss: 0.049167 Acc: 1.0000\n",
            "Train Epoch: 15 [83200/133257] Loss: 0.368767 Acc: 0.8750\n",
            "Train Epoch: 15 [86400/133257] Loss: 0.073423 Acc: 1.0000\n",
            "Train Epoch: 15 [89600/133257] Loss: 0.113403 Acc: 0.9688\n",
            "Train Epoch: 15 [92800/133257] Loss: 0.222779 Acc: 0.8750\n",
            "Train Epoch: 15 [96000/133257] Loss: 0.152522 Acc: 0.9375\n",
            "Train Epoch: 15 [99200/133257] Loss: 0.207036 Acc: 0.9375\n",
            "Train Epoch: 15 [102400/133257] Loss: 0.249214 Acc: 0.9062\n",
            "Train Epoch: 15 [105600/133257] Loss: 0.152802 Acc: 0.9375\n",
            "Train Epoch: 15 [108800/133257] Loss: 0.049917 Acc: 1.0000\n",
            "Train Epoch: 15 [112000/133257] Loss: 0.144872 Acc: 0.9375\n",
            "Train Epoch: 15 [115200/133257] Loss: 0.098581 Acc: 0.9688\n",
            "Train Epoch: 15 [118400/133257] Loss: 0.427226 Acc: 0.8750\n",
            "Train Epoch: 15 [121600/133257] Loss: 0.194446 Acc: 0.9375\n",
            "Train Epoch: 15 [124800/133257] Loss: 0.073411 Acc: 0.9688\n",
            "Train Epoch: 15 [128000/133257] Loss: 0.058514 Acc: 1.0000\n",
            "Train Epoch: 15 [131200/133257] Loss: 0.192172 Acc: 0.9062\n",
            "Elapsed 670.71s, 41.92 s/epoch, 0.01 s/batch, ets 167.68s\n",
            "\n",
            "Test set: Average loss: 0.2881, Accuracy: 33091/36032 (92%)\n",
            "\n",
            "Train Epoch: 16 [3200/133257] Loss: 0.110764 Acc: 0.9688\n",
            "Train Epoch: 16 [6400/133257] Loss: 0.511633 Acc: 0.8438\n",
            "Train Epoch: 16 [9600/133257] Loss: 0.221038 Acc: 0.9062\n",
            "Train Epoch: 16 [12800/133257] Loss: 0.165621 Acc: 0.9688\n",
            "Train Epoch: 16 [16000/133257] Loss: 0.142933 Acc: 0.9375\n",
            "Train Epoch: 16 [19200/133257] Loss: 0.148222 Acc: 0.9375\n",
            "Train Epoch: 16 [22400/133257] Loss: 0.053806 Acc: 1.0000\n",
            "Train Epoch: 16 [25600/133257] Loss: 0.023443 Acc: 1.0000\n",
            "Train Epoch: 16 [28800/133257] Loss: 0.264967 Acc: 0.9375\n",
            "Train Epoch: 16 [32000/133257] Loss: 0.141004 Acc: 0.9688\n",
            "Train Epoch: 16 [35200/133257] Loss: 0.059658 Acc: 1.0000\n",
            "Train Epoch: 16 [38400/133257] Loss: 0.103545 Acc: 0.9688\n",
            "Train Epoch: 16 [41600/133257] Loss: 0.024306 Acc: 1.0000\n",
            "Train Epoch: 16 [44800/133257] Loss: 0.331993 Acc: 0.8750\n",
            "Train Epoch: 16 [48000/133257] Loss: 0.062268 Acc: 0.9688\n",
            "Train Epoch: 16 [51200/133257] Loss: 0.533096 Acc: 0.9375\n",
            "Train Epoch: 16 [54400/133257] Loss: 0.354636 Acc: 0.8750\n",
            "Train Epoch: 16 [57600/133257] Loss: 0.165916 Acc: 0.9375\n",
            "Train Epoch: 16 [60800/133257] Loss: 0.241386 Acc: 0.9375\n",
            "Train Epoch: 16 [64000/133257] Loss: 0.626671 Acc: 0.8438\n",
            "Train Epoch: 16 [67200/133257] Loss: 0.065718 Acc: 0.9688\n",
            "Train Epoch: 16 [70400/133257] Loss: 0.184433 Acc: 0.9375\n",
            "Train Epoch: 16 [73600/133257] Loss: 0.126245 Acc: 0.9375\n",
            "Train Epoch: 16 [76800/133257] Loss: 0.045094 Acc: 0.9688\n",
            "Train Epoch: 16 [80000/133257] Loss: 0.259350 Acc: 0.8750\n",
            "Train Epoch: 16 [83200/133257] Loss: 0.074393 Acc: 0.9688\n",
            "Train Epoch: 16 [86400/133257] Loss: 0.054004 Acc: 1.0000\n",
            "Train Epoch: 16 [89600/133257] Loss: 0.505735 Acc: 0.9062\n",
            "Train Epoch: 16 [92800/133257] Loss: 0.065687 Acc: 0.9688\n",
            "Train Epoch: 16 [96000/133257] Loss: 0.161233 Acc: 0.9375\n",
            "Train Epoch: 16 [99200/133257] Loss: 0.025478 Acc: 1.0000\n",
            "Train Epoch: 16 [102400/133257] Loss: 0.425314 Acc: 0.9062\n",
            "Train Epoch: 16 [105600/133257] Loss: 0.433576 Acc: 0.8750\n",
            "Train Epoch: 16 [108800/133257] Loss: 0.198330 Acc: 0.9688\n",
            "Train Epoch: 16 [112000/133257] Loss: 0.111951 Acc: 0.9688\n",
            "Train Epoch: 16 [115200/133257] Loss: 0.113866 Acc: 0.9688\n",
            "Train Epoch: 16 [118400/133257] Loss: 0.275556 Acc: 0.9375\n",
            "Train Epoch: 16 [121600/133257] Loss: 0.092644 Acc: 0.9688\n",
            "Train Epoch: 16 [124800/133257] Loss: 0.126023 Acc: 0.9688\n",
            "Train Epoch: 16 [128000/133257] Loss: 0.097591 Acc: 1.0000\n",
            "Train Epoch: 16 [131200/133257] Loss: 0.123264 Acc: 0.9688\n",
            "Elapsed 713.32s, 41.96 s/epoch, 0.01 s/batch, ets 125.88s\n",
            "\n",
            "Test set: Average loss: 0.2767, Accuracy: 33220/36032 (92%)\n",
            "\n",
            "Train Epoch: 17 [3200/133257] Loss: 0.256315 Acc: 0.9375\n",
            "Train Epoch: 17 [6400/133257] Loss: 0.039523 Acc: 1.0000\n",
            "Train Epoch: 17 [9600/133257] Loss: 0.021730 Acc: 1.0000\n",
            "Train Epoch: 17 [12800/133257] Loss: 0.103824 Acc: 0.9688\n",
            "Train Epoch: 17 [16000/133257] Loss: 0.163032 Acc: 0.9375\n",
            "Train Epoch: 17 [19200/133257] Loss: 0.143153 Acc: 0.9688\n",
            "Train Epoch: 17 [22400/133257] Loss: 0.159357 Acc: 0.9688\n",
            "Train Epoch: 17 [25600/133257] Loss: 0.179356 Acc: 0.9062\n",
            "Train Epoch: 17 [28800/133257] Loss: 0.201931 Acc: 0.9375\n",
            "Train Epoch: 17 [32000/133257] Loss: 0.082581 Acc: 0.9688\n",
            "Train Epoch: 17 [35200/133257] Loss: 0.201276 Acc: 0.9688\n",
            "Train Epoch: 17 [38400/133257] Loss: 0.123104 Acc: 0.9375\n",
            "Train Epoch: 17 [41600/133257] Loss: 0.132476 Acc: 0.9375\n",
            "Train Epoch: 17 [44800/133257] Loss: 0.051979 Acc: 0.9688\n",
            "Train Epoch: 17 [48000/133257] Loss: 0.072620 Acc: 1.0000\n",
            "Train Epoch: 17 [51200/133257] Loss: 0.147878 Acc: 0.9062\n",
            "Train Epoch: 17 [54400/133257] Loss: 0.111920 Acc: 0.9688\n",
            "Train Epoch: 17 [57600/133257] Loss: 0.147675 Acc: 0.9688\n",
            "Train Epoch: 17 [60800/133257] Loss: 0.098892 Acc: 0.9688\n",
            "Train Epoch: 17 [64000/133257] Loss: 0.275885 Acc: 0.9375\n",
            "Train Epoch: 17 [67200/133257] Loss: 0.158010 Acc: 0.9375\n",
            "Train Epoch: 17 [70400/133257] Loss: 0.107455 Acc: 0.9688\n",
            "Train Epoch: 17 [73600/133257] Loss: 0.112366 Acc: 0.9375\n",
            "Train Epoch: 17 [76800/133257] Loss: 0.076654 Acc: 0.9688\n",
            "Train Epoch: 17 [80000/133257] Loss: 0.144445 Acc: 0.9375\n",
            "Train Epoch: 17 [83200/133257] Loss: 0.026392 Acc: 1.0000\n",
            "Train Epoch: 17 [86400/133257] Loss: 0.149509 Acc: 0.9062\n",
            "Train Epoch: 17 [89600/133257] Loss: 0.107862 Acc: 0.9688\n",
            "Train Epoch: 17 [92800/133257] Loss: 0.071290 Acc: 1.0000\n",
            "Train Epoch: 17 [96000/133257] Loss: 0.068144 Acc: 1.0000\n",
            "Train Epoch: 17 [99200/133257] Loss: 0.115220 Acc: 0.9688\n",
            "Train Epoch: 17 [102400/133257] Loss: 0.113397 Acc: 0.9688\n",
            "Train Epoch: 17 [105600/133257] Loss: 0.063178 Acc: 0.9688\n",
            "Train Epoch: 17 [108800/133257] Loss: 0.324843 Acc: 0.9688\n",
            "Train Epoch: 17 [112000/133257] Loss: 0.236644 Acc: 0.9062\n",
            "Train Epoch: 17 [115200/133257] Loss: 0.436875 Acc: 0.8750\n",
            "Train Epoch: 17 [118400/133257] Loss: 0.201512 Acc: 0.9375\n",
            "Train Epoch: 17 [121600/133257] Loss: 0.107944 Acc: 0.9688\n",
            "Train Epoch: 17 [124800/133257] Loss: 0.160055 Acc: 0.9375\n",
            "Train Epoch: 17 [128000/133257] Loss: 0.058078 Acc: 1.0000\n",
            "Train Epoch: 17 [131200/133257] Loss: 0.099793 Acc: 1.0000\n",
            "Elapsed 755.97s, 42.00 s/epoch, 0.01 s/batch, ets 84.00s\n",
            "\n",
            "Test set: Average loss: 0.2829, Accuracy: 33139/36032 (92%)\n",
            "\n",
            "Train Epoch: 18 [3200/133257] Loss: 0.054019 Acc: 1.0000\n",
            "Train Epoch: 18 [6400/133257] Loss: 0.109811 Acc: 0.9688\n",
            "Train Epoch: 18 [9600/133257] Loss: 0.446466 Acc: 0.8125\n",
            "Train Epoch: 18 [12800/133257] Loss: 0.090573 Acc: 1.0000\n",
            "Train Epoch: 18 [16000/133257] Loss: 0.064032 Acc: 0.9688\n",
            "Train Epoch: 18 [19200/133257] Loss: 0.083155 Acc: 0.9688\n",
            "Train Epoch: 18 [22400/133257] Loss: 0.399670 Acc: 0.8750\n",
            "Train Epoch: 18 [25600/133257] Loss: 0.141390 Acc: 0.9375\n",
            "Train Epoch: 18 [28800/133257] Loss: 0.104075 Acc: 0.9688\n",
            "Train Epoch: 18 [32000/133257] Loss: 0.444783 Acc: 0.9375\n",
            "Train Epoch: 18 [35200/133257] Loss: 0.030830 Acc: 1.0000\n",
            "Train Epoch: 18 [38400/133257] Loss: 0.009976 Acc: 1.0000\n",
            "Train Epoch: 18 [41600/133257] Loss: 0.135004 Acc: 0.9375\n",
            "Train Epoch: 18 [44800/133257] Loss: 0.068515 Acc: 1.0000\n",
            "Train Epoch: 18 [48000/133257] Loss: 0.041577 Acc: 1.0000\n",
            "Train Epoch: 18 [51200/133257] Loss: 0.061625 Acc: 0.9688\n",
            "Train Epoch: 18 [54400/133257] Loss: 0.139580 Acc: 0.9375\n",
            "Train Epoch: 18 [57600/133257] Loss: 0.024669 Acc: 1.0000\n",
            "Train Epoch: 18 [60800/133257] Loss: 0.356139 Acc: 0.9062\n",
            "Train Epoch: 18 [64000/133257] Loss: 0.283723 Acc: 0.9062\n",
            "Train Epoch: 18 [67200/133257] Loss: 0.134047 Acc: 0.9375\n",
            "Train Epoch: 18 [70400/133257] Loss: 0.135812 Acc: 0.9688\n",
            "Train Epoch: 18 [73600/133257] Loss: 0.052834 Acc: 1.0000\n",
            "Train Epoch: 18 [76800/133257] Loss: 0.012546 Acc: 1.0000\n",
            "Train Epoch: 18 [80000/133257] Loss: 0.035751 Acc: 1.0000\n",
            "Train Epoch: 18 [83200/133257] Loss: 0.126747 Acc: 0.9375\n",
            "Train Epoch: 18 [86400/133257] Loss: 0.035119 Acc: 1.0000\n",
            "Train Epoch: 18 [89600/133257] Loss: 0.035611 Acc: 1.0000\n",
            "Train Epoch: 18 [92800/133257] Loss: 0.296733 Acc: 0.8750\n",
            "Train Epoch: 18 [96000/133257] Loss: 0.089093 Acc: 0.9688\n",
            "Train Epoch: 18 [99200/133257] Loss: 0.167937 Acc: 0.9062\n",
            "Train Epoch: 18 [102400/133257] Loss: 0.099759 Acc: 0.9375\n",
            "Train Epoch: 18 [105600/133257] Loss: 0.100645 Acc: 0.9688\n",
            "Train Epoch: 18 [108800/133257] Loss: 0.296450 Acc: 0.9062\n",
            "Train Epoch: 18 [112000/133257] Loss: 0.070363 Acc: 0.9688\n",
            "Train Epoch: 18 [115200/133257] Loss: 0.059520 Acc: 0.9688\n",
            "Train Epoch: 18 [118400/133257] Loss: 0.081393 Acc: 0.9688\n",
            "Train Epoch: 18 [121600/133257] Loss: 0.288398 Acc: 0.9375\n",
            "Train Epoch: 18 [124800/133257] Loss: 0.137969 Acc: 0.9688\n",
            "Train Epoch: 18 [128000/133257] Loss: 0.013574 Acc: 1.0000\n",
            "Train Epoch: 18 [131200/133257] Loss: 0.122899 Acc: 0.9688\n",
            "Elapsed 798.11s, 42.01 s/epoch, 0.01 s/batch, ets 42.01s\n",
            "\n",
            "Test set: Average loss: 0.2815, Accuracy: 33222/36032 (92%)\n",
            "\n",
            "Train Epoch: 19 [3200/133257] Loss: 0.107878 Acc: 0.9688\n",
            "Train Epoch: 19 [6400/133257] Loss: 0.118651 Acc: 0.9688\n",
            "Train Epoch: 19 [9600/133257] Loss: 0.102978 Acc: 0.9688\n",
            "Train Epoch: 19 [12800/133257] Loss: 0.052354 Acc: 1.0000\n",
            "Train Epoch: 19 [16000/133257] Loss: 0.126441 Acc: 0.9688\n",
            "Train Epoch: 19 [19200/133257] Loss: 0.121704 Acc: 0.9688\n",
            "Train Epoch: 19 [22400/133257] Loss: 0.063772 Acc: 0.9688\n",
            "Train Epoch: 19 [25600/133257] Loss: 0.220212 Acc: 0.9375\n",
            "Train Epoch: 19 [28800/133257] Loss: 0.164295 Acc: 0.9688\n",
            "Train Epoch: 19 [32000/133257] Loss: 0.219165 Acc: 0.9375\n",
            "Train Epoch: 19 [35200/133257] Loss: 0.153581 Acc: 0.9688\n",
            "Train Epoch: 19 [38400/133257] Loss: 0.136999 Acc: 0.9688\n",
            "Train Epoch: 19 [41600/133257] Loss: 0.259860 Acc: 0.9375\n",
            "Train Epoch: 19 [44800/133257] Loss: 0.105590 Acc: 0.9375\n",
            "Train Epoch: 19 [48000/133257] Loss: 0.292808 Acc: 0.9062\n",
            "Train Epoch: 19 [51200/133257] Loss: 0.279347 Acc: 0.9062\n",
            "Train Epoch: 19 [54400/133257] Loss: 0.071038 Acc: 0.9688\n",
            "Train Epoch: 19 [57600/133257] Loss: 0.206521 Acc: 0.9062\n",
            "Train Epoch: 19 [60800/133257] Loss: 0.144480 Acc: 0.9688\n",
            "Train Epoch: 19 [64000/133257] Loss: 0.182295 Acc: 0.9375\n",
            "Train Epoch: 19 [67200/133257] Loss: 0.058405 Acc: 0.9688\n",
            "Train Epoch: 19 [70400/133257] Loss: 0.037472 Acc: 1.0000\n",
            "Train Epoch: 19 [73600/133257] Loss: 0.097357 Acc: 0.9688\n",
            "Train Epoch: 19 [76800/133257] Loss: 0.140988 Acc: 0.9688\n",
            "Train Epoch: 19 [80000/133257] Loss: 0.216338 Acc: 0.9375\n",
            "Train Epoch: 19 [83200/133257] Loss: 0.073899 Acc: 0.9688\n",
            "Train Epoch: 19 [86400/133257] Loss: 0.144069 Acc: 0.9062\n",
            "Train Epoch: 19 [89600/133257] Loss: 0.128973 Acc: 0.9375\n",
            "Train Epoch: 19 [92800/133257] Loss: 0.079435 Acc: 0.9688\n",
            "Train Epoch: 19 [96000/133257] Loss: 0.047882 Acc: 0.9688\n",
            "Train Epoch: 19 [99200/133257] Loss: 0.115901 Acc: 0.9688\n",
            "Train Epoch: 19 [102400/133257] Loss: 0.121079 Acc: 0.9688\n",
            "Train Epoch: 19 [105600/133257] Loss: 0.128968 Acc: 0.9688\n",
            "Train Epoch: 19 [108800/133257] Loss: 0.141863 Acc: 0.9688\n",
            "Train Epoch: 19 [112000/133257] Loss: 0.088919 Acc: 0.9688\n",
            "Train Epoch: 19 [115200/133257] Loss: 0.151659 Acc: 0.9688\n",
            "Train Epoch: 19 [118400/133257] Loss: 0.074410 Acc: 1.0000\n",
            "Train Epoch: 19 [121600/133257] Loss: 0.032708 Acc: 1.0000\n",
            "Train Epoch: 19 [124800/133257] Loss: 0.162468 Acc: 0.9375\n",
            "Train Epoch: 19 [128000/133257] Loss: 0.197756 Acc: 0.9062\n",
            "Train Epoch: 19 [131200/133257] Loss: 0.174287 Acc: 0.8750\n",
            "Elapsed 840.88s, 42.04 s/epoch, 0.01 s/batch, ets 0.00s\n",
            "\n",
            "Test set: Average loss: 0.2843, Accuracy: 33180/36032 (92%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUMHs1Revk4O",
        "colab_type": "code",
        "outputId": "acdcacfd-3177-485d-831e-97cd808da076",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"Total time: {:.2f}, Best Loss: {:.3f}\".format(time.time() - t_begin, best_loss))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total time: 849.06, Best Loss: 0.277\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeeFfy2I99GO",
        "colab_type": "code",
        "outputId": "9ad6464e-c9e7-4577-f0df-47dfcc08bb71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        }
      },
      "source": [
        "# Plot loss\n",
        "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
        "x = range(len(epoch_train_loss))\n",
        "\n",
        "plt.plot(x, epoch_train_loss, 'r', label=\"train loss\")\n",
        "plt.plot(x, epoch_test_loss, 'b',label=\"validation loss\")\n",
        "\n",
        "plt.xlabel('epoch no.')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmEAAAGDCAYAAABjkcdfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJEmSOrAIkyRJ6uBv6WYNXcldI2MAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtAuFBvK99I9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#save model with best loss\n",
        "torch.save(best_model.state_dict(), '../content/gdrive/My Drive/models/MNIST_SVHM_model.pth')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TXaeqk499Kj",
        "colab_type": "code",
        "outputId": "4070e320-0cf8-43eb-f6b8-e4262c4efd84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "#download and test model\n",
        "model = LeNetBN()\n",
        "model.load_state_dict(torch.load('../content/gdrive/My Drive/models/MNIST_SVHM_model.pth'))\n",
        "#print(model1)\n",
        "images, labels = next(iter(test_loader))\n",
        "plt.imshow(images[0][0],'gray')\n",
        "image = images[0,:]#.to(device)\n",
        "image = image[None]\n",
        "plt.show()\n",
        "score = model(image)\n",
        "prob = nn.functional.softmax(score[0], dim=0)\n",
        "y_pred =  prob.argmax()\n",
        "print(\"Predicted class {} with probability {}\".format(y_pred, prob[y_pred]))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWgAAAFlCAYAAADGe3ILAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAThElEQVR4nO3db4xddZ3H8c+HoWipIi1lsdDuthTE\n0HWpm6bWgBuQP1KfgESNJItsYlJNJMGsD5bwQGGzm7gb/6yPWGsgdqNIUerSoNm1QQiQrC2ltPwr\nSpESWtqppC20Eqkz/e6DOU2GOtP5duZc7vf2vl/JpPee+cw9v8MpH36cub97HBECANRzUrcHAAAY\nGwUNAEVR0ABQFAUNAEVR0ABQFAUNAEWd/E7uzDbv6QOAo0SEx9o+pRm07att/8b2Ntu3TOW1AABv\n58kuVLE9IOm3kq6UtEPS45Kuj4jnjvEzzKAB4CidmEEvlbQtIn4XEYck3SPpmim8HgBglKkU9DmS\nXhn1fEezDQDQgo7/ktD2CkkrOr0fADjRTKWgd0qaN+r53Gbb20TESkkrJa5BA8DxmMoljsclnW97\nge1TJH1O0tp2hgUAmPQMOiKGbN8k6X8lDUi6KyKebW1kANDnJv02u0ntjEscAPBnOrJQBQDQORQ0\nABRFQQNAURQ0ABRFQQNAURQ0ABRFQQNAURQ0ABRFQQNAURQ0ABRFQQNAURQ0ABRFQQNAURQ0ABRF\nQQNAURQ0ABRFQQNAURQ0ABRFQQNAURQ0ABRFQQNAURQ0ABRFQQNAURQ0ABRFQQNAURQ0ABRFQQNA\nURQ0ABRFQQNAURQ0ABRFQQNAURQ0ABRFQQNAURQ0ABRFQQNAURQ0ABRFQQNAURQ0ABRFQQNAURQ0\nABRFQQNAURQ0ABR18lR+2PZ2SQckDUsaioglbQwKADDFgm5cFhGvtfA6AIBRuMQBAEVNtaBD0i9t\nP2F7RRsDAgCMmOoljksiYqftv5C0zvbzEfHI6EBT3JQ3ABwnR0Q7L2TfJulgRHzzGJl2dgYAJ5CI\n8FjbJ32Jw/YM2+898ljSVZKemezrAQDebiqXOM6S9DPbR17n7oj4n1ZGBQBo7xJHamdc4gCAP9P6\nJQ4AQGdR0ABQFAUNAEVR0ABQFAUNAEVR0ABQFAUNAEVR0ABQFAUNAEVR0ABQVBt3VMExNJ9VkjJt\n2rRWc20v4x8aGkpnh4eHU7nsGA8fPpzeN3CiYAYNAEVR0ABQFAUNAEVR0ABQFAUNAEVR0ABQFAUN\nAEVR0ABQFAUNAEWxkrDDTjvttHT2yiuvTOU+85nPpHJvvPFGKvfmm2+mck888UQqJ0lbtmxJ5Xbv\n3p3KDQ4OpvcNnCiYQQNAURQ0ABRFQQNAURQ0ABRFQQNAURQ0ABRFQQNAURQ0ABRFQQNAUW77vnXH\n3Jn9zu2siPPOOy+d/eEPf5jKXXDBBalc2/f7y644lKTXX389lXvttddSuVdeeSW9b4wve1/JXbt2\npXI/+clPUrnnn38+lZOkP/7xj+nsiSIixrx5KTNoACiKggaAoihoACiKggaAoihoACiKggaAoiho\nACiKggaAoihoACiKexJ2WPa+gJK0Zs2aVO4DH/hAKvf73/8+lXvf+96Xys2dOzeVk6T58+encosW\nLWo1l13BOHPmzFROkk46qd15THbl5ltvvZV+zeHh4VTu1FNPTeWyf28PHDiQyu3cuTOVk/pzJeF4\nmEEDQFETFrTtu2zvsf3MqG2zbK+z/ULzZ346AgBIycygfyDp6qO23SLpwYg4X9KDzXMAQIsmLOiI\neETS3qM2XyNpVfN4laRrWx4XAPS9yV6DPisijnwe4W5JZ7U0HgBAY8rv4oiIONbnPNteIWnFVPcD\nAP1msjPoQdtzJKn5c894wYhYGRFLImLJJPcFAH1psgW9VtKNzeMbJd3fznAAAEdk3mb3Y0n/J+kC\n2ztsf0HSNyRdafsFSVc0zwEALZrwGnREXD/Oty5veSwnpP3796ezq1evTuWyq+Cyq7ymT5+eys2a\nNSuVk6Szzz47lVuwYEEqN3v27FRu+/btqVx2NaYkDQwMpLMZ2VV/+/btS7/m+9///lTuhhtuSOVm\nzJiRymVXJra9GrNf8E8NAIqioAGgKAoaAIqioAGgKAoaAIqioAGgKAoaAIqioAGgKAoaAIqioAGg\nKG4a22GHDh1KZ19++eVWc9108sm5v1qnnXZaq7k9e8b9YMW3yS5Fl9pfppxd6p39ZyhJV111VSo3\nNDSUyu3de/Q9Osa2cePGVO7NN99M5fB2zKABoCgKGgCKoqABoCgKGgCKoqABoCgKGgCKoqABoCgK\nGgCKoqABoChWEqIj2l6xls1lbdu2rdXXOx7ZFYLLli1Lv+bHPvaxVC57Xh5++OFU7te//nUq94c/\n/CGVw9sxgwaAoihoACiKggaAoihoACiKggaAoihoACiKggaAoihoACiKggaAolhJCLzDzjjjjFTu\niiuuSL/m8uXLU7nBwcFU7u67707l9u3bl8odPnw4lcPbMYMGgKIoaAAoioIGgKIoaAAoioIGgKIo\naAAoioIGgKIoaAAoioIGgKJYSQi0ZGBgIJXL3mswe59BKX+fw1dffTWVe+mll1I5Vgh2FjNoAChq\nwoK2fZftPbafGbXtNts7bW9uvj7Z2WECQP/JzKB/IOnqMbZ/JyIWN1+/aHdYAIAJCzoiHpG09x0Y\nCwBglKlcg77J9lPNJZCZrY0IACBp8gV9h6SFkhZL2iXpW+MFba+wvdH2xknuCwD60qQKOiIGI2I4\nIg5L+r6kpcfIroyIJRGxZLKDBIB+NKmCtj1n1NNPSXpmvCwAYHImfHe77R9LulTSbNs7JH1d0qW2\nF0sKSdslfbGDYwSAvjRhQUfE9WNsvrMDYwF62sKFC1O56667LpW7+OKL0/vetGlTKnfrrbemci++\n+GIqNzw8nMphclhJCABFUdAAUBQFDQBFUdAAUBQFDQBFUdAAUBQFDQBFUdAAUBQFDQBFUdAAUBQ3\njQVasmjRolTu3HPPTeX27duX3vf69etTuS1btqRyLOGugRk0ABRFQQNAURQ0ABRFQQNAURQ0ABRF\nQQNAURQ0ABRFQQNAURQ0ABTFSkJgAtOmTUvlLrroolTu7LPPTuWeeuqpVE6Sfv7zn6dyBw8eTL8m\nuo8ZNAAURUEDQFEUNAAURUEDQFEUNAAURUEDQFEUNAAURUEDQFEUNAAUxUpCYAIf+chHUrlly5al\ncrZTuQ0bNqRykrRp06ZULiLSr4nuYwYNAEVR0ABQFAUNAEVR0ABQFAUNAEVR0ABQFAUNAEVR0ABQ\nFAUNAEWxkhAnlJNOys055s2bl37Nz3/+86ncokWLUrnsvQYfe+yxVE6S9u7dm86idzCDBoCiJixo\n2/NsP2T7OdvP2r652T7L9jrbLzR/zuz8cAGgf2Rm0EOSvhoRF0paJunLti+UdIukByPifEkPNs8B\nAC2ZsKAjYldEbGoeH5C0VdI5kq6RtKqJrZJ0bacGCQD96LiuQdueL+nDktZLOisidjXf2i3prFZH\nBgB9Lv0uDtvvkXSfpK9ExBujP9M2IsL2mB80a3uFpBVTHSgA9JvUDNr2NI2U848iYk2zedD2nOb7\ncyTtGetnI2JlRCyJiCVtDBgA+kXmXRyWdKekrRHx7VHfWivpxubxjZLub394ANC/Mpc4LpZ0g6Sn\nbW9utt0q6RuS7rX9BUkvS/psZ4YIAP1pwoKOiMckjXcTtcvbHQ4A4AiWeqMnZG+0OmPGjFTuuuuu\nS+97+fLlqVx2mfnDDz+cyj355JOpHE5cLPUGgKIoaAAoioIGgKIoaAAoioIGgKIoaAAoioIGgKIo\naAAoioIGgKJYSYiecOqpp6ZyS5bkPjTxS1/6Unrfs2bNSuXWrVuXyq1fvz6V27NnzA+IRB9hBg0A\nRVHQAFAUBQ0ARVHQAFAUBQ0ARVHQAFAUBQ0ARVHQAFAUBQ0ARbGSEF2VvY/f/PnzU7nvfve7qdzC\nhQtTOUnavn17Krd69epUbvPmzel9o78xgwaAoihoACiKggaAoihoACiKggaAoihoACiKggaAoiho\nACiKggaAolhJiK6aPn16KnfuueemchdeeGEqNzAwkMpJ0ve+971U7tFHH03lDhw4kN43+hszaAAo\nioIGgKIoaAAoioIGgKIoaAAoioIGgKIoaAAoioIGgKIoaAAoipWE6IgZM2akcpdddlkq97WvfS2V\nGxoaSuWyqwMlae3atanc4OBgKhcR6X2jvzGDBoCiJixo2/NsP2T7OdvP2r652X6b7Z22Nzdfn+z8\ncAGgf2QucQxJ+mpEbLL9XklP2F7XfO87EfHNzg0PAPrXhAUdEbsk7WoeH7C9VdI5nR4YAPS747oG\nbXu+pA9LWt9susn2U7bvsj2z5bEBQF9LF7Tt90i6T9JXIuINSXdIWihpsUZm2N8a5+dW2N5oe2ML\n4wWAvpEqaNvTNFLOP4qINZIUEYMRMRwRhyV9X9LSsX42IlZGxJKIWNLWoAGgH2TexWFJd0raGhHf\nHrV9zqjYpyQ90/7wAKB/Zd7FcbGkGyQ9bXtzs+1WSdfbXiwpJG2X9MWOjBAA+lTmXRyPSfIY3/pF\n+8MBABzBUm90xIIFC1K5T3ziE6nchz70oVTuT3/6Uyr3q1/9KpWTpF27drW6byCLpd4AUBQFDQBF\nUdAAUBQFDQBFUdAAUBQFDQBFUdAAUBQFDQBFUdAAUBQrCZF2xhlnpLOXXHJJKnf55Zencu9617tS\nuUOHDqVye/fuTeWk/I1ogbYxgwaAoihoACiKggaAoihoACiKggaAoihoACiKggaAoihoACiKggaA\nolhJiLR58+als0uXLk3lzj///FRueHg4lXv99ddTueyKQ0mKiHQWaBMzaAAoioIGgKIoaAAoioIG\ngKIoaAAoioIGgKIoaAAoioIGgKIoaAAoipWESJs9e3Y6e+aZZ6Zy2VV6r776air3wAMPpHK7d+9O\n5aT8KkagbcygAaAoChoAiqKgAaAoChoAiqKgAaAoChoAiqKgAaAoChoAiqKgAaAoChoAimKpN9IO\nHDiQzr700kup3Pr161O5DRs2pHK33357Knc8x8JNY9EtE86gbb/b9gbbW2w/a/v2ZvsC2+ttb7O9\n2vYpnR8uAPSPzCWOtyR9PCIukrRY0tW2l0n6N0nfiYjzJO2T9IXODRMA+s+EBR0jDjZPpzVfIenj\nkn7abF8l6dqOjBAA+lTql4S2B2xvlrRH0jpJL0raHxFDTWSHpHM6M0QA6E+pgo6I4YhYLGmupKWS\nPpjdge0Vtjfa3jjJMQJAXzqut9lFxH5JD0n6qKTTbR95F8hcSTvH+ZmVEbEkIpZMaaQA0Gcy7+I4\n0/bpzePpkq6UtFUjRf3pJnajpPs7NUgA6EeZ90HPkbTK9oBGCv3eiHjA9nOS7rH9L5KelHRnB8cJ\nAH1nwoKOiKckfXiM7b/TyPVoAEAH+J1cJWWbJVkAcJSI8Fjb+SwOACiKggaAoihoACiKggaAoiho\nACiKggaAoihoACiKggaAoihoACjqnb4n4WuSXj5q2+xm+4mAY6mJY6mJYxnxV+N94x1d6j3mAOyN\nJ8pHkXIsNXEsNXEsE+MSBwAURUEDQFEVCnpltwfQIo6lJo6lJo5lAl2/Bg0AGFuFGTQAYAxdLWjb\nV9v+je1ttm/p5limyvZ220/b3txrdzC3fZftPbafGbVtlu11tl9o/pzZzTFmjXMst9ne2ZybzbY/\n2c0xZtieZ/sh28/Zftb2zc32njsvxziWXjwv77a9wfaW5lhub7YvsL2+6bLVtk9pZX/dusTR3OPw\ntxq5Ce0OSY9Luj4inuvKgKbI9nZJSyKi597XafvvJB2U9F8R8dfNtn+XtDcivtH8x3NmRPxTN8eZ\nMc6x3CbpYER8s5tjOx6250iaExGbbL9X0hOSrpX0D+qx83KMY/mseu+8WNKMiDhoe5qkxyTdLOkf\nJa2JiHts/6ekLRFxx1T3180Z9FJJ2yLidxFxSNI9kq7p4nj6VkQ8ImnvUZuvkbSqebxKI/9ClTfO\nsfSciNgVEZuaxwckbZV0jnrwvBzjWHpOjDjYPJ3WfIWkj0v6abO9tfPSzYI+R9Iro57vUI+etEZI\n+qXtJ2yv6PZgWnBWROxqHu+WdFY3B9OCm2w/1VwCKX9ZYDTb8zVy4+b16vHzctSxSD14XmwP2N4s\naY+kdZJelLQ/IoaaSGtdxi8J23NJRPytpOWSvtz8r/YJIUaug/Xy233ukLRQ0mJJuyR9q7vDybP9\nHkn3SfpKRLwx+nu9dl7GOJaePC8RMRwRiyXN1ciVgA92al/dLOidkuaNej632daTImJn8+ceST/T\nyInrZYPNtcMj1xD3dHk8kxYRg82/VIclfV89cm6aa5z3SfpRRKxpNvfkeRnrWHr1vBwREfslPSTp\no5JOt33ks41a67JuFvTjks5vfvt5iqTPSVrbxfFMmu0ZzS8/ZHuGpKskPXPsnypvraQbm8c3Srq/\ni2OZkiOF1viUeuDcNL+MulPS1oj49qhv9dx5Ge9YevS8nGn79ObxdI28yWGrRor6002stfPS1YUq\nzdtq/kPSgKS7IuJfuzaYKbB9rkZmzdLIJwTe3UvHYvvHki7VyCdyDUr6uqT/lnSvpL/UyCcQfjYi\nyv/ybZxjuVQj/xsdkrZL+uKo67gl2b5E0qOSnpZ0uNl8q0au3fbUeTnGsVyv3jsvf6ORXwIOaGSC\ne29E/HPTAfdImiXpSUl/HxFvTXl/rCQEgJr4JSEAFEVBA0BRFDQAFEVBA0BRFDQAFEVBA0BRFDQA\nFEVBA0BR/w+3yPveWL4lYwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Predicted class 7 with probability 0.9999582767486572\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHZWLiNkUHjk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "test = pd.read_csv(\"../content/gdrive/My Drive/MNIST/test.csv\")\n",
        "test_image = test.loc[:,test.columns != \"label\"]\n",
        "test_dataset = torch.from_numpy(np.reshape(test_image.to_numpy().astype(np.uint8), (test_image.shape[0], 1, 28,28)))\n",
        "results = []\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    for img in test_dataset:\n",
        "        img = transforms.ToPILImage()(img)\n",
        "        img = transforms.Resize((32, 32))(img)\n",
        "        img = transforms.ToTensor()(img)\n",
        "        img = transforms.Normalize((0.1307, ), (0.3081, ))(img)\n",
        "        test_im = img#.to(device)\n",
        "        test_im = test_im[None]\n",
        "        output = model(test_im)\n",
        "        prob = nn.functional.softmax(output[0], dim=0)\n",
        "        y_pred =  prob.argmax()\n",
        "        results.append( y_pred.cpu().data.numpy().tolist())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eA6dqcyzwMy6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e977f732-281b-4bba-f4ca-a8777d2ca4ab"
      },
      "source": [
        "len(results)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhHO-Ii9wOpI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = np.array(results).flatten()\n",
        "submissions=pd.DataFrame({\"ImageId\": list(range(1,len(predictions)+1)),\n",
        "                         \"Label\": predictions})\n",
        "submissions.to_csv(\"../content/gdrive/My Drive/MNIST/my_submissions01.csv\", index=False, header=True)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}