{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TrainDigitRecognizer_MNIST_aug.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_fPhAC9u2FE",
        "colab_type": "code",
        "outputId": "79958ec2-585c-4c92-a784-7ab681528c47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#use Google Coolab with free GPU to train model\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWQfJ0MPvGXJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import time\n",
        "import copy\n",
        "\n",
        "from typing import Iterable\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MIBzDDPvI80",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#LeNet with Batch Normalization\n",
        "class LeNetBN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # convolution layers\n",
        "        self._body = nn.Sequential(\n",
        "            # First convolution Layer\n",
        "            # input size = (32, 32), output size = (28, 28)\n",
        "            nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5),\n",
        "            nn.BatchNorm2d(6),\n",
        "            # ReLU activation\n",
        "            nn.ReLU(inplace=True),\n",
        "            # Max pool 2-d\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            \n",
        "            # Second convolution layer\n",
        "            # input size = (14, 14), output size = (10, 10)\n",
        "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            # output size = (5, 5)\n",
        "        )\n",
        "        \n",
        "        # Fully connected layers\n",
        "        self._head = nn.Sequential(\n",
        "            # First fully connected layer\n",
        "            # in_features = total number of weight in last conv layer = 16 * 5 * 5\n",
        "            nn.Linear(in_features=16 * 5 * 5, out_features=120), \n",
        "            \n",
        "            # ReLU activation\n",
        "            nn.ReLU(inplace=True),\n",
        "            \n",
        "            # second fully connected layer\n",
        "            # in_features = output of last linear layer = 120 \n",
        "            nn.Linear(in_features=120, out_features=84), \n",
        "            \n",
        "            # ReLU activation\n",
        "            nn.ReLU(inplace=True),\n",
        "            \n",
        "            # Third fully connected layer. It is also output layer\n",
        "            # in_features = output of last linear layer = 84\n",
        "            # and out_features = number of classes = 10 (0-9)\n",
        "            nn.Linear(in_features=84, out_features=10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # apply feature extractor\n",
        "        x = self._body(x)\n",
        "        # flatten the output of conv layers\n",
        "        # dimension should be batch_size * number_of weight_in_last conv_layer\n",
        "        x = x.view(x.size()[0], -1)\n",
        "        # apply classification head\n",
        "        x = self._head(x)\n",
        "        return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0skH0fkYQSsj",
        "colab_type": "code",
        "outputId": "3901d15a-b41e-4e48-90e0-11bef59ab472",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        }
      },
      "source": [
        "model = LeNetBN()\n",
        "print(model)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LeNetBN(\n",
            "  (_body): Sequential(\n",
            "    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (_head): Sequential(\n",
            "    (0): Linear(in_features=400, out_features=120, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Linear(in_features=120, out_features=84, bias=True)\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): Linear(in_features=84, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRAT2WWsvWQz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_test_transforms_MNIST = transforms.Compose([\n",
        "    #resize to (32,32)                                              \n",
        "    transforms.Resize((32, 32)),\n",
        "    # re-scale image tensor values between 0-1. image_tensor /= 255\n",
        "    transforms.ToTensor(),\n",
        "    # subtract mean (0.1216) and divide by variance (0.2796).\n",
        "    # This mean and variance is calculated on training data\n",
        "    transforms.Normalize((0.1216, ), (0.2796, ))\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Jzggwr8TOwo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#download data from Google Drive\n",
        "trainMNIST = datasets.MNIST('../content/gdrive/My Drive', train=True, download=False, transform=train_test_transforms_MNIST)\n",
        "testMNIST = datasets.MNIST('../content/gdrive/My Drive', train=False, download=False, transform=train_test_transforms_MNIST)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSPTxVZOFBz1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "242acc92-13c8-4e90-956c-df8d15b4261d"
      },
      "source": [
        "#generate diferent image of digits bby OpenCV\n",
        "train_aug = []\n",
        "for digit in range(10):\n",
        "    for font in [0,2,3,4,6,7]:\n",
        "        for scale in [0.8, 1.0]:\n",
        "            for color in [70, 100, 120, 160, 200, 220, 255]:\n",
        "                for thik in [0,1,2]:\n",
        "                    for line in [cv2.FILLED, cv2.LINE_4, cv2.LINE_8, cv2.LINE_AA]:\n",
        "                        img = np.zeros((32,32), dtype=np.uint8)\n",
        "                        img = cv2.putText(img, str(digit), (6,25), font, scale, color, thik, line)\n",
        "                        if(len(train_aug)==500):\n",
        "                          #for example\n",
        "                          plt.imshow(img,'gray');plt.show()\n",
        "                        train_aug.append((transforms.Normalize((0.1216, ), (0.2796, ))(transforms.ToTensor()(img)), digit)) #transform to tensor and normalize. put in list"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWrFRuVcE-P1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "59f8650b-45f3-4037-9b1a-ea4aae429d99"
      },
      "source": [
        "test_aug = []\n",
        "for digit in range(10):\n",
        "    for scale in [0.8, 1.0]:\n",
        "        for color in [70, 100, 120, 160, 200, 220, 255]:\n",
        "            for thik in [0,1,2]:\n",
        "                for line in [cv2.FILLED, cv2.LINE_4, cv2.LINE_8, cv2.LINE_AA]:\n",
        "                    img = np.zeros((32,32), dtype=np.uint8)\n",
        "                    img = cv2.putText(img, str(digit), (6,25), 16, scale, color, thik, line)\n",
        "                    #plt.imshow(img,'gray');plt.show()\n",
        "                    test_aug.append((transforms.Normalize((0.1216, ), (0.2796, ))(transforms.ToTensor()(img)), digit))#transform to tensor and normalize. put in list\n",
        "print(len(test_aug))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1680\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kk2gufzwQfgs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.manual_seed(42)\n",
        "#if torch.cuda.is_available():\n",
        "#    torch.backend.cudnn_benchmark_enabled = True\n",
        "#    torch.backend.cudnn.deterministic = True\n",
        "\n",
        "#training parameters\n",
        "batch_size = 32\n",
        "epochs_count = 20\n",
        "learning_rate = 0.01\n",
        "log_interval = 100\n",
        "test_interval = 1\n",
        "num_workers  = 10\n",
        "device = 'cuda'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhG-KqsEviSQ",
        "colab_type": "code",
        "outputId": "a3b88f2c-2ccb-40e1-ceae-e164fb532147",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "    print('GPU')\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "    batch_size = 16\n",
        "    num_workers = 0\n",
        "    epochs_count = 10\n",
        "    print('CPU')\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "#SGDoptimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "#array for plotting\n",
        "best_loss = torch.tensor(np.inf)\n",
        "\n",
        "# epoch train/test loss\n",
        "epoch_train_loss = np.array([])\n",
        "epoch_test_loss = np.array([])\n",
        "\n",
        "# epch train/test accuracy\n",
        "epoch_train_acc = np.array([])\n",
        "epoch_test_acc = np.array([])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOdWeCXxvkaC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train dataloader\n",
        "train_loader_MNIST = torch.utils.data.DataLoader(\n",
        "     trainMNIST + train_aug,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=num_workers\n",
        ")\n",
        "# test dataloader\n",
        "test_loader_MNIST = torch.utils.data.DataLoader(\n",
        "    testMNIST + test_aug,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=num_workers\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VE6H7m-svksn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, optimizer, train_loader, epoch_idx):\n",
        "    # change model in training mood\n",
        "    model.train()\n",
        "\n",
        "    # to get batch loss\n",
        "    batch_loss = np.array([])\n",
        "\n",
        "    # to get batch accuracy\n",
        "    batch_acc = np.array([])\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "\n",
        "        # clone target\n",
        "        indx_target = target.clone()\n",
        "        # send data to device (its is medatory if GPU has to be used)\n",
        "        data = data.to(device)\n",
        "        # send target to device\n",
        "        target = target.to(device)\n",
        "\n",
        "        # reset parameters gradient to zero\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward pass to the model\n",
        "        output = model(data)\n",
        "\n",
        "        # cross entropy loss\n",
        "        loss = F.cross_entropy(output, target)\n",
        "\n",
        "        # find gradients w.r.t training parameters\n",
        "        loss.backward()\n",
        "        # Update parameters using gardients\n",
        "        optimizer.step()\n",
        "\n",
        "        batch_loss = np.append(batch_loss, [loss.item()])\n",
        "\n",
        "        # Score to probability using softmax\n",
        "        prob = F.softmax(output, dim=1)\n",
        "\n",
        "        # get the index of the max probability\n",
        "        pred = prob.data.max(dim=1)[1]\n",
        "\n",
        "        # correct prediction\n",
        "        correct = pred.cpu().eq(indx_target).sum()\n",
        "\n",
        "        # accuracy\n",
        "        acc = float(correct) / float(len(data))\n",
        "\n",
        "        batch_acc = np.append(batch_acc, [acc])\n",
        "\n",
        "        if batch_idx % log_interval == 0 and batch_idx > 0:\n",
        "            print(\n",
        "                'Train Epoch: {} [{}/{}] Loss: {:.6f} Acc: {:.4f}'.format(\n",
        "                    epoch_idx, batch_idx * len(data), len(train_loader.dataset), loss.item(), acc\n",
        "                )\n",
        "            )\n",
        "\n",
        "    epoch_loss = batch_loss.mean()\n",
        "    epoch_acc = batch_acc.mean()\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "\n",
        "def validate(model, test_loader):\n",
        "    #\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    count_corect_predictions = 0\n",
        "    for data, target in test_loader:\n",
        "        indx_target = target.clone()\n",
        "        data = data.to(device)\n",
        "\n",
        "        target = target.to(device)\n",
        "\n",
        "        output = model(data)\n",
        "        # add loss for each mini batch\n",
        "        test_loss += F.cross_entropy(output, target).item()\n",
        "\n",
        "        # Score to probability using softmax\n",
        "        prob = F.softmax(output, dim=1)\n",
        "\n",
        "        # get the index of the max probability\n",
        "        pred = prob.data.max(dim=1)[1]\n",
        "\n",
        "        # add correct prediction count\n",
        "        count_corect_predictions += pred.cpu().eq(indx_target).sum()\n",
        "\n",
        "    # average over number of mini-batches\n",
        "    test_loss = test_loss / len(test_loader)\n",
        "\n",
        "    # average over number of dataset\n",
        "    accuracy = 100. * count_corect_predictions / len(test_loader.dataset)\n",
        "\n",
        "    print(\n",
        "        '\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "            test_loss, count_corect_predictions, len(test_loader.dataset), accuracy\n",
        "        )\n",
        "    )\n",
        "    return test_loss, accuracy / 100.0\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjythJS-vkv_",
        "colab_type": "code",
        "outputId": "a9e77a03-84a1-4c55-9743-25f94f488729",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#train\n",
        "t_begin = time.time()\n",
        "#for save best model\n",
        "best_model = copy.deepcopy(model)\n",
        "for epoch in range(epochs_count):\n",
        "\n",
        "    train_loss, train_acc = train(model, optimizer, train_loader_MNIST, epoch)\n",
        "\n",
        "    epoch_train_loss = np.append(epoch_train_loss, [train_loss])\n",
        "\n",
        "    epoch_train_acc = np.append(epoch_train_acc, [train_acc])\n",
        "\n",
        "    elapsed_time = time.time() - t_begin\n",
        "    speed_epoch = elapsed_time / (epoch + 1)\n",
        "    speed_batch = speed_epoch / len(train_loader_MNIST)\n",
        "    eta = speed_epoch * epochs_count - elapsed_time\n",
        "\n",
        "    print(\n",
        "        \"Elapsed {:.2f}s, {:.2f} s/epoch, {:.2f} s/batch, ets {:.2f}s\".format(\n",
        "            elapsed_time, speed_epoch, speed_batch, eta\n",
        "        )\n",
        "    )\n",
        "\n",
        "    if epoch % test_interval == 0:\n",
        "        current_loss, current_accuracy = validate(model, test_loader_MNIST)\n",
        "\n",
        "        epoch_test_loss = np.append(epoch_test_loss, [current_loss])\n",
        "\n",
        "        epoch_test_acc = np.append(epoch_test_acc, [current_accuracy])\n",
        "\n",
        "        if current_loss < best_loss:\n",
        "            best_loss = current_loss\n",
        "            best_model = copy.deepcopy(model)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [3200/70080] Loss: 2.129182 Acc: 0.2812\n",
            "Train Epoch: 0 [6400/70080] Loss: 1.502907 Acc: 0.7188\n",
            "Train Epoch: 0 [9600/70080] Loss: 1.071797 Acc: 0.7500\n",
            "Train Epoch: 0 [12800/70080] Loss: 0.465455 Acc: 0.9062\n",
            "Train Epoch: 0 [16000/70080] Loss: 0.482729 Acc: 0.7812\n",
            "Train Epoch: 0 [19200/70080] Loss: 0.244675 Acc: 0.9375\n",
            "Train Epoch: 0 [22400/70080] Loss: 0.406056 Acc: 0.8750\n",
            "Train Epoch: 0 [25600/70080] Loss: 0.316554 Acc: 0.9375\n",
            "Train Epoch: 0 [28800/70080] Loss: 0.268289 Acc: 0.9375\n",
            "Train Epoch: 0 [32000/70080] Loss: 0.087407 Acc: 1.0000\n",
            "Train Epoch: 0 [35200/70080] Loss: 0.217588 Acc: 0.9688\n",
            "Train Epoch: 0 [38400/70080] Loss: 0.075118 Acc: 1.0000\n",
            "Train Epoch: 0 [41600/70080] Loss: 0.057804 Acc: 1.0000\n",
            "Train Epoch: 0 [44800/70080] Loss: 0.090358 Acc: 1.0000\n",
            "Train Epoch: 0 [48000/70080] Loss: 0.093878 Acc: 1.0000\n",
            "Train Epoch: 0 [51200/70080] Loss: 0.072117 Acc: 0.9688\n",
            "Train Epoch: 0 [54400/70080] Loss: 0.071466 Acc: 1.0000\n",
            "Train Epoch: 0 [57600/70080] Loss: 0.068465 Acc: 0.9688\n",
            "Train Epoch: 0 [60800/70080] Loss: 0.176806 Acc: 0.9688\n",
            "Train Epoch: 0 [64000/70080] Loss: 0.087527 Acc: 0.9688\n",
            "Train Epoch: 0 [67200/70080] Loss: 0.025990 Acc: 1.0000\n",
            "Elapsed 21.60s, 21.60 s/epoch, 0.01 s/batch, ets 410.47s\n",
            "\n",
            "Test set: Average loss: 0.1004, Accuracy: 11392/11680 (98%)\n",
            "\n",
            "Train Epoch: 1 [3200/70080] Loss: 0.054943 Acc: 1.0000\n",
            "Train Epoch: 1 [6400/70080] Loss: 0.145675 Acc: 0.9688\n",
            "Train Epoch: 1 [9600/70080] Loss: 0.102648 Acc: 0.9688\n",
            "Train Epoch: 1 [12800/70080] Loss: 0.102544 Acc: 0.9688\n",
            "Train Epoch: 1 [16000/70080] Loss: 0.045785 Acc: 0.9688\n",
            "Train Epoch: 1 [19200/70080] Loss: 0.051242 Acc: 1.0000\n",
            "Train Epoch: 1 [22400/70080] Loss: 0.093639 Acc: 0.9688\n",
            "Train Epoch: 1 [25600/70080] Loss: 0.136588 Acc: 0.9688\n",
            "Train Epoch: 1 [28800/70080] Loss: 0.220428 Acc: 0.9688\n",
            "Train Epoch: 1 [32000/70080] Loss: 0.025888 Acc: 1.0000\n",
            "Train Epoch: 1 [35200/70080] Loss: 0.164530 Acc: 0.9688\n",
            "Train Epoch: 1 [38400/70080] Loss: 0.030475 Acc: 1.0000\n",
            "Train Epoch: 1 [41600/70080] Loss: 0.064757 Acc: 0.9688\n",
            "Train Epoch: 1 [44800/70080] Loss: 0.032338 Acc: 1.0000\n",
            "Train Epoch: 1 [48000/70080] Loss: 0.020125 Acc: 1.0000\n",
            "Train Epoch: 1 [51200/70080] Loss: 0.032754 Acc: 1.0000\n",
            "Train Epoch: 1 [54400/70080] Loss: 0.094053 Acc: 0.9688\n",
            "Train Epoch: 1 [57600/70080] Loss: 0.072042 Acc: 0.9688\n",
            "Train Epoch: 1 [60800/70080] Loss: 0.374783 Acc: 0.9062\n",
            "Train Epoch: 1 [64000/70080] Loss: 0.058609 Acc: 0.9688\n",
            "Train Epoch: 1 [67200/70080] Loss: 0.058091 Acc: 0.9688\n",
            "Elapsed 46.89s, 23.44 s/epoch, 0.01 s/batch, ets 421.98s\n",
            "\n",
            "Test set: Average loss: 0.0730, Accuracy: 11458/11680 (98%)\n",
            "\n",
            "Train Epoch: 2 [3200/70080] Loss: 0.099927 Acc: 0.9375\n",
            "Train Epoch: 2 [6400/70080] Loss: 0.019022 Acc: 1.0000\n",
            "Train Epoch: 2 [9600/70080] Loss: 0.036999 Acc: 1.0000\n",
            "Train Epoch: 2 [12800/70080] Loss: 0.092187 Acc: 0.9688\n",
            "Train Epoch: 2 [16000/70080] Loss: 0.043169 Acc: 0.9688\n",
            "Train Epoch: 2 [19200/70080] Loss: 0.024134 Acc: 1.0000\n",
            "Train Epoch: 2 [22400/70080] Loss: 0.029537 Acc: 1.0000\n",
            "Train Epoch: 2 [25600/70080] Loss: 0.069843 Acc: 0.9375\n",
            "Train Epoch: 2 [28800/70080] Loss: 0.028871 Acc: 1.0000\n",
            "Train Epoch: 2 [32000/70080] Loss: 0.017433 Acc: 1.0000\n",
            "Train Epoch: 2 [35200/70080] Loss: 0.074922 Acc: 0.9688\n",
            "Train Epoch: 2 [38400/70080] Loss: 0.019205 Acc: 1.0000\n",
            "Train Epoch: 2 [41600/70080] Loss: 0.075641 Acc: 0.9688\n",
            "Train Epoch: 2 [44800/70080] Loss: 0.012513 Acc: 1.0000\n",
            "Train Epoch: 2 [48000/70080] Loss: 0.117166 Acc: 0.9688\n",
            "Train Epoch: 2 [51200/70080] Loss: 0.197474 Acc: 0.9688\n",
            "Train Epoch: 2 [54400/70080] Loss: 0.010236 Acc: 1.0000\n",
            "Train Epoch: 2 [57600/70080] Loss: 0.013722 Acc: 1.0000\n",
            "Train Epoch: 2 [60800/70080] Loss: 0.006299 Acc: 1.0000\n",
            "Train Epoch: 2 [64000/70080] Loss: 0.253098 Acc: 0.9688\n",
            "Train Epoch: 2 [67200/70080] Loss: 0.080385 Acc: 0.9688\n",
            "Elapsed 72.05s, 24.02 s/epoch, 0.01 s/batch, ets 408.29s\n",
            "\n",
            "Test set: Average loss: 0.0553, Accuracy: 11504/11680 (98%)\n",
            "\n",
            "Train Epoch: 3 [3200/70080] Loss: 0.027724 Acc: 1.0000\n",
            "Train Epoch: 3 [6400/70080] Loss: 0.011425 Acc: 1.0000\n",
            "Train Epoch: 3 [9600/70080] Loss: 0.004631 Acc: 1.0000\n",
            "Train Epoch: 3 [12800/70080] Loss: 0.044525 Acc: 0.9688\n",
            "Train Epoch: 3 [16000/70080] Loss: 0.011715 Acc: 1.0000\n",
            "Train Epoch: 3 [19200/70080] Loss: 0.020755 Acc: 1.0000\n",
            "Train Epoch: 3 [22400/70080] Loss: 0.027707 Acc: 1.0000\n",
            "Train Epoch: 3 [25600/70080] Loss: 0.006648 Acc: 1.0000\n",
            "Train Epoch: 3 [28800/70080] Loss: 0.001326 Acc: 1.0000\n",
            "Train Epoch: 3 [32000/70080] Loss: 0.079352 Acc: 0.9688\n",
            "Train Epoch: 3 [35200/70080] Loss: 0.117744 Acc: 0.9688\n",
            "Train Epoch: 3 [38400/70080] Loss: 0.040877 Acc: 1.0000\n",
            "Train Epoch: 3 [41600/70080] Loss: 0.019456 Acc: 1.0000\n",
            "Train Epoch: 3 [44800/70080] Loss: 0.120977 Acc: 0.9688\n",
            "Train Epoch: 3 [48000/70080] Loss: 0.087392 Acc: 0.9688\n",
            "Train Epoch: 3 [51200/70080] Loss: 0.053935 Acc: 0.9688\n",
            "Train Epoch: 3 [54400/70080] Loss: 0.056915 Acc: 0.9688\n",
            "Train Epoch: 3 [57600/70080] Loss: 0.016002 Acc: 1.0000\n",
            "Train Epoch: 3 [60800/70080] Loss: 0.005887 Acc: 1.0000\n",
            "Train Epoch: 3 [64000/70080] Loss: 0.016718 Acc: 1.0000\n",
            "Train Epoch: 3 [67200/70080] Loss: 0.004439 Acc: 1.0000\n",
            "Elapsed 97.51s, 24.38 s/epoch, 0.01 s/batch, ets 390.04s\n",
            "\n",
            "Test set: Average loss: 0.0472, Accuracy: 11522/11680 (99%)\n",
            "\n",
            "Train Epoch: 4 [3200/70080] Loss: 0.022474 Acc: 1.0000\n",
            "Train Epoch: 4 [6400/70080] Loss: 0.146015 Acc: 0.9688\n",
            "Train Epoch: 4 [9600/70080] Loss: 0.037787 Acc: 1.0000\n",
            "Train Epoch: 4 [12800/70080] Loss: 0.011119 Acc: 1.0000\n",
            "Train Epoch: 4 [16000/70080] Loss: 0.255086 Acc: 0.9375\n",
            "Train Epoch: 4 [19200/70080] Loss: 0.029167 Acc: 1.0000\n",
            "Train Epoch: 4 [22400/70080] Loss: 0.024516 Acc: 1.0000\n",
            "Train Epoch: 4 [25600/70080] Loss: 0.002924 Acc: 1.0000\n",
            "Train Epoch: 4 [28800/70080] Loss: 0.017617 Acc: 1.0000\n",
            "Train Epoch: 4 [32000/70080] Loss: 0.009130 Acc: 1.0000\n",
            "Train Epoch: 4 [35200/70080] Loss: 0.007418 Acc: 1.0000\n",
            "Train Epoch: 4 [38400/70080] Loss: 0.034088 Acc: 1.0000\n",
            "Train Epoch: 4 [41600/70080] Loss: 0.043935 Acc: 1.0000\n",
            "Train Epoch: 4 [44800/70080] Loss: 0.034246 Acc: 0.9688\n",
            "Train Epoch: 4 [48000/70080] Loss: 0.009146 Acc: 1.0000\n",
            "Train Epoch: 4 [51200/70080] Loss: 0.020724 Acc: 1.0000\n",
            "Train Epoch: 4 [54400/70080] Loss: 0.011951 Acc: 1.0000\n",
            "Train Epoch: 4 [57600/70080] Loss: 0.027294 Acc: 1.0000\n",
            "Train Epoch: 4 [60800/70080] Loss: 0.154720 Acc: 0.9062\n",
            "Train Epoch: 4 [64000/70080] Loss: 0.030142 Acc: 1.0000\n",
            "Train Epoch: 4 [67200/70080] Loss: 0.033087 Acc: 1.0000\n",
            "Elapsed 122.95s, 24.59 s/epoch, 0.01 s/batch, ets 368.84s\n",
            "\n",
            "Test set: Average loss: 0.0394, Accuracy: 11533/11680 (99%)\n",
            "\n",
            "Train Epoch: 5 [3200/70080] Loss: 0.026061 Acc: 1.0000\n",
            "Train Epoch: 5 [6400/70080] Loss: 0.004032 Acc: 1.0000\n",
            "Train Epoch: 5 [9600/70080] Loss: 0.090041 Acc: 0.9375\n",
            "Train Epoch: 5 [12800/70080] Loss: 0.009532 Acc: 1.0000\n",
            "Train Epoch: 5 [16000/70080] Loss: 0.087318 Acc: 0.9688\n",
            "Train Epoch: 5 [19200/70080] Loss: 0.161711 Acc: 0.9375\n",
            "Train Epoch: 5 [22400/70080] Loss: 0.027167 Acc: 1.0000\n",
            "Train Epoch: 5 [25600/70080] Loss: 0.089774 Acc: 0.9688\n",
            "Train Epoch: 5 [28800/70080] Loss: 0.007635 Acc: 1.0000\n",
            "Train Epoch: 5 [32000/70080] Loss: 0.006200 Acc: 1.0000\n",
            "Train Epoch: 5 [35200/70080] Loss: 0.005174 Acc: 1.0000\n",
            "Train Epoch: 5 [38400/70080] Loss: 0.061656 Acc: 0.9688\n",
            "Train Epoch: 5 [41600/70080] Loss: 0.016869 Acc: 1.0000\n",
            "Train Epoch: 5 [44800/70080] Loss: 0.098629 Acc: 0.9688\n",
            "Train Epoch: 5 [48000/70080] Loss: 0.004865 Acc: 1.0000\n",
            "Train Epoch: 5 [51200/70080] Loss: 0.005762 Acc: 1.0000\n",
            "Train Epoch: 5 [54400/70080] Loss: 0.020703 Acc: 1.0000\n",
            "Train Epoch: 5 [57600/70080] Loss: 0.004463 Acc: 1.0000\n",
            "Train Epoch: 5 [60800/70080] Loss: 0.004756 Acc: 1.0000\n",
            "Train Epoch: 5 [64000/70080] Loss: 0.017236 Acc: 1.0000\n",
            "Train Epoch: 5 [67200/70080] Loss: 0.003745 Acc: 1.0000\n",
            "Elapsed 148.15s, 24.69 s/epoch, 0.01 s/batch, ets 345.67s\n",
            "\n",
            "Test set: Average loss: 0.0408, Accuracy: 11540/11680 (99%)\n",
            "\n",
            "Train Epoch: 6 [3200/70080] Loss: 0.002793 Acc: 1.0000\n",
            "Train Epoch: 6 [6400/70080] Loss: 0.002389 Acc: 1.0000\n",
            "Train Epoch: 6 [9600/70080] Loss: 0.021054 Acc: 1.0000\n",
            "Train Epoch: 6 [12800/70080] Loss: 0.030628 Acc: 0.9688\n",
            "Train Epoch: 6 [16000/70080] Loss: 0.001503 Acc: 1.0000\n",
            "Train Epoch: 6 [19200/70080] Loss: 0.092847 Acc: 0.9688\n",
            "Train Epoch: 6 [22400/70080] Loss: 0.037885 Acc: 1.0000\n",
            "Train Epoch: 6 [25600/70080] Loss: 0.020264 Acc: 1.0000\n",
            "Train Epoch: 6 [28800/70080] Loss: 0.055128 Acc: 0.9688\n",
            "Train Epoch: 6 [32000/70080] Loss: 0.013500 Acc: 1.0000\n",
            "Train Epoch: 6 [35200/70080] Loss: 0.007168 Acc: 1.0000\n",
            "Train Epoch: 6 [38400/70080] Loss: 0.019961 Acc: 1.0000\n",
            "Train Epoch: 6 [41600/70080] Loss: 0.006894 Acc: 1.0000\n",
            "Train Epoch: 6 [44800/70080] Loss: 0.002619 Acc: 1.0000\n",
            "Train Epoch: 6 [48000/70080] Loss: 0.005612 Acc: 1.0000\n",
            "Train Epoch: 6 [51200/70080] Loss: 0.008292 Acc: 1.0000\n",
            "Train Epoch: 6 [54400/70080] Loss: 0.030507 Acc: 1.0000\n",
            "Train Epoch: 6 [57600/70080] Loss: 0.014110 Acc: 1.0000\n",
            "Train Epoch: 6 [60800/70080] Loss: 0.009092 Acc: 1.0000\n",
            "Train Epoch: 6 [64000/70080] Loss: 0.018746 Acc: 1.0000\n",
            "Train Epoch: 6 [67200/70080] Loss: 0.003813 Acc: 1.0000\n",
            "Elapsed 173.74s, 24.82 s/epoch, 0.01 s/batch, ets 322.66s\n",
            "\n",
            "Test set: Average loss: 0.0372, Accuracy: 11543/11680 (99%)\n",
            "\n",
            "Train Epoch: 7 [3200/70080] Loss: 0.013640 Acc: 1.0000\n",
            "Train Epoch: 7 [6400/70080] Loss: 0.011489 Acc: 1.0000\n",
            "Train Epoch: 7 [9600/70080] Loss: 0.011096 Acc: 1.0000\n",
            "Train Epoch: 7 [12800/70080] Loss: 0.002237 Acc: 1.0000\n",
            "Train Epoch: 7 [16000/70080] Loss: 0.001760 Acc: 1.0000\n",
            "Train Epoch: 7 [19200/70080] Loss: 0.010863 Acc: 1.0000\n",
            "Train Epoch: 7 [22400/70080] Loss: 0.026845 Acc: 0.9688\n",
            "Train Epoch: 7 [25600/70080] Loss: 0.030602 Acc: 1.0000\n",
            "Train Epoch: 7 [28800/70080] Loss: 0.002253 Acc: 1.0000\n",
            "Train Epoch: 7 [32000/70080] Loss: 0.115147 Acc: 0.9688\n",
            "Train Epoch: 7 [35200/70080] Loss: 0.004289 Acc: 1.0000\n",
            "Train Epoch: 7 [38400/70080] Loss: 0.017520 Acc: 1.0000\n",
            "Train Epoch: 7 [41600/70080] Loss: 0.015145 Acc: 1.0000\n",
            "Train Epoch: 7 [44800/70080] Loss: 0.031066 Acc: 1.0000\n",
            "Train Epoch: 7 [48000/70080] Loss: 0.298729 Acc: 0.9688\n",
            "Train Epoch: 7 [51200/70080] Loss: 0.040909 Acc: 0.9688\n",
            "Train Epoch: 7 [54400/70080] Loss: 0.005315 Acc: 1.0000\n",
            "Train Epoch: 7 [57600/70080] Loss: 0.002467 Acc: 1.0000\n",
            "Train Epoch: 7 [60800/70080] Loss: 0.006590 Acc: 1.0000\n",
            "Train Epoch: 7 [64000/70080] Loss: 0.003965 Acc: 1.0000\n",
            "Train Epoch: 7 [67200/70080] Loss: 0.008598 Acc: 1.0000\n",
            "Elapsed 199.32s, 24.91 s/epoch, 0.01 s/batch, ets 298.97s\n",
            "\n",
            "Test set: Average loss: 0.0400, Accuracy: 11544/11680 (99%)\n",
            "\n",
            "Train Epoch: 8 [3200/70080] Loss: 0.003955 Acc: 1.0000\n",
            "Train Epoch: 8 [6400/70080] Loss: 0.005844 Acc: 1.0000\n",
            "Train Epoch: 8 [9600/70080] Loss: 0.008327 Acc: 1.0000\n",
            "Train Epoch: 8 [12800/70080] Loss: 0.050072 Acc: 0.9688\n",
            "Train Epoch: 8 [16000/70080] Loss: 0.071478 Acc: 0.9688\n",
            "Train Epoch: 8 [19200/70080] Loss: 0.065534 Acc: 0.9688\n",
            "Train Epoch: 8 [22400/70080] Loss: 0.004681 Acc: 1.0000\n",
            "Train Epoch: 8 [25600/70080] Loss: 0.029554 Acc: 1.0000\n",
            "Train Epoch: 8 [28800/70080] Loss: 0.051351 Acc: 0.9688\n",
            "Train Epoch: 8 [32000/70080] Loss: 0.004245 Acc: 1.0000\n",
            "Train Epoch: 8 [35200/70080] Loss: 0.023874 Acc: 1.0000\n",
            "Train Epoch: 8 [38400/70080] Loss: 0.001223 Acc: 1.0000\n",
            "Train Epoch: 8 [41600/70080] Loss: 0.000528 Acc: 1.0000\n",
            "Train Epoch: 8 [44800/70080] Loss: 0.030745 Acc: 0.9688\n",
            "Train Epoch: 8 [48000/70080] Loss: 0.008639 Acc: 1.0000\n",
            "Train Epoch: 8 [51200/70080] Loss: 0.045163 Acc: 0.9688\n",
            "Train Epoch: 8 [54400/70080] Loss: 0.027065 Acc: 1.0000\n",
            "Train Epoch: 8 [57600/70080] Loss: 0.021782 Acc: 1.0000\n",
            "Train Epoch: 8 [60800/70080] Loss: 0.001881 Acc: 1.0000\n",
            "Train Epoch: 8 [64000/70080] Loss: 0.001298 Acc: 1.0000\n",
            "Train Epoch: 8 [67200/70080] Loss: 0.002059 Acc: 1.0000\n",
            "Elapsed 224.83s, 24.98 s/epoch, 0.01 s/batch, ets 274.79s\n",
            "\n",
            "Test set: Average loss: 0.0341, Accuracy: 11561/11680 (99%)\n",
            "\n",
            "Train Epoch: 9 [3200/70080] Loss: 0.043264 Acc: 0.9688\n",
            "Train Epoch: 9 [6400/70080] Loss: 0.035751 Acc: 0.9688\n",
            "Train Epoch: 9 [9600/70080] Loss: 0.044907 Acc: 0.9688\n",
            "Train Epoch: 9 [12800/70080] Loss: 0.015089 Acc: 1.0000\n",
            "Train Epoch: 9 [16000/70080] Loss: 0.001822 Acc: 1.0000\n",
            "Train Epoch: 9 [19200/70080] Loss: 0.009664 Acc: 1.0000\n",
            "Train Epoch: 9 [22400/70080] Loss: 0.015540 Acc: 1.0000\n",
            "Train Epoch: 9 [25600/70080] Loss: 0.247317 Acc: 0.9375\n",
            "Train Epoch: 9 [28800/70080] Loss: 0.009959 Acc: 1.0000\n",
            "Train Epoch: 9 [32000/70080] Loss: 0.015274 Acc: 1.0000\n",
            "Train Epoch: 9 [35200/70080] Loss: 0.010556 Acc: 1.0000\n",
            "Train Epoch: 9 [38400/70080] Loss: 0.021585 Acc: 1.0000\n",
            "Train Epoch: 9 [41600/70080] Loss: 0.002737 Acc: 1.0000\n",
            "Train Epoch: 9 [44800/70080] Loss: 0.018227 Acc: 1.0000\n",
            "Train Epoch: 9 [48000/70080] Loss: 0.001383 Acc: 1.0000\n",
            "Train Epoch: 9 [51200/70080] Loss: 0.015924 Acc: 1.0000\n",
            "Train Epoch: 9 [54400/70080] Loss: 0.018668 Acc: 1.0000\n",
            "Train Epoch: 9 [57600/70080] Loss: 0.002877 Acc: 1.0000\n",
            "Train Epoch: 9 [60800/70080] Loss: 0.017884 Acc: 1.0000\n",
            "Train Epoch: 9 [64000/70080] Loss: 0.012215 Acc: 1.0000\n",
            "Train Epoch: 9 [67200/70080] Loss: 0.008495 Acc: 1.0000\n",
            "Elapsed 250.20s, 25.02 s/epoch, 0.01 s/batch, ets 250.20s\n",
            "\n",
            "Test set: Average loss: 0.0324, Accuracy: 11563/11680 (99%)\n",
            "\n",
            "Train Epoch: 10 [3200/70080] Loss: 0.016973 Acc: 1.0000\n",
            "Train Epoch: 10 [6400/70080] Loss: 0.004700 Acc: 1.0000\n",
            "Train Epoch: 10 [9600/70080] Loss: 0.003652 Acc: 1.0000\n",
            "Train Epoch: 10 [12800/70080] Loss: 0.021898 Acc: 1.0000\n",
            "Train Epoch: 10 [16000/70080] Loss: 0.000342 Acc: 1.0000\n",
            "Train Epoch: 10 [19200/70080] Loss: 0.007906 Acc: 1.0000\n",
            "Train Epoch: 10 [22400/70080] Loss: 0.056263 Acc: 0.9688\n",
            "Train Epoch: 10 [25600/70080] Loss: 0.058225 Acc: 0.9688\n",
            "Train Epoch: 10 [28800/70080] Loss: 0.007721 Acc: 1.0000\n",
            "Train Epoch: 10 [32000/70080] Loss: 0.052432 Acc: 0.9688\n",
            "Train Epoch: 10 [35200/70080] Loss: 0.125592 Acc: 0.9375\n",
            "Train Epoch: 10 [38400/70080] Loss: 0.044439 Acc: 0.9688\n",
            "Train Epoch: 10 [41600/70080] Loss: 0.010999 Acc: 1.0000\n",
            "Train Epoch: 10 [44800/70080] Loss: 0.016656 Acc: 1.0000\n",
            "Train Epoch: 10 [48000/70080] Loss: 0.016157 Acc: 1.0000\n",
            "Train Epoch: 10 [51200/70080] Loss: 0.002310 Acc: 1.0000\n",
            "Train Epoch: 10 [54400/70080] Loss: 0.002738 Acc: 1.0000\n",
            "Train Epoch: 10 [57600/70080] Loss: 0.005997 Acc: 1.0000\n",
            "Train Epoch: 10 [60800/70080] Loss: 0.028178 Acc: 0.9688\n",
            "Train Epoch: 10 [64000/70080] Loss: 0.006801 Acc: 1.0000\n",
            "Train Epoch: 10 [67200/70080] Loss: 0.048113 Acc: 0.9688\n",
            "Elapsed 275.67s, 25.06 s/epoch, 0.01 s/batch, ets 225.55s\n",
            "\n",
            "Test set: Average loss: 0.0317, Accuracy: 11568/11680 (99%)\n",
            "\n",
            "Train Epoch: 11 [3200/70080] Loss: 0.001437 Acc: 1.0000\n",
            "Train Epoch: 11 [6400/70080] Loss: 0.094148 Acc: 0.9688\n",
            "Train Epoch: 11 [9600/70080] Loss: 0.002767 Acc: 1.0000\n",
            "Train Epoch: 11 [12800/70080] Loss: 0.010617 Acc: 1.0000\n",
            "Train Epoch: 11 [16000/70080] Loss: 0.159583 Acc: 0.9688\n",
            "Train Epoch: 11 [19200/70080] Loss: 0.151127 Acc: 0.9375\n",
            "Train Epoch: 11 [22400/70080] Loss: 0.012507 Acc: 1.0000\n",
            "Train Epoch: 11 [25600/70080] Loss: 0.017612 Acc: 1.0000\n",
            "Train Epoch: 11 [28800/70080] Loss: 0.027790 Acc: 1.0000\n",
            "Train Epoch: 11 [32000/70080] Loss: 0.020032 Acc: 1.0000\n",
            "Train Epoch: 11 [35200/70080] Loss: 0.154053 Acc: 0.9062\n",
            "Train Epoch: 11 [38400/70080] Loss: 0.000650 Acc: 1.0000\n",
            "Train Epoch: 11 [41600/70080] Loss: 0.060290 Acc: 0.9688\n",
            "Train Epoch: 11 [44800/70080] Loss: 0.009167 Acc: 1.0000\n",
            "Train Epoch: 11 [48000/70080] Loss: 0.002565 Acc: 1.0000\n",
            "Train Epoch: 11 [51200/70080] Loss: 0.012857 Acc: 1.0000\n",
            "Train Epoch: 11 [54400/70080] Loss: 0.005159 Acc: 1.0000\n",
            "Train Epoch: 11 [57600/70080] Loss: 0.007413 Acc: 1.0000\n",
            "Train Epoch: 11 [60800/70080] Loss: 0.009112 Acc: 1.0000\n",
            "Train Epoch: 11 [64000/70080] Loss: 0.004344 Acc: 1.0000\n",
            "Train Epoch: 11 [67200/70080] Loss: 0.472130 Acc: 0.9688\n",
            "Elapsed 301.57s, 25.13 s/epoch, 0.01 s/batch, ets 201.05s\n",
            "\n",
            "Test set: Average loss: 0.0299, Accuracy: 11576/11680 (99%)\n",
            "\n",
            "Train Epoch: 12 [3200/70080] Loss: 0.002371 Acc: 1.0000\n",
            "Train Epoch: 12 [6400/70080] Loss: 0.042529 Acc: 0.9688\n",
            "Train Epoch: 12 [9600/70080] Loss: 0.007233 Acc: 1.0000\n",
            "Train Epoch: 12 [12800/70080] Loss: 0.004713 Acc: 1.0000\n",
            "Train Epoch: 12 [16000/70080] Loss: 0.011110 Acc: 1.0000\n",
            "Train Epoch: 12 [19200/70080] Loss: 0.001057 Acc: 1.0000\n",
            "Train Epoch: 12 [22400/70080] Loss: 0.000312 Acc: 1.0000\n",
            "Train Epoch: 12 [25600/70080] Loss: 0.013137 Acc: 1.0000\n",
            "Train Epoch: 12 [28800/70080] Loss: 0.001159 Acc: 1.0000\n",
            "Train Epoch: 12 [32000/70080] Loss: 0.001155 Acc: 1.0000\n",
            "Train Epoch: 12 [35200/70080] Loss: 0.005868 Acc: 1.0000\n",
            "Train Epoch: 12 [38400/70080] Loss: 0.095013 Acc: 0.9688\n",
            "Train Epoch: 12 [41600/70080] Loss: 0.005264 Acc: 1.0000\n",
            "Train Epoch: 12 [44800/70080] Loss: 0.004666 Acc: 1.0000\n",
            "Train Epoch: 12 [48000/70080] Loss: 0.012824 Acc: 1.0000\n",
            "Train Epoch: 12 [51200/70080] Loss: 0.009152 Acc: 1.0000\n",
            "Train Epoch: 12 [54400/70080] Loss: 0.016604 Acc: 1.0000\n",
            "Train Epoch: 12 [57600/70080] Loss: 0.001032 Acc: 1.0000\n",
            "Train Epoch: 12 [60800/70080] Loss: 0.014668 Acc: 1.0000\n",
            "Train Epoch: 12 [64000/70080] Loss: 0.002515 Acc: 1.0000\n",
            "Train Epoch: 12 [67200/70080] Loss: 0.020768 Acc: 1.0000\n",
            "Elapsed 327.80s, 25.22 s/epoch, 0.01 s/batch, ets 176.51s\n",
            "\n",
            "Test set: Average loss: 0.0305, Accuracy: 11575/11680 (99%)\n",
            "\n",
            "Train Epoch: 13 [3200/70080] Loss: 0.064312 Acc: 0.9688\n",
            "Train Epoch: 13 [6400/70080] Loss: 0.002066 Acc: 1.0000\n",
            "Train Epoch: 13 [9600/70080] Loss: 0.003614 Acc: 1.0000\n",
            "Train Epoch: 13 [12800/70080] Loss: 0.002573 Acc: 1.0000\n",
            "Train Epoch: 13 [16000/70080] Loss: 0.013123 Acc: 1.0000\n",
            "Train Epoch: 13 [19200/70080] Loss: 0.006954 Acc: 1.0000\n",
            "Train Epoch: 13 [22400/70080] Loss: 0.049208 Acc: 0.9688\n",
            "Train Epoch: 13 [25600/70080] Loss: 0.005786 Acc: 1.0000\n",
            "Train Epoch: 13 [28800/70080] Loss: 0.003113 Acc: 1.0000\n",
            "Train Epoch: 13 [32000/70080] Loss: 0.078346 Acc: 0.9688\n",
            "Train Epoch: 13 [35200/70080] Loss: 0.015004 Acc: 1.0000\n",
            "Train Epoch: 13 [38400/70080] Loss: 0.029322 Acc: 1.0000\n",
            "Train Epoch: 13 [41600/70080] Loss: 0.007868 Acc: 1.0000\n",
            "Train Epoch: 13 [44800/70080] Loss: 0.017385 Acc: 1.0000\n",
            "Train Epoch: 13 [48000/70080] Loss: 0.001940 Acc: 1.0000\n",
            "Train Epoch: 13 [51200/70080] Loss: 0.001691 Acc: 1.0000\n",
            "Train Epoch: 13 [54400/70080] Loss: 0.013121 Acc: 1.0000\n",
            "Train Epoch: 13 [57600/70080] Loss: 0.003184 Acc: 1.0000\n",
            "Train Epoch: 13 [60800/70080] Loss: 0.014018 Acc: 1.0000\n",
            "Train Epoch: 13 [64000/70080] Loss: 0.040986 Acc: 0.9688\n",
            "Train Epoch: 13 [67200/70080] Loss: 0.001927 Acc: 1.0000\n",
            "Elapsed 353.82s, 25.27 s/epoch, 0.01 s/batch, ets 151.64s\n",
            "\n",
            "Test set: Average loss: 0.0282, Accuracy: 11579/11680 (99%)\n",
            "\n",
            "Train Epoch: 14 [3200/70080] Loss: 0.075348 Acc: 0.9688\n",
            "Train Epoch: 14 [6400/70080] Loss: 0.079546 Acc: 0.9688\n",
            "Train Epoch: 14 [9600/70080] Loss: 0.003806 Acc: 1.0000\n",
            "Train Epoch: 14 [12800/70080] Loss: 0.005267 Acc: 1.0000\n",
            "Train Epoch: 14 [16000/70080] Loss: 0.009139 Acc: 1.0000\n",
            "Train Epoch: 14 [19200/70080] Loss: 0.022092 Acc: 1.0000\n",
            "Train Epoch: 14 [22400/70080] Loss: 0.003979 Acc: 1.0000\n",
            "Train Epoch: 14 [25600/70080] Loss: 0.002111 Acc: 1.0000\n",
            "Train Epoch: 14 [28800/70080] Loss: 0.012327 Acc: 1.0000\n",
            "Train Epoch: 14 [32000/70080] Loss: 0.005038 Acc: 1.0000\n",
            "Train Epoch: 14 [35200/70080] Loss: 0.026022 Acc: 1.0000\n",
            "Train Epoch: 14 [38400/70080] Loss: 0.004098 Acc: 1.0000\n",
            "Train Epoch: 14 [41600/70080] Loss: 0.013592 Acc: 1.0000\n",
            "Train Epoch: 14 [44800/70080] Loss: 0.084699 Acc: 0.9688\n",
            "Train Epoch: 14 [48000/70080] Loss: 0.023163 Acc: 1.0000\n",
            "Train Epoch: 14 [51200/70080] Loss: 0.052033 Acc: 0.9688\n",
            "Train Epoch: 14 [54400/70080] Loss: 0.003387 Acc: 1.0000\n",
            "Train Epoch: 14 [57600/70080] Loss: 0.002354 Acc: 1.0000\n",
            "Train Epoch: 14 [60800/70080] Loss: 0.001371 Acc: 1.0000\n",
            "Train Epoch: 14 [64000/70080] Loss: 0.073761 Acc: 0.9688\n",
            "Train Epoch: 14 [67200/70080] Loss: 0.001166 Acc: 1.0000\n",
            "Elapsed 379.68s, 25.31 s/epoch, 0.01 s/batch, ets 126.56s\n",
            "\n",
            "Test set: Average loss: 0.0310, Accuracy: 11568/11680 (99%)\n",
            "\n",
            "Train Epoch: 15 [3200/70080] Loss: 0.008889 Acc: 1.0000\n",
            "Train Epoch: 15 [6400/70080] Loss: 0.008978 Acc: 1.0000\n",
            "Train Epoch: 15 [9600/70080] Loss: 0.000458 Acc: 1.0000\n",
            "Train Epoch: 15 [12800/70080] Loss: 0.005516 Acc: 1.0000\n",
            "Train Epoch: 15 [16000/70080] Loss: 0.007556 Acc: 1.0000\n",
            "Train Epoch: 15 [19200/70080] Loss: 0.026811 Acc: 1.0000\n",
            "Train Epoch: 15 [22400/70080] Loss: 0.078923 Acc: 0.9688\n",
            "Train Epoch: 15 [25600/70080] Loss: 0.001544 Acc: 1.0000\n",
            "Train Epoch: 15 [28800/70080] Loss: 0.003067 Acc: 1.0000\n",
            "Train Epoch: 15 [32000/70080] Loss: 0.003901 Acc: 1.0000\n",
            "Train Epoch: 15 [35200/70080] Loss: 0.006502 Acc: 1.0000\n",
            "Train Epoch: 15 [38400/70080] Loss: 0.004182 Acc: 1.0000\n",
            "Train Epoch: 15 [41600/70080] Loss: 0.029549 Acc: 1.0000\n",
            "Train Epoch: 15 [44800/70080] Loss: 0.001820 Acc: 1.0000\n",
            "Train Epoch: 15 [48000/70080] Loss: 0.002041 Acc: 1.0000\n",
            "Train Epoch: 15 [51200/70080] Loss: 0.000403 Acc: 1.0000\n",
            "Train Epoch: 15 [54400/70080] Loss: 0.000810 Acc: 1.0000\n",
            "Train Epoch: 15 [57600/70080] Loss: 0.007538 Acc: 1.0000\n",
            "Train Epoch: 15 [60800/70080] Loss: 0.012886 Acc: 1.0000\n",
            "Train Epoch: 15 [64000/70080] Loss: 0.114134 Acc: 0.9688\n",
            "Train Epoch: 15 [67200/70080] Loss: 0.001831 Acc: 1.0000\n",
            "Elapsed 405.71s, 25.36 s/epoch, 0.01 s/batch, ets 101.43s\n",
            "\n",
            "Test set: Average loss: 0.0314, Accuracy: 11568/11680 (99%)\n",
            "\n",
            "Train Epoch: 16 [3200/70080] Loss: 0.001472 Acc: 1.0000\n",
            "Train Epoch: 16 [6400/70080] Loss: 0.006297 Acc: 1.0000\n",
            "Train Epoch: 16 [9600/70080] Loss: 0.001671 Acc: 1.0000\n",
            "Train Epoch: 16 [12800/70080] Loss: 0.020355 Acc: 1.0000\n",
            "Train Epoch: 16 [16000/70080] Loss: 0.015640 Acc: 1.0000\n",
            "Train Epoch: 16 [19200/70080] Loss: 0.000652 Acc: 1.0000\n",
            "Train Epoch: 16 [22400/70080] Loss: 0.001830 Acc: 1.0000\n",
            "Train Epoch: 16 [25600/70080] Loss: 0.010485 Acc: 1.0000\n",
            "Train Epoch: 16 [28800/70080] Loss: 0.003285 Acc: 1.0000\n",
            "Train Epoch: 16 [32000/70080] Loss: 0.004891 Acc: 1.0000\n",
            "Train Epoch: 16 [35200/70080] Loss: 0.009442 Acc: 1.0000\n",
            "Train Epoch: 16 [38400/70080] Loss: 0.000621 Acc: 1.0000\n",
            "Train Epoch: 16 [41600/70080] Loss: 0.000976 Acc: 1.0000\n",
            "Train Epoch: 16 [44800/70080] Loss: 0.010533 Acc: 1.0000\n",
            "Train Epoch: 16 [48000/70080] Loss: 0.002528 Acc: 1.0000\n",
            "Train Epoch: 16 [51200/70080] Loss: 0.000795 Acc: 1.0000\n",
            "Train Epoch: 16 [54400/70080] Loss: 0.002434 Acc: 1.0000\n",
            "Train Epoch: 16 [57600/70080] Loss: 0.011673 Acc: 1.0000\n",
            "Train Epoch: 16 [60800/70080] Loss: 0.072885 Acc: 0.9688\n",
            "Train Epoch: 16 [64000/70080] Loss: 0.004631 Acc: 1.0000\n",
            "Train Epoch: 16 [67200/70080] Loss: 0.021487 Acc: 1.0000\n",
            "Elapsed 431.71s, 25.39 s/epoch, 0.01 s/batch, ets 76.18s\n",
            "\n",
            "Test set: Average loss: 0.0331, Accuracy: 11575/11680 (99%)\n",
            "\n",
            "Train Epoch: 17 [3200/70080] Loss: 0.026020 Acc: 0.9688\n",
            "Train Epoch: 17 [6400/70080] Loss: 0.027486 Acc: 1.0000\n",
            "Train Epoch: 17 [9600/70080] Loss: 0.039459 Acc: 0.9688\n",
            "Train Epoch: 17 [12800/70080] Loss: 0.000417 Acc: 1.0000\n",
            "Train Epoch: 17 [16000/70080] Loss: 0.000833 Acc: 1.0000\n",
            "Train Epoch: 17 [19200/70080] Loss: 0.000571 Acc: 1.0000\n",
            "Train Epoch: 17 [22400/70080] Loss: 0.003008 Acc: 1.0000\n",
            "Train Epoch: 17 [25600/70080] Loss: 0.000450 Acc: 1.0000\n",
            "Train Epoch: 17 [28800/70080] Loss: 0.000590 Acc: 1.0000\n",
            "Train Epoch: 17 [32000/70080] Loss: 0.002153 Acc: 1.0000\n",
            "Train Epoch: 17 [35200/70080] Loss: 0.003433 Acc: 1.0000\n",
            "Train Epoch: 17 [38400/70080] Loss: 0.022110 Acc: 1.0000\n",
            "Train Epoch: 17 [41600/70080] Loss: 0.008948 Acc: 1.0000\n",
            "Train Epoch: 17 [44800/70080] Loss: 0.017957 Acc: 1.0000\n",
            "Train Epoch: 17 [48000/70080] Loss: 0.033751 Acc: 1.0000\n",
            "Train Epoch: 17 [51200/70080] Loss: 0.000447 Acc: 1.0000\n",
            "Train Epoch: 17 [54400/70080] Loss: 0.001838 Acc: 1.0000\n",
            "Train Epoch: 17 [57600/70080] Loss: 0.023779 Acc: 1.0000\n",
            "Train Epoch: 17 [60800/70080] Loss: 0.002467 Acc: 1.0000\n",
            "Train Epoch: 17 [64000/70080] Loss: 0.001243 Acc: 1.0000\n",
            "Train Epoch: 17 [67200/70080] Loss: 0.002219 Acc: 1.0000\n",
            "Elapsed 457.31s, 25.41 s/epoch, 0.01 s/batch, ets 50.81s\n",
            "\n",
            "Test set: Average loss: 0.0297, Accuracy: 11572/11680 (99%)\n",
            "\n",
            "Train Epoch: 18 [3200/70080] Loss: 0.000944 Acc: 1.0000\n",
            "Train Epoch: 18 [6400/70080] Loss: 0.002511 Acc: 1.0000\n",
            "Train Epoch: 18 [9600/70080] Loss: 0.006375 Acc: 1.0000\n",
            "Train Epoch: 18 [12800/70080] Loss: 0.010053 Acc: 1.0000\n",
            "Train Epoch: 18 [16000/70080] Loss: 0.003166 Acc: 1.0000\n",
            "Train Epoch: 18 [19200/70080] Loss: 0.002426 Acc: 1.0000\n",
            "Train Epoch: 18 [22400/70080] Loss: 0.002904 Acc: 1.0000\n",
            "Train Epoch: 18 [25600/70080] Loss: 0.002387 Acc: 1.0000\n",
            "Train Epoch: 18 [28800/70080] Loss: 0.003839 Acc: 1.0000\n",
            "Train Epoch: 18 [32000/70080] Loss: 0.000849 Acc: 1.0000\n",
            "Train Epoch: 18 [35200/70080] Loss: 0.003574 Acc: 1.0000\n",
            "Train Epoch: 18 [38400/70080] Loss: 0.462866 Acc: 0.9688\n",
            "Train Epoch: 18 [41600/70080] Loss: 0.002224 Acc: 1.0000\n",
            "Train Epoch: 18 [44800/70080] Loss: 0.038573 Acc: 0.9688\n",
            "Train Epoch: 18 [48000/70080] Loss: 0.079976 Acc: 0.9375\n",
            "Train Epoch: 18 [51200/70080] Loss: 0.015460 Acc: 1.0000\n",
            "Train Epoch: 18 [54400/70080] Loss: 0.000581 Acc: 1.0000\n",
            "Train Epoch: 18 [57600/70080] Loss: 0.095580 Acc: 0.9688\n",
            "Train Epoch: 18 [60800/70080] Loss: 0.013094 Acc: 1.0000\n",
            "Train Epoch: 18 [64000/70080] Loss: 0.001820 Acc: 1.0000\n",
            "Train Epoch: 18 [67200/70080] Loss: 0.044319 Acc: 0.9688\n",
            "Elapsed 482.49s, 25.39 s/epoch, 0.01 s/batch, ets 25.39s\n",
            "\n",
            "Test set: Average loss: 0.0284, Accuracy: 11578/11680 (99%)\n",
            "\n",
            "Train Epoch: 19 [3200/70080] Loss: 0.007261 Acc: 1.0000\n",
            "Train Epoch: 19 [6400/70080] Loss: 0.001129 Acc: 1.0000\n",
            "Train Epoch: 19 [9600/70080] Loss: 0.004896 Acc: 1.0000\n",
            "Train Epoch: 19 [12800/70080] Loss: 0.056103 Acc: 0.9688\n",
            "Train Epoch: 19 [16000/70080] Loss: 0.152759 Acc: 0.9688\n",
            "Train Epoch: 19 [19200/70080] Loss: 0.021685 Acc: 1.0000\n",
            "Train Epoch: 19 [22400/70080] Loss: 0.001611 Acc: 1.0000\n",
            "Train Epoch: 19 [25600/70080] Loss: 0.003802 Acc: 1.0000\n",
            "Train Epoch: 19 [28800/70080] Loss: 0.032373 Acc: 0.9688\n",
            "Train Epoch: 19 [32000/70080] Loss: 0.004465 Acc: 1.0000\n",
            "Train Epoch: 19 [35200/70080] Loss: 0.000344 Acc: 1.0000\n",
            "Train Epoch: 19 [38400/70080] Loss: 0.000912 Acc: 1.0000\n",
            "Train Epoch: 19 [41600/70080] Loss: 0.000928 Acc: 1.0000\n",
            "Train Epoch: 19 [44800/70080] Loss: 0.043475 Acc: 0.9688\n",
            "Train Epoch: 19 [48000/70080] Loss: 0.001751 Acc: 1.0000\n",
            "Train Epoch: 19 [51200/70080] Loss: 0.001967 Acc: 1.0000\n",
            "Train Epoch: 19 [54400/70080] Loss: 0.014717 Acc: 1.0000\n",
            "Train Epoch: 19 [57600/70080] Loss: 0.000145 Acc: 1.0000\n",
            "Train Epoch: 19 [60800/70080] Loss: 0.033727 Acc: 0.9688\n",
            "Train Epoch: 19 [64000/70080] Loss: 0.211640 Acc: 0.9688\n",
            "Train Epoch: 19 [67200/70080] Loss: 0.141844 Acc: 0.9375\n",
            "Elapsed 507.26s, 25.36 s/epoch, 0.01 s/batch, ets 0.00s\n",
            "\n",
            "Test set: Average loss: 0.0338, Accuracy: 11566/11680 (99%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUMHs1Revk4O",
        "colab_type": "code",
        "outputId": "1a4b30f1-a0b4-4e63-e5cf-14da09685e80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"Total time: {:.2f}, Best Loss: {:.3f}\".format(time.time() - t_begin, best_loss))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total time: 510.55, Best Loss: 0.028\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeeFfy2I99GO",
        "colab_type": "code",
        "outputId": "a59b4bc3-650b-4f3b-ae67-409329056e0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        }
      },
      "source": [
        "# Plot loss\n",
        "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
        "x = range(len(epoch_train_loss))\n",
        "\n",
        "plt.plot(x, epoch_train_loss, 'r', label=\"train loss\")\n",
        "plt.plot(x, epoch_test_loss, 'b',label=\"validation loss\")\n",
        "\n",
        "plt.xlabel('epoch .')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmEAAAGDCAYAAABjkcdfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtAuFBvK99I9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#save model with best loss\n",
        "torch.save(best_model.state_dict(), '../content/gdrive/My Drive/models/MNIST_aug_model.pth')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TXaeqk499Kj",
        "colab_type": "code",
        "outputId": "a3a86afc-d313-4fa7-ac32-7de5ae05e407",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "#download and test model\n",
        "model = LeNetBN()\n",
        "model.load_state_dict(torch.load('../content/gdrive/My Drive/models/MNIST_aug_model.pth'))\n",
        "#print(model1)\n",
        "images, labels = next(iter(test_loader_MNIST))\n",
        "plt.imshow(images[0][0],'gray')\n",
        "image = images[0,:]#.to(device)\n",
        "image = image[None]\n",
        "plt.show()\n",
        "score = model(image)\n",
        "prob = nn.functional.softmax(score[0], dim=0)\n",
        "y_pred =  prob.argmax()\n",
        "print(\"Predicted class {} with probability {}\".format(y_pred, prob[y_pred]))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWgAAAFlCAYAAADGe3ILAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFEVBA0BR/w+3yPveWL4lYwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Predicted class 7 with probability 0.9999927282333374\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjaJm7q2eI6x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "test = pd.read_csv(\"../content/gdrive/My Drive/MNIST/test.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iImU4e_feOKr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_image = test.loc[:,test.columns != \"label\"]\n",
        "test_dataset = torch.from_numpy(np.reshape(test_image.to_numpy().astype(np.uint8), (test_image.shape[0], 1, 28,28)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lBFNCtyb5gB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results = []\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    for img in test_dataset:\n",
        "        img = transforms.ToPILImage()(img)\n",
        "        img = transforms.Resize((32, 32))(img)\n",
        "        img = transforms.ToTensor()(img)\n",
        "        img = transforms.Normalize((0.1216, ), (0.2796, ))(img)\n",
        "        test_im = img#.to(device)\n",
        "        test_im = test_im[None]\n",
        "        output = model(test_im)\n",
        "        prob = nn.functional.softmax(output[0], dim=0)\n",
        "        y_pred =  prob.argmax()\n",
        "        results.append( y_pred.cpu().data.numpy().tolist())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WJoHD0xcKVv",
        "colab_type": "code",
        "outputId": "06a2511f-0610-4c16-a870-4ef2bce4afb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(results)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TJeDsB7cMZh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = np.array(results).flatten()\n",
        "submissions=pd.DataFrame({\"ImageId\": list(range(1,len(predictions)+1)),\n",
        "                         \"Label\": predictions})\n",
        "submissions.to_csv(\"../content/gdrive/My Drive/MNIST/my_submissions06.csv\", index=False, header=True)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}